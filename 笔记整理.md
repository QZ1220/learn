# 笔记整理

标签（空格分隔）： 笔记

---

涉及的东西还是很多很广的，总体而言感觉广发的技术实力还是很不错的。

问题
=====

大致的一些内容包括java、jvm、*多线程、线程池、进程与线程*、数据结构、*set的底层实现*、*Java的深复制浅复制*、*main函数执行所发生的一系列动作*、*sql优化、什么情况导致不走索引*、*数据库的隔离性的4个等级*、*常用的设计模式*、*单例的实现及多种实现方式*、*GC回收算法、现在的JDK使用什么GC算法*、*Java的reader和writer使用了什么设计模式*、spring的前后端交互时所经历的一系列过程、object怎么转换成json串的、*hashmap遇到key重复会怎样、怎么取出重复位置指定的value、collection与collections的区别*、*java中length,length(),size()区别*，*quartz的任务调度怎么进行的以及如何选择节点来执行定时任务*、*hashcode函数与equals方法*、偏向锁、数据库索引优化、查询优化和存储优化、*string类的源码分析、为什么是不可变的*、*jvm新生代1和新生代2的区别*、*什么是Servlet，Servlet的作用，生命周期，如何创建、配置Servlet*、*Spring事务的传播特性和隔离级别*、bean的生命周期、applicationContext和beanFactory的关系、*concurrentHashMap的底层实现*、*java类的加载过程*、分布式的基本知识、*violatile关键字*、*为什么spring的DAO只使用接口就可以调用xml*、spring的filter使用了什么设计模式、spring的AOP使用了什么设计模式、*Java的threadlocal类*、*JDK动态代理与CGLIB动态代理*、nginx负载均衡、Java锁的类型、java实现冒泡排序和快速排序 

技术栈
======

：前端angular、vue、react，后端微服务、MVC、spring、springMVC、springBoot、springCloud，分布式Docker、hadoop+spark

collection与collections的区别
-------------------------
/home/audi/Pictures/collection.png
首先，Collection是接口，Collections是类。
Collection是set和list的父接口，需要注意的是map也是一个接口，它和collection没有继承派生的关系。
Collections是针对集合类的一个工具类，提供了操作集合的工具方法：一系列静态方法实现对各种集合的搜索、排序、线程安全化(比如Collections.synchronizedList使链表安全)等操作。它通过将**构造函数私有化**来避免创建对象。

hashmap遇到key重复会怎样、怎么取出重复位置指定的value
----------------------------------
首先看一篇参考笔记：
文章写的很好，以JDK1.8为例，详细的讲述了hashmap中几个重要的函数，get\put\resize等。
hashmap的key重复的话，那么必然会导致记得得到的hash（key）重复，存储的位置也会相同，所以是value会产生**覆盖**的效果。
hashmap的所有节点元素都是一个Node<k,v>，Node节点的源代码（链接上的哥们儿给的注释，挺详细的）如下所示：
```java
    //Node是单向链表，它实现了Map.Entry接口
    static class Node<k,v> implements Map.Entry<k,v> {
        final int hash;
        final K key;
        V value;
        Node<k,v> next;
        //构造函数Hash值 键 值 下一个节点
        Node(int hash, K key, V value, Node<k,v> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }
     
        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + = + value; }
     
        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }
     
        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }
        //判断两个node是否相等,若key和value都相等，返回true。可以与自身比较为true
        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<!--?,?--> e = (Map.Entry<!--?,?-->)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
```
hashmap的构造函数可以指定初始容量（注意不一定要是2的幂次，是基数也可以），也可以不指定，此时大小为默认值16，也可以指定hahmap的装载比，源码如下：
```java
//构造函数1
public HashMap(int initialCapacity, float loadFactor) {
    //指定的初始容量非负
    if (initialCapacity < 0)
        throw new IllegalArgumentException(Illegal initial capacity:  +
                                           initialCapacity);
    //如果指定的初始容量大于最大容量,置为最大容量
    if (initialCapacity > MAXIMUM_CAPACITY)
        initialCapacity = MAXIMUM_CAPACITY;
    //填充比为正
    if (loadFactor <= 0 || Float.isNaN(loadFactor))
        throw new IllegalArgumentException(Illegal load factor:  +
                                           loadFactor);
    this.loadFactor = loadFactor;
    this.threshold = tableSizeFor(initialCapacity);//新的扩容临界值
}
 
//构造函数2
public HashMap(int initialCapacity) {
    this(initialCapacity, DEFAULT_LOAD_FACTOR);
}
 
//构造函数3
public HashMap() {
    this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
}
 
//构造函数4用m的元素初始化散列映射
public HashMap(Map<!--? extends K, ? extends V--> m) {
    this.loadFactor = DEFAULT_LOAD_FACTOR;
    putMapEntries(m, false);
}
```
hashmap获取value的机制：使用hash（key）和n-1（n=tab.length）进行位与操作，避免出现越界的情况。
取元素的时候，首先根据hash（key）定位元素的位置，然后首个元素是不是需要的元素（判断的原则是hash值相等，且key相等），如果不是就继续取元素，取的时候分为链表遍历和红黑树遍历。
```java
public V get(Object key) {
        Node<K,V> e;
        return (e = getNode(hash(key), key)) == null ? null : e.value;
    }
	  /**
     * Implements Map.get and related methods
     *
     * @param hash hash for key
     * @param key the key
     * @return the node, or null if none
     */
	final Node<K,V> getNode(int hash, Object key) {
        Node<K,V>[] tab;//Entry对象数组
	Node<K,V> first,e; //在tab数组中经过散列的第一个位置
	int n;
	K k;
	/*找到插入的第一个Node，方法是hash值和n-1相与，tab[(n - 1) & hash]*/
	//也就是说在一条链上的hash值相同的
        if ((tab = table) != null && (n = tab.length) > 0 &&(first = tab[(n - 1) & hash]) != null) {
	/*检查第一个Node是不是要找的Node*/
            if (first.hash == hash && // always check first node
                ((k = first.key) == key || (key != null && key.equals(k))))//判断条件是hash值要相同，key值要相同
                return first;
	  /*检查first后面的node*/
            if ((e = first.next) != null) {
                if (first instanceof TreeNode)
                    return ((TreeNode<K,V>)first).getTreeNode(hash, key);
				/*遍历后面的链表，找到key值和hash值都相同的Node*/
                do {
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        return e;
                } while ((e = e.next) != null);
            }
        }
        return null;
    }
```
hashmap存储Node的机制：下面是源码。
put的时候首先判断hash表是否创建了，没有就创建一个；
然后使用hash（key）&（n-1）来确定该put的位置，如果要put的位置没有元素，那么直接put；否则，
如果key相同，那么就执行替换操作；
如果不同，且是treeNode，那么就按照红黑树的put方式放入元素；
是链表，就在链表的尾部put元素。
```java
public V put(K key, V value) {
        return putVal(hash(key), key, value, false, true);
    }
	 /**
     * Implements Map.put and related methods
     *
     * @param hash hash for key
     * @param key the key
     * @param value the value to put
     * @param onlyIfAbsent if true, don't change existing value
     * @param evict if false, the table is in creation mode.
     * @return previous value, or null if none 注意这个返回值情况，因为在set的add方法中会调用这个putval方法，并且通过返回值来判断是否加入成功的
     * putval函数会在执行完put操作以后进行判断，是否需要扩容resize
     */
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; 
	Node<K,V> p; 
	int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;
	/*如果table的在（n-1）&hash的值是空，就新建一个节点插入在该位置*/
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
	/*表示有冲突,开始处理冲突*/
        else {
            Node<K,V> e; 
	    K k;
	/*检查第一个Node，p是不是要找的值*/
            if (p.hash == hash &&((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                for (int binCount = 0; ; ++binCount) {
		/*指针为空就挂在后面*/
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
		       //如果冲突的节点数已经达到8个，看是否需要改变冲突节点的存储结构，　　　　　　　　　　　　　
　　　　　　　　　　　　//treeifyBin首先判断当前hashMap的长度，如果不足64，只进行
                        //resize，扩容table，如果达到64，那么将冲突的存储结构为红黑树
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
		/*如果有相同的key值就结束遍历*/
                    if (e.hash == hash &&((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
	/*就是链表上有相同的key值*/
            if (e != null) { // existing mapping for key，就是key的Value存在
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;//返回存在的Value值
            }
        }
        ++modCount;
     /*如果当前大小大于门限，门限原本是初始容量*0.75*/
        if (++size > threshold)
            resize();//扩容两倍
        afterNodeInsertion(evict);
        return null;
    }
```
hashmap的resize()函数
构造hash表时，如果不指明初始大小，默认大小为16（即Node数组大小16），如果Node[]数组中的元素达到（填充比*Node.length）重新调整HashMap大小 变为原来2倍大小,**扩容很耗时**
resize函数不仅仅要完成map的扩容，还要完成hash表元素位置的迁移。
resize的时候，可能有2种情况。
一是hash表本就是空的，且门限值thresHold>0,此时需要将 
```java
newCap = oldThr;  
```
如果门限值thresHold<=0,需要按照默认大小进行hash表的创建
二是hash表不为空，此时需要进行2倍扩容和Node元素存储位置更改，元素位置的移动要参考http://www.importnew.com/20386.html，注意其中resize的部分

下面是resize函数的源码：
```java
  /**
     * Initializes or doubles table size.  If null, allocates in
     * accord with initial capacity target held in field threshold.
     * Otherwise, because we are using power-of-two expansion, the
     * elements from each bin must either stay at same index, or move
     * with a power of two offset in the new table.
     *
     * @return the table
     */
    final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
		
	/*如果旧表的长度不是空*/
        if (oldCap > 0) {
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
	/*把新表的长度设置为旧表长度的两倍，newCap=2*oldCap*/
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
	      /*把新表的门限设置为旧表门限的两倍，newThr=oldThr*2*/
                newThr = oldThr << 1; // double threshold
        }
     /*如果旧表的长度的是0，就是说第一次初始化表*/
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
		
		
		
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;//新表长度乘以加载因子
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
	/*下面开始构造新表，初始化表中的数据*/
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;//把新表赋值给table
        if (oldTab != null) {//原表不是空要把原表中数据移动到新表中	
            /*遍历原来的旧表*/		
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)//说明这个node没有链表直接放在新表的e.hash & (newCap - 1)位置
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
	/*如果e后边有链表,到这里表示e后面带着个单链表，需要遍历单链表，将每个结点重*/
                    else { // preserve order保证顺序
					////新计算在新表的位置，并进行搬运
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
						
                        do {
                            next = e.next;//记录下一个结点
			  //新表是旧表的两倍容量，实例上就把单链表拆分为两队，
　　　　　　　　　　　　　　//e.hash&oldCap为偶数一队，e.hash&oldCap为奇数一对
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
						
                        if (loTail != null) {//lo队不为null，放在新表原位置
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {//hi队不为null，放在新表j+oldCap位置
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    }
```
如果同一个entryset位置存储的元素>=8时，hashmap就会采用红黑树的方式来存储元素，可以查找的时间复杂度从O(N)降为O(logN)。

JDK1.8HashMap的红黑树是这样解决的：
------------------------

如果某个桶中的记录过大的话（当前是TREEIFY_THRESHOLD = 8），HashMap会动态的使用一个专门的treemap实现来替换掉它。这样做的结果会更好，是O(logn)，而不是糟糕的O(n)。
它是如何工作的？前面产生冲突的那些KEY对应的记录只是简单的追加到一个链表后面，这些记录只能通过遍历来进行查找。但是超过这个阈值后HashMap开始将列表升级成一个二叉树，使用哈希值作为树的分支变量，如果两个哈希值不等，但指向同一个桶的话，较大的那个会插入到右子树里。如果哈希值相等，HashMap希望key值最好是实现了Comparable接口的，这样它可以按照顺序来进行插入。这对HashMap的key来说并不是必须的，不过如果实现了当然最好。如果没有实现这个接口，在出现严重的哈希碰撞的时候，你就并别指望能获得性能提升了。

java中length,length(),size()区别
-----------------------------
java中的length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了length这个属性.
java中的length()方法是针对字符串String说的,如果想看这个字符串的长度则用到length()这个方法，需要注意的是，它计算长度是unicode编码格式的，也就是说1个中文字符算1个，而不是2个。
下面是string类的length()方法的源码：
```java
 /**
     * Returns the length of this string.
     * The length is equal to the number of <a href="Character.html#unicode">Unicode
     * code units</a> in the string.
     *
     * @return  the length of the sequence of characters represented by this
     *          object.
     */
    public int length() {
        return value.length;
    }
```
java中的size()方法是针对泛型集合（Map，Collection）说的,如果想看这个泛型有多少个元素,就调用此方法来查看
map接口中的size()源码：
```java
    /**
     * Returns the number of key-value mappings in this map.  If the
     * map contains more than <tt>Integer.MAX_VALUE</tt> elements, returns
     * <tt>Integer.MAX_VALUE</tt>.
     *
     * @return the number of key-value mappings in this map
     */
    int size();
```
collection接口中的size()源码：
```java
/**
     * Returns the number of elements in this collection.  If this collection
     * contains more than <tt>Integer.MAX_VALUE</tt> elements, returns
     * <tt>Integer.MAX_VALUE</tt>.
     *
     * @return the number of elements in this collection
     */
    int size();
```

进程与线程 区别
--------
**进程**：cpu资源分配的最小单位，拥有独立的内存单元，可以拥有一个或者多个线程。多个线程共享内存。共享复杂，但是同步简单，需要使用IPC（Inter-Process Communication）。
**线程**：cpu调度的最小单位，线程间共享进程的资源，共享简单，但是同步复杂，需要加锁。
**同一进程间的线程究竟共享哪些资源呢，而又各自独享哪些资源呢？**

 - 共享的资源有

**a. 堆**  由于堆是在进程空间中开辟出来的，所以它是理所当然地被共享的；因此new出来的都是共享的（16位平台上分全局堆和局部堆，局部堆是独享的）
**b. 全局变量** 它是与具体某一函数无关的，所以也与特定线程无关；因此也是共享的
**c. 静态变量** 虽然对于局部变量来说，它在代码中是“放”在某一函数中的，但是其存放位置和全局变量一样，存于堆中开辟的.bss和.data段，是共享的
**d. 文件等公用资源**  这个是共享的，使用这些公共资源的线程必须同步。Win32 提供了几种同步资源的方式，包括信号、临界区、事件和互斥体。

 - 独享的资源有

**a. 栈** 栈（虚拟机栈）是独享的
**b. 寄存器**  这个可能会误解，因为电脑的寄存器是物理的，每个线程去取值难道不一样吗？其实线程里存放的是副本，包括程序计数器PC
**c.程序计数器**它可以看做是当前线程所执行的字节码的行号指示器。
进程间通信方式
---------

 - 管道(pipe)

    管道是一种具有两个端点的通信通道，一个管道实际上就是只存在在内存中的**文件**，**对这个文件操作需要两个已经打开文件进行，他们代表管道的两端，也叫两个句柄**，管道是一种特殊的文件，不属于一种文件系统，而是一种独立的文件系统，有自己的数据结构，根据管道的使用范围划分为无名管道和命名管道。
**无名管道**用于父进程和子进程之间，通常父进程创建管道，然后由通信的子进程继承父进程的读端点句柄和写端点句柄，或者父进程有读写句柄的子进程，这些子进程可以使用管道直接通信，不需要通过父进程。
**命名管道**，命名管道是为了解决无名管道只能在父子进程间通信而设计的，命名管道是建立在实际的磁盘介质或文件系统(而不是只存在内存中)，任何进程可以通过文件名或路径建立与该文件的联系，命名管道需要一种FIFO文件(有先进先出的原则)，虽然FIFO文件的inode节点在磁盘上，但仅是一个节点而已，文件的数据还是存在于内存缓冲页面中，和普通管道相同。

 - 信号量( semophore ) ：

 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

 - 消息队列( message queue ) ：

 消息队列是消息的链表，包括Posix消息队列和system v消息队列(Posix常用于线程，system常用于进程)，有权限的进程可以向消息队列中添加消息，有读权限的进程可以读走消息队列的消息。
消息队列克服了信号承载信息量少，管道只能承载无格式字节流及缓冲区大小受限等缺陷。

 - 共享内存( shared memory )

 ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是**最快**的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。传递文件最好用共享内存的方式。 
套接字( socket ) ： 套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于**不同机器**间的进程通信。

多线程之生产者消费者
----------
注意，下面的代码中的if判断，应该改为while循环。
下面的代码创建了2个消费者线程，这样会出现数据越界的错误，就是因为没有使用while的原因。
```java
/*
    生产和消费
*/
package multiThread;

class SynStack 
{
    private char[] data = new char[6];
    private int cnt = 0; //表示数组有效元素的个数
    
    public synchronized void push(char ch)
    {
        if (cnt >= data.length)
        {
            try
            {
                System.out.println("生产线程"+Thread.currentThread().getName()+"准备休眠");
                this.wait();
                System.out.println("生产线程"+Thread.currentThread().getName()+"休眠结束了");
            }
            catch (Exception e)
            {
                e.printStackTrace();
            }
        }
        this.notify(); 
        data[cnt] = ch;
        ++cnt;
        System.out.printf("生产线程"+Thread.currentThread().getName()+"正在生产第%d个产品，该产品是: %c\n", cnt, ch);
    }
    
    public synchronized char pop()
    {
        char ch;
        if (cnt <= 0)
        {
            try
            {
                System.out.println("消费线程"+Thread.currentThread().getName()+"准备休眠");
                this.wait();
                System.out.println("消费线程"+Thread.currentThread().getName()+"休眠结束了");
            }
            catch (Exception e)
            {
                e.printStackTrace();
            }
        }
        this.notify();
        ch = data[cnt-1];
        System.out.printf("消费线程"+Thread.currentThread().getName()+"正在消费第%d个产品，该产品是: %c\n", cnt, ch);
        --cnt;
        return ch;        
    }    
}

class Producer implements Runnable
{
    private SynStack ss = null;
    public Producer(SynStack ss)
    {
        this.ss = ss;
    }
    
    public void run()
    {
        char ch;
        for (int i=0; i<10; ++i)
        {
//            try{
//            Thread.sleep(100);
//            }
//            catch (Exception e){            
//            }
                
            ch = (char)('a'+i);
            ss.push(ch);
        }
    }
}

class Consumer implements Runnable
{
    private SynStack ss = null;
    
    public Consumer(SynStack ss)
    {
        this.ss = ss;
    }
    
    public void run()
    {
        for (int i=0; i<10; ++i)
        {
            /*try{
            Thread.sleep(100);
            }
            catch (Exception e){            
            }*/
            
            //System.out.printf("%c\n", ss.pop());
            ss.pop();
        }
    }
}


public class TestPC2
{
    public static void main(String[] args)
    {
        SynStack ss = new SynStack();
        Producer p = new Producer(ss);
        Consumer c = new Consumer(ss);
        
        
        Thread t1 = new Thread(p);
        t1.setName("1号");
        t1.start();
        /*Thread t2 = new Thread(p);
        t2.setName("2号");
        t2.start();*/
                
        Thread t6 = new Thread(c);
        t6.setName("6号");
        t6.start();
        Thread t7 = new Thread(c);
        t7.setName("7号");
        t7.start();
    }
}
```

Java中的访问控制符号
------------
需要特别说明“无修饰符”这个情况，子类能否访问父类中无修饰符的变量/方法，取决于**子类的位置**。如果**子类和父类在同一个包**中，那么子类可以访问父类中的无修饰符的变量/方法，否则不行。
```java
/*
		 * ----------
           			private default protected   public
		----------
		同一个类    	√       √       √           √
		----------	
		同一个包    			√		√			√
		----------
		子类							√			√
		----------
		全局										√
		----------
		 */
```


Java中set的底层实现
-------------
Set是对数学上集的抽象,Set中不包含重复的元素.如何界定是否是重复元素?Set最多可含一个null元素;对于任意的非null元素e1和e2,都满足e1.equals(e2)==false.
**Set实现**
HashSet是使用一个哈希表存储元素的,是非排序的,可以随机访问,是Set的最优性能实现.TreeSet实现了treeMap接口,使用一个红黑树（如果出现相等的key，那么新的元素直接替换旧的元素）来存储元素,提供了元素的有序存储和访问.
```java
// Dummy value to associate with an Object in the backing Map
// 意思就是为了和hashmap的key-value方式保持一致，set这里假定了一个value值，没有实际意义
    private static final Object PRESENT = new Object();
/**
     * Constructs a new, empty set; the backing <tt>HashMap</tt> instance has
     * default initial capacity (16) and load factor (0.75).
     *   可以看出hashset的底层是使用hashmap来实现的
     */
    public HashSet() {
        map = new HashMap<>();
    }

    /**
     * Constructs a new set containing the elements in the specified
     * collection.  The <tt>HashMap</tt> is created with default load factor
     * (0.75) and an initial capacity sufficient to contain the elements in
     * the specified collection.
     *
     * @param c the collection whose elements are to be placed into this set
     * @throws NullPointerException if the specified collection is null
     * 注意下面构造函数的+1操作，因为是int格式转换的时候浮点数是向下转的，也就是说6.66f会转为6
     */
    public HashSet(Collection<? extends E> c) {
        map = new HashMap<>(Math.max((int) (c.size()/.75f) + 1, 16));
        addAll(c);
    }

    /**
     * Constructs a new, empty set; the backing <tt>HashMap</tt> instance has
     * the specified initial capacity and the specified load factor.
     *
     * @param      initialCapacity   the initial capacity of the hash map
     * @param      loadFactor        the load factor of the hash map
     * @throws     IllegalArgumentException if the initial capacity is less
     *             than zero, or if the load factor is nonpositive
     */
    public HashSet(int initialCapacity, float loadFactor) {
        map = new HashMap<>(initialCapacity, loadFactor);
    }

    /**
     * Constructs a new, empty set; the backing <tt>HashMap</tt> instance has
     * the specified initial capacity and default load factor (0.75).
     *
     * @param      initialCapacity   the initial capacity of the hash table
     * @throws     IllegalArgumentException if the initial capacity is less
     *             than zero
     */
    public HashSet(int initialCapacity) {
        map = new HashMap<>(initialCapacity);
    }

    /**
     * Constructs a new, empty linked hash set.  (This package private
     * constructor is only used by LinkedHashSet.) The backing
     * HashMap instance is a LinkedHashMap with the specified initial
     * capacity and the specified load factor.
     *
     * @param      initialCapacity   the initial capacity of the hash map
     * @param      loadFactor        the load factor of the hash map
     * @param      dummy             ignored (distinguishes this
     *             constructor from other int, float constructor.)
     * @throws     IllegalArgumentException if the initial capacity is less
     *             than zero, or if the load factor is nonpositive
     * 下满函数是default访问权限的，作域在当前类或者同包中
     */
    HashSet(int initialCapacity, float loadFactor, boolean dummy) {
        map = new LinkedHashMap<>(initialCapacity, loadFactor);
    }
```
由于set要求内部所有的元素都不重复，那么我们就来看一下它的contains函数和add函数以及remove函数的实现过程。
```java
/**
     * Returns <tt>true</tt> if this set contains the specified element.
     * More formally, returns <tt>true</tt> if and only if this set
     * contains an element <tt>e</tt> such that
     * <tt>(o==null&nbsp;?&nbsp;e==null&nbsp;:&nbsp;o.equals(e))</tt>.
     *
     * @param o element whose presence in this set is to be tested
     * @return <tt>true</tt> if this set contains the specified element
     * 这里contains方法调用了hashmap的containsKey方法，因为hashmap的key总是唯一的，不存在重复的情况。
     */
    public boolean contains(Object o) {
        return map.containsKey(o);
    }

    /**
     * Adds the specified element to this set if it is not already present.
     * More formally, adds the specified element <tt>e</tt> to this set if
     * this set contains no element <tt>e2</tt> such that
     * <tt>(e==null&nbsp;?&nbsp;e2==null&nbsp;:&nbsp;e.equals(e2))</tt>.
     * If this set already contains the element, the call leaves the set
     * unchanged and returns <tt>false</tt>.
     *
     * @param e element to be added to this set
     * @return <tt>true</tt> if this set did not already contain the specified
     * element
     * 如果元素存在，那么不对set做任何更改，并且返回false
     */
    public boolean add(E e) {
        return map.put(e, PRESENT)==null;
    }

    /**
     * Removes the specified element from this set if it is present.
     * More formally, removes an element <tt>e</tt> such that
     * <tt>(o==null&nbsp;?&nbsp;e==null&nbsp;:&nbsp;o.equals(e))</tt>,
     * if this set contains such an element.  Returns <tt>true</tt> if
     * this set contained the element (or equivalently, if this set
     * changed as a result of the call).  (This set will not contain the
     * element once the call returns.)
     *
     * @param o object to be removed from this set, if present
     * @return <tt>true</tt> if the set contained the specified element
     * map.remove函数如果成功remove一个存在的元素，那么会返回remove的元素的value 
     */
    public boolean remove(Object o) {
        return map.remove(o)==PRESENT;
    }
```
下面看下TreeSet的实现过程，从源码中可以看出treeset的实现是通过treemap来实现的,TreeMap是NavigableMap的子类,NavigableMap是SortedMap的子类，SortedMap实现了Map接口  
下面是源码：
```java
/**
     * Constructs a set backed by the specified navigable map.
     *  TreeMap是NavigableMap的子类
     */
    TreeSet(NavigableMap<E,Object> m) {
        this.m = m;
    }

    /**
     * Constructs a new, empty tree set, sorted according to the
     * natural ordering of its elements.  All elements inserted into
     * the set must implement the {@link Comparable} interface.
     * Furthermore, all such elements must be <i>mutually
     * comparable</i>: {@code e1.compareTo(e2)} must not throw a
     * {@code ClassCastException} for any elements {@code e1} and
     * {@code e2} in the set.  If the user attempts to add an element
     * to the set that violates this constraint (for example, the user
     * attempts to add a string element to a set whose elements are
     * integers), the {@code add} call will throw a
     * {@code ClassCastException}.
     */
    public TreeSet() {
        this(new TreeMap<E,Object>());
    }

    /**
     * Constructs a new, empty tree set, sorted according to the specified
     * comparator.  All elements inserted into the set must be <i>mutually
     * comparable</i> by the specified comparator: {@code comparator.compare(e1,
     * e2)} must not throw a {@code ClassCastException} for any elements
     * {@code e1} and {@code e2} in the set.  If the user attempts to add
     * an element to the set that violates this constraint, the
     * {@code add} call will throw a {@code ClassCastException}.
     *
     * @param comparator the comparator that will be used to order this set.
     *        If {@code null}, the {@linkplain Comparable natural
     *        ordering} of the elements will be used.
     */
    public TreeSet(Comparator<? super E> comparator) {
        this(new TreeMap<>(comparator));
    }

    /**
     * Constructs a new tree set containing the elements in the specified
     * collection, sorted according to the <i>natural ordering</i> of its
     * elements.  All elements inserted into the set must implement the
     * {@link Comparable} interface.  Furthermore, all such elements must be
     * <i>mutually comparable</i>: {@code e1.compareTo(e2)} must not throw a
     * {@code ClassCastException} for any elements {@code e1} and
     * {@code e2} in the set.
     *
     * @param c collection whose elements will comprise the new set
     * @throws ClassCastException if the elements in {@code c} are
     *         not {@link Comparable}, or are not mutually comparable
     * @throws NullPointerException if the specified collection is null
     */
    public TreeSet(Collection<? extends E> c) {
        this();
        addAll(c);
    }

    /**
     * Constructs a new tree set containing the same elements and
     * using the same ordering as the specified sorted set.
     *
     * @param s sorted set whose elements will comprise the new set
     * @throws NullPointerException if the specified sorted set is null
     */
    public TreeSet(SortedSet<E> s) {
        this(s.comparator());
        addAll(s);
    }
```

为什么需要重写hashcode方法与equals方法
--------------------------
这个主要是考虑到java集合中的set特性，它要求元素不重复，那么需要hashcode和equals都相等才可以。这里需要注意，hashset的底层也是使用hashmap来实现的，hashmap在put元素的时候，就需要先判断hashcode，如果相等，再判断equals，如果还相等，那么证明元素相等，就进行**替换**。
另外，如果不进行重写，对于我们自定义的对象，程序也不知道我们判断两个对象相等的条件是（比如People对象，身份证号码（ID）相等就是相等），所以必须进行重写，否则根本无法判断对象是否重复。
看一下Object.hashCode的通用约定（摘自《Effective Java》第45页）
    

 - 在一个应用程序执行期间，如果一个对象的equals方法做比较所用到的信息没有被修改的话，那么，对该对象调用hashCode方法多次，它必须始终如一地返回 同一个整数。在同一个应用程序的多次执行过程中，这个整数可以不同，即这个应用程序这次执行返回的整数与下一次执行返回的整数可以不一致。
    

 - 如果两个对象根据equals(Object)方法是相等的，那么调用这两个对象中任一个对象的hashCode方法必须产生同样的整数结果。
  

 - 如果两个对象根据equals(Object)方法是不相等的，那么调用这两个对象中任一个对象的hashCode方法，不要求必须产生不同的整数结果。然而，程序员应该意识到这样的事实，对于不相等的对象产生截然不同的整数结果，有可能提高散列表（hash
   table）的性能。

     如果只重写了equals方法而没有重写hashCode方法的话，则会违反约定的第二条：相等的对象必须具有相等的散列码（hashCode）
     同时对于HashSet和HashMap这些基于散列值（hash）实现的类。HashMap的底层处理机制是以数组的方法保存放入的数据的(Node<K,V>[] table)，其中的关键是数组下标的处理。数组的下标是根据传入的元素hashCode方法的返回值再和特定的值异或决定的。如果该数组位置上已经有放入的值了，且传入的键值相等则不处理，若不相等则覆盖原来的值，如果数组位置没有条目，则插入，并加入到相应的链表中。检查键是否存在也是根据hashCode值来确定的。所以如果不重写hashCode的话，可能导致HashSet、HashMap不能正常的运作、
  如果我们将某个自定义对象存到HashMap或者HashSet及其类似实现类中的时候，如果该对象的属性参与了hashCode的计算，那么就不能修改该对象参数hashCode计算的属性了。有可能会移除不了元素，导致内存泄漏。

Java的深复制浅复制
-----------
參考文章http://blog.csdn.net/zhangjg_blog/article/details/18369201
 - 浅复制
对于简单对象，都是浅复制。对于引用对象，浅复制就是栈指针指向堆中同一地址空间。
 - 深复制
 对于堆中的对象，在堆中产生一份相同的拷贝，并在栈中建立一个指向这个地址空间的引用。
Java当中，一般可以通过new关键字或者clone方法创建或者复制一个对象。
程序执行到new操作符时，首先去看new操作符后面的类型，因为知道了类型，才能知道要分配多大的内存空间。分配完内存之后，再调用构造函数，填充对象的各个域，这一步叫做对象的初始化，构造方法返回后，一个对象创建完毕，可以把他的引用（地址）发布到外部，在外部就可以使用这个引用操纵这个对象。
而clone在第一步是和new相似的，都是分配内存，调用clone方法时，分配的内存和源对象（即调用clone方法的对象）相同，然后再使用原对象中对应的各个域，填充新对象的域，填充完成之后，clone方法返回，一个新的相同的对象被创建，同样可以把这个新对象的引用发布到外部。 
下面展示一个完全深拷贝的例子，尤其要注意类中嵌套类（区别于简单对象）的形式。
```java
package com.audi;

public class TestClone
{
	static class Body implements Cloneable
	{
		public Head head;

		public Body()
		{
		}

		public Body(Head head)
		{
			this.head = head;
		}

		@Override
		protected Object clone() throws CloneNotSupportedException
		{
			Body newBody = (Body) super.clone();
			newBody.head = (Head) head.clone();
			return newBody;
		}

	}

	static class Head implements Cloneable
	{
		public Face face;

		public Head()
		{
		}

		public Head(Face face)
		{
			this.face = face;
		}

		@Override
		protected Object clone() throws CloneNotSupportedException
		{
			//return super.clone();
			
			// 实现完全的深拷贝
			Head newHead = (Head) super.clone();  
	        newHead.face = (Face) this.face.clone();  
	        return newHead;
		}
	}

	// static class Face{}

	// 为了实现完全的深拷贝  所以Face类也需要实现cloneable接口
	static class Face implements Cloneable
	{
		@Override
		protected Object clone() throws CloneNotSupportedException
		{
			return super.clone();
		}
	}

	public static void main(String[] args) throws CloneNotSupportedException
	{
		Body body = new Body(new Head(new Face()));
		Body body1 = (Body) body.clone();

		System.out.println("body == body1 : " + (body == body1));
		System.out.println("body.head == body1.head : " + (body.head == body1.head));
		System.out.println("body.head.face == body1.head.face : " + (body.head.face == body1.head.face));
	}
}

```

string类的源码分析，为什么是不可变的
---------------------
string类之所以是immutable的，是因为value字符数组它一旦初始化，就无法更改，且没有提供相应的set方法来修改。
```java
/** The value is used for character storage. */
    private final char value[];
```
那么string类的replace方法是如何进行替换的呢？源代码中很明显的指出：如果存在可以替换的字符，那么就替换字符，并且使用string的构造函数新建一个对象进行返回；
如果不存在替换的字符，那么直接返回原字符串对象。
上源码：
```java
/**
     * Returns a string resulting from replacing all occurrences of
     * {@code oldChar} in this string with {@code newChar}.
     * <p>
     * If the character {@code oldChar} does not occur in the
     * character sequence represented by this {@code String} object,
     * then a reference to this {@code String} object is returned.
     * Otherwise, a {@code String} object is returned that
     * represents a character sequence identical to the character sequence
     * represented by this {@code String} object, except that every
     * occurrence of {@code oldChar} is replaced by an occurrence
     * of {@code newChar}.
     * <p>
     * Examples:
     * <blockquote><pre>
     * "mesquite in your cellar".replace('e', 'o')
     *         returns "mosquito in your collar"
     * "the war of baronets".replace('r', 'y')
     *         returns "the way of bayonets"
     * "sparring with a purple porpoise".replace('p', 't')
     *         returns "starring with a turtle tortoise"
     * "JonL".replace('q', 'x') returns "JonL" (no change)
     * </pre></blockquote>
     *
     * @param   oldChar   the old character.
     * @param   newChar   the new character.
     * @return  a string derived from this string by replacing every
     *          occurrence of {@code oldChar} with {@code newChar}.
     */
    public String replace(char oldChar, char newChar) {
        if (oldChar != newChar) {
            int len = value.length;
            int i = -1;
            char[] val = value; /* avoid getfield opcode */

            while (++i < len) {
                if (val[i] == oldChar) {
                    break;
                }
            }
            if (i < len) {
                char buf[] = new char[len];
                for (int j = 0; j < i; j++) {
                    buf[j] = val[j];
                }
                while (i < len) {
                    char c = val[i];
                    buf[i] = (c == oldChar) ? newChar : c;
                    i++;
                }
                return new String(buf, true);
            }
        }
        return this;
    }
```
对于不可变对象，如果使用反射的方法，是可以改变某些值的。

quartz的任务调度怎么进行的以及如何选择节点来执行定时任务
-------------------------------
下面是spring中一种典型的quartz集成配置方式
```xml
<!------------ 配置调度程序quartz ，其中配置JobDetail有两种方式-------------->    
    <!--方式一：使用JobDetailBean，任务类必须实现Job接口 -->     
    <bean id="myjob" class="org.springframework.scheduling.quartz.JobDetailBean">    
     <property name="name" value="exampleJob"></property>    
     <property name="jobClass" value="com.ncs.hj.SpringQtz"></property>   
     <property name="jobDataAsMap">  
<map>  
    <entry key="service"><value>simple is the beat</value></entry>  
</map>  
;/property>  
    </bean>   
    <!--运行时请将方式一注释掉！ -->    
    <!-- 方式二：使用MethodInvokingJobDetailFactoryBean，任务类可以不实现Job接口，通过targetMethod指定调用方法-->    
    <!-- 定义目标bean和bean中的方法 -->  
    <!-- SpringQtzJob就是定时任务到的时候需要调用知道的bean，execute就是这个bean内部的一个方法 -->  
    <bean id="SpringQtzJob" class="com.ncs.hj.SpringQtz"/>  
    <bean id="SpringQtzJobMethod" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean">  
    <property name="targetObject">  
        <ref bean="SpringQtzJob"/>  
    </property>  
    <property name="targetMethod">  <!-- 要执行的方法名称 -->  
        <value>execute</value>  
    </property>  
</bean>  
  
<!-- ======================== 调度触发器 ======================== -->  
<bean id="CronTriggerBean" class="org.springframework.scheduling.quartz.CronTriggerBean">  
    <property name="jobDetail" ref="SpringQtzJobMethod"></property>  
    <property name="cronExpression" value="0/5 * * * * ?"></property>  
</bean>  
  
<!-- ======================== 调度工厂 ======================== -->  
<bean id="SpringJobSchedulerFactoryBean" class="org.springframework.scheduling.quartz.SchedulerFactoryBean">  
    <property name="triggers">  
        <list>  
            <ref bean="CronTriggerBean"/>  
        </list>  
    </property>  
</bean>    
```

任务调度的机制
-------
参考链接：https://www.cnblogs.com/zhangchengzhangtuo/p/5705672.html
在这里将几个重要的类调用的过程以序列图的形式展现出来，上半部分展现的是启动过程，下半部分展现的是任务调度的过程。

步骤1.用户首先需要生成一个调度器工厂SchedulerFactory，可以用下面的方式实现自己的定制化：
步骤2.然后通过getScheduler()方法从调度器工厂里得到调度器实例，首先查找有没有这样的调度器，没有的话，就生成一个，有的话直接返回。所以得到的一般是单例，即默认的调度器。
步骤3.Scheduler有一个QuartzSchedulerThread（Thread的子类）属性，在scheduler实例化的时候，实例化了一个对象，并用ThreadExecutor启动该线程对象。该线程就是调度线程，主要任务就是不停的从JobStore中获取即将被触发的触发器（默认30s调度一次）。在这个时候调度线程虽然启动，但是处于pause状态。
步骤4.接下来是任务调度的部分：
client通过scheduleJob()方法将任务和触发器存储在JobStore中，通过start()方法将QuartzSchedulerThread的pause状态设为false，通知调度线程执行任务，此后调度线程不停的从JobStore中去取即将触发的任务。  
上半部分展现的是任务执行之前准备工作的时序，下半部分展现的是任务执行的时序。

步骤1.调度线程首先去线程池中获取可用的线程，如果没有的话，就阻塞。
步骤2.从JobStore(从存储介质中获取触发器，存储介质可以是内存也可以是数据库)获取（接下来30s内的）触发器，然后等待该触发器触发。
步骤3.调度线程创建一个JobRunShell(就是一个Runnable)，然后从线程池中调用线程执行该任务。
接下来就是任务执行的时序：
步骤4.获取trigger、JobDetail以及生成Job实例，然后执行job的execute接口函数。

sql优化、什么情况导致不走索引
----------------
主要原则就是应尽量避免全表扫描，应该考虑在where及order by 涉及的列上建立索引。

 - 一个表的索引不是越多越好，也没有一个具体的数字，根据以往的经验，一个表的索引最多不能超过**6个**，因为索引越多，对update和insert操作也会有性能的影响，涉及到索引的新建和重建操作。
 - 建立索引的方法论为：
1、多数查询经常使用的列；
2、很少进行修改操作的列；
3、索引需要建立在数据差异化大的列上
 - 对应的sql优化的方法有
1、建立复合索引；
2、like语句优化，比如下面的语句，前后都加了%，该查询必然会走全表扫描
```sql
SELECT id FROM A WHERE name like '%abc%'
```
3、在where子句中使用 ！= 或 <>操作符，索引将被放弃使用，会进行全表查询。
4、in和not in 也要慎用，否则也会导致全表扫描。

　　 方案一：between替换in

　　 如SQL:SELECT id FROM A WHERE num in(1,2,3) 优化成：SELECT id FROM A WHERE num between 1 and 3

　　 方案二：exist替换in

　　 如SQL:SELECT id FROM A WHERE num in(select num from b ) 优化成：SELECT num FROM A WHERE num exists(select 1 from B where B.num = A.num)

　　 方案三：left join替换in

　　 如SQL:SELECT id FROM A WHERE num in(select num from B) 优化成：SELECT id FROM A LEFT JOIN B ON A.num = B.num
5、不要在where子句中的“=”**左边**进行函数、算数运算或其他表达式运算，否则系统将可能无法正确使用索引。

　　 如SQL:SELECT id FROM A WHERE num/2 = 100 优化成：SELECT id FROM A WHERE num = 100*2

　　 如SQL:SELECT id FROM A WHERE substring(name,1,3) = 'abc' 优化成：SELECT id FROM A WHERE LIKE 'abc%'

　　 如SQL:SELECT id FROM A WHERE datediff(day,createdate,'2016-11-30')=0 优化成：SELECT id FROM A WHERE createdate>='2016-11-30' and createdate<'2016-12-1'

　　 如SQL:SELECT id FROM A WHERE year(addate) <2016 优化成：SELECT id FROM A where addate<'2016-01-01'
6、任何地方都不要用 select * from table ，用具体的字段列表替换"*"，不要返回用不到的字段　
7、使用“临时表”暂存中间结果
8、limit分页优化
mysql的limit函数语法：
```sql
SELECT * FROM table  LIMIT [offset,] rows | rows OFFSET offset
```
当偏移量特别时，limit效率会非常低

　　　　SELECT id FROM A LIMIT 1000,10   很快

　　　　SELECT id FROM A LIMIT 90000,10 很慢

　　　　优化方法：

　　　　方法一：select id from A **order by** id limit 90000,10; 很快，0.04秒就OK。 因为用了id主键做索引当然快

　　　　方法二：select id,title from A where id>=(select id from collect order by id limit 90000,1) limit 10;

     　　 方法三：select id from A order by id  between 10000000 and 10000010;
9、尽量不要使用 BY RAND()命令
BY RAND()是随机显示结果，这个函数可能会为表中每一个独立的行执行BY RAND()命令，这个会消耗处理器的处理能力。
10、排序的索引问题　
Mysql查询**只是用一个索引**，因此如果where子句中**已经**使用了索引的话，那么order by中的列是**不会使用索引**的。因此数据库默认排序可以符合要求情况下不要使用排序操作；
尽量不要包含多个列的排序，如果需要最好给这些列**创建复合索引**。
11、尽量用 union add 替换 union
union和union all的差异主要是前者需要将两个（或者多个）结果集合并后再进行唯一性过滤操作，这就会涉及到排序，增加大量的cpu运算，加大资源消耗及延迟。所以当我们可以确认不可能出现重复结果集或者不在乎重复结果集的时候，尽量使用union all而不是union
12、Inner join 和 left join、right join、子查询
第一：inner join内连接也叫等值连接是，left/rightjoin是外连接。

　　　　 SELECT A.id,A.name,B.id,B.name FROM A LEFT JOIN B ON A.id =B.id;

　　　　 SELECT A.id,A.name,B.id,B.name FROM A RIGHT JOIN ON B A.id= B.id;

　　　　 SELECT A.id,A.name,B.id,B.name FROM A INNER JOIN ON A.id =B.id;

　　 　　经过来自多方面的证实inner join性能比较快，因为inner join是等值连接，或许返回的行数比较少。但是我们要记得有些语句隐形的用到了等值连接，如：

 　　　　SELECT A.id,A.name,B.id,B.name FROM A,B WHERE A.id = B.id;

 　　　　推荐：能用inner join连接尽量使用inner join连接

　　 第二：**子查询的性能又比外连接性能慢**，尽量用外连接来替换子查询。

　　　　Select* from A where exists (select * from B where id>=3000 and A.uuid=B.uuid);

　　　　A表的数据为十万级表，B表为百万级表，在本机执行差不多用2秒左右，我们可以通过explain可以查看到子查询是一个相关子查询(DEPENDENCE SUBQUERY);Mysql是先对外表A执行全表查询，然后根据uuid逐次执行子查询，如果外层表是一个很大的表，我们可以想象查询性能会表现比这个更加糟糕。

      　　一种简单的优化就是用innerjoin的方法来代替子查询，查询语句改为：

  　　　Select* from A inner join B ON A.uuid=B.uuid using(uuid) where b.uuid>=3000;  这个语句执行测试不到一秒；

　　第三：使用JOIN时候，应该用小的结果驱动大的结果（left join 左边表结果尽量小，如果有条件应该放到左边先处理，right join同理反向），同时尽量把牵涉到多表联合的查询拆分多个query	(多个表查询效率低，容易锁表和阻塞)。如：

	　　Select * from A left join B A.id=B.ref_id where  A.id>10;可以以优化为：
	　　select * from (select * from A wehre id >10) T1 left join B on T1.id=B.ref_id;
13、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描
select id from t where num is null 
可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： 
select id from t where num=0 
14、尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。
15、尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 
16、尽量避免大事务操作，提高系统并发能力。



**所以，sql查询不走索引大致可以分为如下几个原因：**

 - 使用了不等号<>作为查询条件
 - 使用了null作为查询的判断条件，形如is null
 - 使用了in作为查询条件（不是绝对的，部分in条件是可以走索引的）
 - 查询条件中的“=”左边进行了运算
 - like查询前都使用了%，形如like '%abc'，但是like 'abc%'还是可以走索引的
 - 使用复合索引时，索引的第一个字段没有作为查询条件
 

navicat查看mysql执行计划各参数意义
-----------------------

 1. ID：Query Optimizer 所选定的执行计划中查询的序列号；
 2. Select_type：所使用的查询类型，主要有以下这几种查询类型
 ◇ DEPENDENT SUBQUERY：子查询中内层的第一个SELECT，依赖于外部查询的结果集；
◇ DEPENDENT UNION：子查询中的UNION，且为UNION 中从第二个SELECT 开始的后面所有SELECT，同样依赖于外部查询的结果集；
◇ PRIMARY：子查询中的最外层查询，注意并不是主键查询；
◇ SIMPLE：除子查询或者UNION 之外的其他查询；
◇ SUBQUERY：子查询内层查询的第一个SELECT，结果不依赖于外部查询结果集；
◇ UNCACHEABLE SUBQUERY：结果集无法缓存的子查询；
◇ UNION：UNION 语句中第二个SELECT 开始的后面所有SELECT，第一个SELECT 为PRIMARY
◇ UNION RESULT：UNION 中的合并结果；
 3. Table：显示这一步所访问的数据库中的表的名称；
 4. Type：告诉我们对表所使用的访问方式，主要包含如下集中类型：
 ◇ all：全表扫描
◇ const：读常量，且最多只会有一条记录匹配，由于是常量，所以实际上只需要读一次；
◇ eq_ref：最多只会有一条匹配结果，一般是通过主键或者唯一键索引来访问；
◇ fulltext：
◇ index：全索引扫描；
◇ index_merge：查询中同时使用两个（或更多）索引，然后对索引结果进行merge 之后再读
取表数据；
◇ index_subquery：子查询中的返回结果字段组合是一个索引（或索引组合），但不是一个
主键或者唯一索引；
◇ rang：索引范围扫描；
◇ ref：Join 语句中被驱动表索引引用查询；
◇ ref_or_null：与ref 的唯一区别就是在使用索引引用查询之外再增加一个空值的查询；
◇ system：系统表，表中只有一行数据；
◇ unique_subquery：子查询中的返回结果字段组合是主键或者唯一约束；
 5. Possible_keys：该查询可以利用的索引. 如果没有任何索引可以使用，就会显示成null，这一项内容对于优化时候索引的调整非常重要；
 6. Key：MySQL Query Optimizer 从possible_keys 中所选择使用的索引（**注意**，mysql默认一次查询只会走一个索引）；
 7. Key_len：被选中使用索引的索引键长度；
 8. Ref：列出是通过常量（const），还是某个表的某个字段（如果是join）来过滤（通过key）的；
 9. Rows：MySQL Query Optimizer 通过系统收集到的统计信息估算出来的结果集记录条数；
 10. Extra：查询中每一步实现的额外细节信息，主要可能会是以下内容：
 ◇ Distinct：查找distinct 值，所以当mysql 找到了第一条匹配的结果后，将停止该值的查询而转为后面其他值的查询；
◇ Full scan on NULL key：子查询中的一种优化方式，主要在遇到无法通过索引访问null
值的使用使用；
◇ Impossible WHERE noticed after reading const tables：MySQL Query Optimizer 通过收集到的统计信息判断出不可能存在结果；
◇ No tables：Query 语句中使用FROM DUAL 或者不包含任何FROM 子句；
◇ Not exists：在某些左连接中MySQL Query Optimizer 所通过改变原有Query 的组成而使用的优化方法，可以部分减少数据访问次数；
◇ Range checked for each record (index map: N)：通过MySQL 官方手册的描述，当MySQL Query Optimizer没有发现好的可以使用的索引的时候，如果发现如果来自前面的表的列值已知，可能部分索引可以使用。对前面的表的每个行组合，MySQL 检查是否可以使用range 或index_merge 访问方法来索取行。
◇ Select tables optimized away：当我们使用某些聚合函数来访问存在索引的某个字段的
时候，MySQL Query Optimizer会通过索引而直接一次定位到所需的数据行完成整个查询。当然，前提是在Query 中不能有GROUP BY 操作。如使用MIN()或者MAX（）的时候；
◇ Using filesort：当我们的Query 中包含ORDER BY操作，而且无法利用索引完成排序操作的时候，MySQL Query Optimizer 不得不选择相应的排序算法来实现。
◇ Using index：所需要的数据只需要在Index即可全部获得而不需要再到表中取数据；
◇ Using index for group-by：数据访问和Using index一样，所需数据只需要读取索引即可，而当Query 中使用了GROUP BY 或者DISTINCT 子句的时候，如果分组字段也在索引中，Extra 中的信息就会是Using index for group-by；
◇ Using temporary：当MySQL在某些操作中必须使用临时表的时候，在Extra 信息中就会出现Using temporary 。主要常见于GROUP BY 和ORDER BY等操作中。
◇ Using where：如果我们不是读取表的所有数据，或者不是仅仅通过索引就可以获取所有需要的数据，则会出现Using where 信息；
◇ Using where with pushed condition：这是一个仅仅在NDBCluster 存储引擎中才会出现的信息，而且还需要通过打开Condition Pushdown 优化功能才可能会被使用。控制参数为engine_condition_pushdown 。
 

数据库事务正确执行的四个基本要素
----------------
包含：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）

 - 原子性

    整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。

 - 一致性
以转账案例为例，假设有五个账户，每个账户余额是100元，那么五个账户总额是500元，如果在这个5个账户之间同时发生多个转账，无论并发多少个，比如在A与B账户之间转账5元，在C与D账户之间转账10元，在B与E之间转账15元，五个账户总额也应该还是500元，这就是保护性和不变性

 - 隔离性
事物相互之间不产生影响。

 - 持久性
在事务完成commit以后，该事务对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。

数据库的隔离性的4个等级
------------
参考链接http://blog.csdn.net/a1324204785/article/details/53156414
数据库事物有4个等级，由低到高，分别是：
Read uncommitted、Read committed、Repeatable read、Serializable、MVCC（这个好像不常用），这五个级别可以逐个解决脏读、不可重复读、幻读这几类问题。

 1. Read uncommitted（读未提交，存在脏读）
 例子：错发工资，空欢喜
         小明 辛苦搬砖一个月，领导给他账户存4000，此时领导发工资事物未提交，但小明查看工资发现账户多了4000，满心欢喜，不幸的是领导发现工资算错了，回滚事务，修改金额2000，提交事务。最终小明工资只有2000，空欢喜一场。
**总结**：此例子中，发生了**脏读**。即A事物修改，未提交，B事物先读到，A事物在修改提交。
 2. Read committed（读已提交，存在不可重复读）
 例子：例子:还是小明小华销售员，余票3张，A来小华那里请求3张订票单，小华接受订单，要卖出3张票，上面的销售步骤执行中的时候，B也来小明那里买票，由于小华的销售事务执行到一半，小明事务没有看到小华的事务执行，读到的票数是3，准备接受订单的时候，小华的销售事务完成了，此时小明的系统变成显示0张票，小明刚想按下鼠标点击接受订单的手又连忙缩了回去。
**总结**：此例子中，发生了**不可重复读**。即事物A查询，事物B修改并提交，事物A读发现修改了。
**oracle默认**系统事务隔离级别是READ COMMITTED,也就是读已提交。
 3. Repeatable read （可重复读，存在幻读）
 例子：老婆查账，打印完账单发现变化
        小明老婆银行工作，经常查看小明消费记录，有一次查看当月消费记录为80，此时小明在外胡吃海喝消费1000，并提交事物，随后老婆打印账单发现消费变成1080,，非常诧异以为出现幻觉。
总结：事物A查询，事物B修改，事物A继续查询，结果变了。
**mysql默认**使用的就是就是这个隔离级别，使用select @@tx_isolation;命令可以进行查询
 4. Serializable（序列化，性能低）
 串行化事物，避免脏读、不可重复读、幻读，但是性能花费太高。
 5. MVCC（Multi-Version Concurrency Control 多版本并发控制 ）
在Java concurrent包中，有copyonwrite系列的类，专门用于优化读远大于写的情况。而其优化的手段就是，在进行写操作时，将数据copy一份，不会影响原有数据，然后进行修改，修改完成后原子替换掉旧的数据，而读操作只会读取原有数据。通过这种方式实现写操作不会阻塞读操作，从而优化读效率。而写操作之间是要互斥的，并且每次写操作都会有一次copy，所以只适合读大于写的情况。
MVCC的原理与copyonwrite类似，全称是Multi-Version Concurrent Control，即多版本并发控制。在MVCC协议下，每个读操作会看到一个一致性的snapshot，并且可以实现非阻塞的读。MVCC允许数据具有多个版本，这个版本可以是时间戳或者是全局递增的事务ID，在同一个时间点，不同的事务看到的数据是不同的。
现在来看看MySQL数据库为我们提供的四种隔离级别：

　　① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。

　　② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。

　　③ Read committed (读已提交)：可避免脏读的发生。

　　④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。
整理成表格的形式就是：
```java
		/*
		 * ----------
           				脏读 	不可重复读 	幻读
		----------
		Read uncommitted √       √           √           
		----------	
		Read committed     		 √		 	 √			
		----------
		Repeatable read 					 √			
		----------
		Serializable 						
		----------
		 */
```

幻读和不可重复读的区别
-----------

 - 不可重复读
 强调在同一个事物中，两次读取同一数据，产生的结果不一致。可能的原因就是当前事物在执行的过程中，另外一个事物对当前事物读取的数据进行了修改。
 - 幻读
 同样的查询条件，不同的事物中分别先后执行，得到的结果不一致。

Spring事务的传播特性和隔离级别
------------------
**事务的几种传播特性**
1. PROPAGATION_REQUIRED: 如果存在一个事务，则支持当前事务。如果没有事务则开启
2. PROPAGATION_SUPPORTS: 如果存在一个事务，支持当前事务。如果没有事务，则非事务的执行
3. PROPAGATION_MANDATORY: 如果已经存在一个事务，支持当前事务。如果没有一个活动的事务，则抛出异常。
4. PROPAGATION_REQUIRES_NEW: 总是开启一个新的事务。如果一个事务已经存在，则将这个存在的事务挂起。
5. PROPAGATION_NOT_SUPPORTED: 总是非事务地执行，并挂起任何存在的事务。
6. PROPAGATION_NEVER: 总是非事务地执行，如果存在一个活动事务，则抛出异常
7. PROPAGATION_NESTED：如果一个活动的事务存在，则运行在一个嵌套的事务中. 如果没有活动事务,则按TransactionDefinition.PROPAGATION_REQUIRED 属性执行
**Spring事务的隔离级别**
1. ISOLATION_DEFAULT： 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别.
下面的2\3\4\5与JDBC的隔离级别相对应
2. ISOLATION_READ_UNCOMMITTED： 这是事务最低的隔离级别，它充许另外一个事务可以看到这个事务未提交的数据。
      这种隔离级别会产生脏读，不可重复读和幻像读。
3. ISOLATION_READ_COMMITTED： 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据
4. ISOLATION_REPEATABLE_READ： 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。
      它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了避免下面的情况产生(不可重复读)。
5. ISOLATION_SERIALIZABLE 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。
      除了防止脏读，不可重复读外，还避免了幻像读。 

Servlet的作用，生命周期，如何创建、配置Servlet
------------------------------
**servlet是什么**
servlet是一个基于java技术的WEB组件，运行在服务器端，我们利用 sevlet可以很轻松的扩展WEB服务器的功能，使它满足特定的应用需要。servlet由servlet容器管理，servlet容器也叫servlet引擎，是servlet的运行环境，给发送的请求和响应之上提供网络服务
**Servlet的生命周期**
servlet的生命周期是指：servlet由创建到销毁的过程。
生命周期涉及几个方法：构造器，init，service，destroy。servlet在请求时创建
构造器方法：只在第一次访问时调用一次，说明servlet是单例的。
init方法：只会在第一次访问servlet时调用一次，对servlet对象进行初始化。
service方法：每次访问时都调用一次，业务逻辑写在这个方法里。
destroy方法：在项目卸载的时候调用一次，即关闭服务器的时候调用一次。

常用设计模式
------
参考链接http://blog.csdn.net/xsl1990/article/details/16359289
创建型模式主要有简单工厂模式（并不是23种设计模式之一）、**工厂方法**、抽象工厂模式、**单例模式**、生成器模式和原型模式。
结构型模式主要有**适配器模式adapter**、**桥接模式bridge**、组合器模式component、装饰器模式decorator、门面模式、亨元模式flyweight和**代理模式proxy**。
行为型模式主要有**命令模式command**、解释器模式、**迭代器模式**、中介者模式、备忘录模式、**观察者模式**、状态模式state、策略模式、模板模式和**访问者模式**。

 - 单例模式
 单例模式。构造函数是私有的，通过一个共有的成员函数还调用这个构造函数，在多线程环境下，还需要对这个成员函数进行加锁。
下面是4种单例的创建方式，最安全也最好的是第4种，使用内部类的方式
```java
// 1、懒汉式单例，线程不安全的
public class Singleton {  
    private static Singleton instance;  
    private Singleton (){}  
  
    public static Singleton getInstance() {  
    if (instance == null) {  
        instance = new Singleton();  
    }  
    return instance;  
    }  
}  

// 2、懒汉式单例，线程安全的，这种写法能够在多线程中很好的工作，而且看起来它也具备很好的lazy loading，但是，遗憾的是，效率很低，99%情况下不需要同步。
public class Singleton {  
    private static Singleton instance;  
    private Singleton (){}  
    public static synchronized Singleton getInstance() {  
    if (instance == null) {  
        instance = new Singleton();  
    }  
    return instance;  
    }  
}  

// 3、饿汉式，线程安全，但不能保证是懒加载的模式
// 这种方式基于classloder机制避免了多线程的同步问题，不过，instance在类装载时就实例化，虽然导致类装载的原因有很多种，在单例模式中大多数都是调用getInstance方法会导致类加载，此时就是lazy loading。 
//但是也不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化instance显然没有达到lazy loading的效果。
public class Singleton {  
    private static Singleton instance = new Singleton();  
    private Singleton (){}  
    public static Singleton getInstance() {  
    return instance;  
    }  
} 

//4、静态内部类方式
//这种方式同样利用了classloder的机制来保证初始化instance时只有一个线程，它跟第三种方式不同的是（很细微的差别）：第三种方式是只要Singleton类被装载了，那么instance就会被实例化（没有达到lazy loading效果）。
//而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显式装载SingletonHolder类，从而实例化instance。
//想象一下，如果实例化instance很消耗资源，我想让他延迟加载，另外一方面，我不希望在Singleton类加载时就实例化，因为我不能确保Singleton类还可能在其他的地方被主动使用从而被加载，那么这个时候实例化instance显然是不合适的。这个时候，这种方式相比第三种方式就显得很合理。
public class Singleton
{
	// 私有的  静态的 
	private static class SingletonHolder
	{
		// 私有的 静态的  final类型的
		private static final Singleton INSTANCE = new Singleton();
	}

	private Singleton()
	{
	}

	public static final Singleton getInstance()
	{
		// 返回内部类的静态属性
		return SingletonHolder.INSTANCE;
	}
}
```
**工厂方法**
调用和被调用者之间不产生直接的以来关系，而是由工厂来负责对象的创建，一般是在工厂的方法中调用类的构造函数，单个或者批量的创建对象。
**代理模式(Proxy)**
代理模式是一种应用非常广泛的设计模式，当客户端代码需要调用某个对象时，客户端实际上不关心是否准确得到该对象，它只要一个能提供该功能的对象即可，此时我们就可返回该对象的代理（Proxy）。
代理就是一个Java对象代表另一个Java对象来采取行动。示例代码如下：
```java
public class ImageProxy implements Image
{
    //组合一个image实例，作为被代理的对象
    private Image image;
    //使用抽象实体来初始化代理对象
    public ImageProxy(Image image)
    {
       this.image = image;
    }
    /**
     * 重写Image接口的show()方法
     * 该方法用于控制对被代理对象的访问，
     * 并根据需要负责创建和删除被代理对象
     */
    public void show()
    {
       //只有当真正需要调用image的show方法时才创建被代理对象
       if (image == null)
       {
           image = new BigImage();
       }
       image.show();
    }
}

// 调用的时候，这么调用
Image image = new ImageProxy(null);
```

**命令模式(Command)**
某个方法需要完成某一个功能，完成这个功能的大部分步骤已经确定了，但可能有少量具体步骤无法确定，必须等到执行该方法时才可以确定。（在某些编程语言如Ruby、Perl里，允许传入一个代码块作为参数。但Java暂时还不支持代码块作为参数）。在Java中，传入该方法的是一个对象，该对象通常是某个接口的匿名实现类的实例，该接口通常被称为命令接口，这种设计方式也被称为命令模式。
下面是源代码：
```java
// Command接口
public interface Command
{
    //接口里定义的process方法用于封装“处理行为”
    void process(int[] target);
}

// ProcessArray

public class ProcessArray
{
    //定义一个each()方法，用于处理数组，
    public void each(int[] target , Command cmd)
    {
       cmd.process(target);
    }
}

// TestCommand 测试
public class TestCommand
{
    public static void main(String[] args)
    {
       ProcessArray pa = new ProcessArray();
       int[] target = {3, -4, 6, 4};
       //第一次处理数组，具体处理行为取决于Command对象
       pa.each(target , new Command()
       {
           //重写process()方法，决定具体的处理行为
           public void process(int[] target)
           {
              for (int tmp : target )
              {
                  System.out.println("迭代输出目标数组的元素:" + tmp);
              }
           }
       });
       System.out.println("------------------");
       //第二次处理数组，具体处理行为取决于Command对象
       pa.each(target , new Command()
       {
           //重写process方法，决定具体的处理行为
           public void process(int[] target)
           {
              int sum = 0;
              for (int tmp : target )
              {
                  sum += tmp;         
              }
              System.out.println("数组元素的总和是:" + sum);
           }
       });
    }
}
```
**策略模式(Strategy)**
策略模式用于封装系列的算法，这些算法通常被封装在一个被称为Context的类中，客户端程序可以自由选择其中一种算法，或让Context为客户端选择一种最佳算法——使用策略模式的优势是为了支持算法的自由切换。
 **门面模式(Facade)**
 将多个类的方法进行封装，对于一些固定的操作，封装成一个方法来执行。
 **桥接模式(Bridge)**
由于实际的需要，某个类具有两个以上的维度变化，如果只是使用继承将无法实现这种需要，或者使得设计变得相当臃肿。而桥接模式的做法是把变化部分抽象出来，使变化部分与主类分离开来，从而将多个的变化彻底分离。最后提供一个管理类来组合不同维度上的变化，通过这种组合来满足业务的需要。
**观察者模式(Observer)**
观察者模式定义了对象间的一对多依赖关系，让一个或多个观察者对象观察一个主题对象。当主题对象的状态发生变化时，系统能通知所有的依赖于此对象的观察者对象，从而使得观察者对象能够自动更新。

I/O中涉及到的设计模式
------------
 - Java的I/O库总体设计是符合**装饰者模式**（Decorator）跟**适配器模式**（Adapter）的。
　　１　装饰者模式：在由 InputStream，OutputStream，Reader和Writer代表的等级结构内部，有一些流处理器可以对另一些流处理器起到装饰作用，形成新的，具有改善了的功能的流处理器，比如bufferedxxxStream。装饰者模式是Java I/O库的整体设计模式。这样的一个原则是符合装饰者模式的。 

　　２　适配器模式：Reader和Writer代表的等级结构内部，实现了**字节流到字符流**的转换，比如InputStreamReader，实现了字节流到字符流的转换。这就是适配器模式的应用。 （需要注意的是，**不能**将字符流转换为字节流）

类的初始化过程
-------
参考链接http://blog.csdn.net/zjl477595675/article/details/48101611
类的初始化和对象初始化是两个不同的概念。
**类的初始化**是发生在类加载过程，是类加载过程的一个阶段，该阶段并不调用类的构造器。
**对象的初始化**是在类加载完成后为对象分配内存，实例变量的初始化，实例变量的赋值及调用类构造器完成对象的初始化过程。

 1. 类的初始化
 分为类的加载——>链接——>**对象初始化**
类的加载
类加载发生在以下几种情况： 
1）new生成新的对象实例。 
2）使用java.lang.reflect包的方法对类进行发射调用时。 
3）当子类进行加载或初始化时。当加载一个类时，如果发现其存在父类并且未被加载则会继续加载父类。 
4）虚拟机启动时，用户指定的执行主类（包含main()的执行入口类），虚拟机会加载加载该类。 
5）调用类变量（静态字段但非静态常量），类方法
注意： 
1. 调用类的静态字段，只有直接定义这个字段的类才会被加载和初始化。通过其子类来引用父类中定义的字段，只会触发父类的初始化而不会触发子类的初始化。 
2. 调用类的静态常量是不触发类的加载过程。如果在A类中调用B类的静态常量，那么在编译阶段会将该静态常量放到A的Class文件的静态常量池中，所以对该常量的调用不涉及B的加载。

```java
 class SuperClass{
    static{
        System.out.println("SuperClass 类初始化");
    }
    static int a=3;
    static final int b=4;
}
class SubClass extends SuperClass{
    static{
        System.out.println("SubClass 类初始化");
    }
}
public class Test {
    public static void main(String[] args) {
        System.out.println(SubClass.a);
    }
}
```
结果是：
```java
SuperClass 类初始化
3
```
如果Test这么写：
```java
public class Test
{
	public static void main(String[] args)
	{
//		System.out.println(SubClass.a);
		System.out.println(SubClass.b);
	}
}
```
那么输出结如下，两个类都没有进行初始化，因为此时b是类常量。
```java
4
```
类加载主要完成如下三件事情：
加载阶段虚拟机主要完成以下三件事： 
1）根据类的路径，定位并获取类的class文件 
2）通过加载器加载class文件，并将class文件里所代表的静态存储结构转化为方法区的运行数据结构 
3）在java堆中生成一个代表这个类的java.lang.Class对象，作为方法区这些数据的访问入口。
2、类的链接
链接又可以细分为验证——>准备——>解析

 - 验证阶段：

确保Class文件的字节流包含的信息符合当前虚拟机的要求，并不会危害虚拟机自身安全。该阶段包括四个部分： 
1.文件格式验证：验证字节流是否符合Class文件格式的规范 
2.元数据验证：对字节码描述的信息进行语义分析，以确保其描述的信息符合java语言规范要求 
3.字节码验证：进行数据流和控制流分析，保证校验类的方法在运行时不会做出危害虚拟机安全的行为。 
4.符号引用验证：在虚拟机中将符号引用转换为直接引用

 - 准备阶段
正式为**类变量**分配内存并设置类变量的默认值，这些内存在**方法区**中进行分配。内存分配仅针对类变量（static变量），不包括实例变量，实例变量是在对象实例化时和对象一起在堆中分配内存。 
类变量的默认值的设置和为了保证变量使用的安全性在对象实例化过程中虚拟机自动地对实例变量进行设置默认值是一样的。默认值的设置如下：
```java
/*数据类型		默认值
	int			0
	long		0L
	char		‘\u0000’
	byte		(byte)0
	boolean		false
	float		0.0f
	double		0.0d
	reference	null*/
```
 - 解析阶段
 将常量池内的符号引用替换为直接应用的过程
 - 对象初始化
 初始化是执行类构造器() (在准备阶段提供的代码，即执行所有类变量的赋值动作及静态代码块内容）。在执行类构造器()时保证父类的构造器()已经执行完毕。即一般我们会看到先执行父类的static方法块内容和static变量的赋值，然后再执行子类的static方法块内容和static变量赋值。

main函数执行所发生的一系列动作
-----------------
1、JVM启动
2、对main函数中需要使用的类，执行加载、链接、初始化
3、启动main线程
4、执行main函数
5、JVM退出

GC回收算法
------
JVM将堆分成了 二个大区  Young 和 Old 。

 - Young区

Young 区又分为 Eden、Survivor1、Survivor2, 两个Survivor 区相对地作为为From 和 To 逻辑区域, 当Servivor1作为 From 时 ， Servivor2 就作为 To,反之亦然。这是因为新生代区域使用的是**复制清理**的GC算法。
关于为什么要这样区分Young（将Young区分为Eden、Servivor1、Servivor2以及相对的From和To ），这要牵涉到JVM的垃圾回收算法的讨论。
1）因为引用计数法无法解决循环引用问题，JVM并没有采用这种算法来判断对象是否存活。
2）JVM一般采用GCRoots的方法，只要从任何一个GCRoots的对象可达，就是不被回收的对象
3）判断了对象生死，怎么进行内存的清理呢？
4）标记-清除算法，先标记那些要被回收的对象，然后进行清理，简单可行，但是**①**标记清除效率低，因为要一个一个标记和清除**②**造成大量不连续的内存碎片，空间碎片太多可能会导致当程序在以后的运行过程中需要分配较大对象的时候无法找到足够的连续内存而不得不触发另一次垃圾收集动作。
5）采用复制-清除算法：将可用内存按照容量分为大小相等的两块，每次只是使用其中的一块。当这一块的内存用完了，就将可用内存中存活的对象依次复制到空闲的内存区域，然后对之前的区域进行清除操作。

 - 那些对象可以作为GC Roots？

    虚拟机栈（栈帧中的本地变量表）中的引用的对象
    方法区中的类静态属性引用的对象
    方法区中的常量引用的对象
    本地方法栈中JNI（Native方法）的引用对象
    
新生代做的是复制-清理、from survivor、to survivor是干啥用的、老年代做的是标记-清理
**标记-清理的优缺点**
优点：好像没什么优点，或许优点是简单、理论上可用空间大等。
缺点：1、效率问题，标记和清理两个两个过程效率都不高。2、空间问题，标记清理完成以后会产生大量的不连续内存区域。可能导致大对象无法获得空间分配，从而触发不必要的GC操作。
**复制-清理的优缺点**
优点：不会产生碎片化问题，实现简单
缺点：会浪费掉一个部分内存区域

concurrentHashMap的底层实现
----------------------
concurrentHashMap在JDK1.8的实现和JDK1.7是不一样的，JDK8默认将table数组的每一项作为一个segment，而JDK7将table数组的几项作为一个segment。
JDK8直接采用transient volatile HashEntry<K,V>[] table保存数据，采用table数组元素作为锁，从而实现了对每一行数据进行加锁，进一步减少并发冲突的概率。

分布式的基本知识
--------
CAP理论是由Eric Brewer提出的分布式系统中最为重要的理论之一：
Consistency：[强]一致性，事务保障，ACID模型。
Availiablity：[高]可用性，冗余以避免单点，至少做到柔性可用（服务降级）。
Partition tolerance：[高]可扩展性（分区容忍性）：一般要求系统能够自动按需扩展，比如HBase。

SpringMVC框架执行步骤（SpringMVC使用Servlet嵌入）：
--------------------------------------
SpringMVC的url请求的核心功能是使用dispatcherServlet来完成的。
1、客户端发出一个http请求给web服务器，web服务器对http请求进行解析，如果匹配DispatcherServlet的请求映射路径（在web.xml中指定），web容器将请求转交给DispatcherServlet.
2、DipatcherServlet接收到这个请求之后将根据请求的信息（包括URL、Http方法、请求报文头和请求参数Cookie等）以及HandlerMapping的配置找到处理请求的处理器（Handler）（也就是controller中对应的url处理类）。
3-4、DispatcherServlet根据HandlerMapping找到对应的Handler,将处理权交给Handler（Handler将具体的处理进行封装），再由具体的HandlerAdapter对Handler进行具体的调用（controller中具体的调用方法）。
5、Handler对数据处理完成以后将返回一个ModelAndView()对象给DispatcherServlet。
6、Handler返回的ModelAndView()只是一个逻辑视图并不是一个正式的视图，DispatcherSevlet通过ViewResolver将逻辑视图转化为真正的视图View。
7、Dispatcher通过model解析出ModelAndView()中的参数进行解析最终展现出完整的view并返回给客户端。

spring bean的生命周期
----------------
bean的生命周期大致会经历以下一些过程。

 1. bean实例化，寻找bean的定义信息并将其实例化。
 2. 设置bean的属性值。
 3. 调用BeanNameAware的setBeanName方法设置bean名称。
 4. 调用BeanFactoryAware的setBeanFactory方法。
 5. 调用BeanPostProcessor的**预初始化**方法。
 6. 调用InitializingBean的afterPropertiesSet方法。
 7. 调用bean的init-method方法
 8. 调用BeanPostProcessor的**后初始化**方法。
 9. bean初始化完成，可以在上下文环境中使用
 10. 容器关闭的时候，需要调用DisposableBean的destroy方法。
 11. 调用bean的destroy方法。
 
面试的时候，上面的步骤应该简化着来回答，一般可以概括为：实例化，初始init，接收请求service，销毁destroy；
1、**实例化**一个Bean－－也就是我们常说的**new**；

    2、按照Spring上下文对实例化的Bean进行配置－－也就是IOC注入；

    3、如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String)方法，此处传递的就是Spring配置文件中Bean的id值

    4、如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory(setBeanFactory(BeanFactory)传递的是Spring工厂自身（可以用这个方式来获取其它Bean，只需在Spring配置文件中配置一个普通的Bean就可以）；

    5、如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文（同样这个方式也可以实现步骤4的内容，但比4更好，因为ApplicationContext是BeanFactory的子接口，有更多的实现方法）；

    6、如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessBeforeInitialization(Object obj, String s)方法，BeanPostProcessor经常被用作是Bean内容的更改，并且由于这个是在Bean初始化结束时调用那个的方法，也可以被应用于内存或缓存技术；

    7、如果Bean在Spring配置文件中配置了init-method属性会自动调用其配置的初始化方法。

    8、如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法、；

    注：以上工作完成以后就可以应用这个Bean了，那这个Bean是一个Singleton的，所以一般情况下我们调用同一个id的Bean会是在内容地址相同的实例，当然在Spring配置文件中也可以配置非Singleton，这里我们不做赘述。

    9、当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用那个其实现的destroy()方法；

    10、最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。

一个文件中定义多个class
--------------
这个是可以的，只是**同一个文件只能有一个类定义成public**的
ArrayList的底层实现
------------
参考链接https://www.cnblogs.com/ITtangtang/p/3948555.html
 - ArrayList底层使用数组实现
 - 当不指定初始容量时，数组出事容量为10
 其中比较重要的函数，如下面源码所示：

根据下表index移除元素时，首先需要判断是否越界。然后才是删除元素，拷贝删除元素后面位置的数组，移除成功就返回移除的元素。
```java
// 移除此列表中指定位置上的元素。  
 public E remove(int index) {  
 // 如果index越界，抛出IndexOutOfBoundsException异常
    RangeCheck(index);  
  
    modCount++;  
    E oldValue = (E) elementData[index];  
  
    int numMoved = size - index - 1;  
    if (numMoved > 0)  
        System.arraycopy(elementData, index+1, elementData, index, numMoved);  
    elementData[--size] = null; // clear to let GC do its work
  
    return oldValue;  
 }
```

根据指定元素o，移除成功返回true，否则返回false
remove(Object o)中通过遍历element寻找是否存在传入对象，一旦找到就调用fastRemove移除对象。为什么找到了元素就知道了index，不通过remove(index)来移除元素呢？因为fastRemove跳过了判断边界的处理，因为找到元素就相当于确定了index不会超过边界，而且fastRemove并不返回被移除的元素。
```java
// 移除此列表中首次出现的指定元素（如果存在）。这是应为ArrayList中允许存放重复的元素。  
 public boolean remove(Object o) {  
    // 由于ArrayList中允许存放null，因此下面通过两种情况来分别处理。  
    if (o == null) {  
        for (int index = 0; index < size; index++)  
            if (elementData[index] == null) {  
                // 类似remove(int index)，移除列表中指定位置上的元素。  
                fastRemove(index);  
                return true;  
            }  
    } else {  
        for (int index = 0; index < size; index++)  
            if (o.equals(elementData[index])) {  
                fastRemove(index);  
                return true;  
            }  
        }  
        return false;  
    } 
}
```

```java
/**
     * Increases the capacity of this <tt>ArrayList</tt> instance, if
     * necessary, to ensure that it can hold at least the number of elements
     * specified by the minimum capacity argument.
     *
     * @param   minCapacity   the desired minimum capacity
     */
    public void ensureCapacity(int minCapacity) {
        int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)
            // any size if not default element table
            ? 0
            // larger than default for default empty table. It's already
            // supposed to be at default size.
            : DEFAULT_CAPACITY;

        if (minCapacity > minExpand) {
            ensureExplicitCapacity(minCapacity);
        }
    }
    
    // ensureExplicitCapacity方法
        private void ensureExplicitCapacity(int minCapacity) {
        modCount++;

        // overflow-conscious code
        if (minCapacity - elementData.length > 0)
            grow(minCapacity);
    }
    
    // grow方法进行实际数组的扩容，先按照2倍进行扩容，如果不够，就按照指定的minCapacity容量进行扩容
    /**
     * Increases the capacity to ensure that it can hold at least the
     * number of elements specified by the minimum capacity argument.
     *
     * @param minCapacity the desired minimum capacity
     */
    private void grow(int minCapacity) {
        // overflow-conscious code
        int oldCapacity = elementData.length;
        int newCapacity = oldCapacity + (oldCapacity >> 1);
        if (newCapacity - minCapacity < 0)
            newCapacity = minCapacity;
        if (newCapacity - MAX_ARRAY_SIZE > 0)
            newCapacity = hugeCapacity(minCapacity);
        // minCapacity is usually close to size, so this is a win:
        elementData = Arrays.copyOf(elementData, newCapacity);
    }
```

Fail-Fast机制： 
ArrayList也采用了快速失败的机制，通过记录modCount参数来实现。在面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险。具体介绍请参考这篇文章深入Java集合学习系列：HashMap的实现原理 中的Fail-Fast机制。
ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法。Arrays.copyof()内部最终也调用了System.arraycopy()，只是调用之前先判断数组类型是否一致，再选择创建相应类型的数组（不懂可以看源码），再调用System.arraycopy()这个native方法进行数组的拷贝。

Java Array和Arrays
-----------------
Array是数组类，Arrays是一个工具类，类似于collection和collections的区别。

分布式如何实现session共享
----------------
参考资料http://blog.csdn.net/sxiaobei/article/details/57086489
 1. 服务器实现的session复制或session共享，这类型的共享session是和服务器紧密相关的，比如webSphere或JBOSS在搭建集群时候可以配置实现session复制或session共享，但是这种方式有一个致命的缺点，就是不好扩展和移植，比如我们更换服务器，那么就要修改服务器配置（因为会涉及到服务器之间的直接http请求，因此会将服务器的IP写死在相应的配置文件中）。
 2. 利用成熟的技术做session复制，比如12306使用的gemfire，比如常见的内存数据库如redis或memorycache，这类方案虽然比较普适，但是严重依赖于第三方，这样当第三方服务器出现问题的时候，那么将是应用的灾难。
 3. 将session维护在客户端，很容易想到就是利用cookie，但是客户端存在风险，数据不安全，而且可以存放的数据量比较小，所以将session维护在客户端还要对session中的信息加密。
 4. 使用Spring Session
Spring Session是Spring的项目之一，GitHub地址：https://github.com/spring-projects/spring-session。
Spring Session提供了一套创建和管理Servlet HttpSession的方案。Spring Session提供了集群Session（Clustered Sessions）功能，默认采用外置的Redis来存储Session数据，以此来解决Session共享的问题。
这是官网的介绍：
Spring Session provides an API and implementations for managing a user’s session information.

Features

Spring Session provides the following features:

API and implementations for managing a user's session
HttpSession - allows replacing the HttpSession in an application container (i.e. Tomcat) neutral way
Clustered Sessions - Spring Session makes it trivial to support clustered sessions without being tied to an application container specific solution.
Multiple Browser Sessions - Spring Session supports managing multiple users' sessions in a single browser instance (i.e. multiple authenticated accounts similar to Google).
RESTful APIs - Spring Session allows providing session ids in headers to work with RESTful APIs
WebSocket - provides the ability to keep the HttpSession alive when receiving WebSocket messages
使用spring-session需要进行如下一些配置：

 - 添加spring-session-data-redis依赖
 
 ```html
 <dependency>  
    <groupId>org.springframework.session</groupId>  
    <artifactId>spring-session-data-redis</artifactId>  
    <version>1.0.1.RELEASE</version>  
</dependency>  
 ```
 
 - 第二步，编写一个配置类，用来启用RedisHttpSession功能，并向Spring容器中注册一个RedisConnectionFactory。
```java
import org.springframework.context.annotation.Bean;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.connection.jedis.JedisConnectionFactory;
import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession;

@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 7200)
public class RedisHttpSessionConfig {

    @Bean
    public RedisConnectionFactory connectionFactory() {
        JedisConnectionFactory connectionFactory = new JedisConnectionFactory();
        connectionFactory.setPort(6379);
        connectionFactory.setHostName("10.18.15.190");
        return connectionFactory;
    }
}
```

 - 第三步，将RedisHttpSessionConfig加入到WebInitializer#getRootConfigClasses()中，让Spring容器加载RedisHttpSessionConfig类。WebInitializer是一个自定义的AbstractAnnotationConfigDispatcherServletInitializer实现类，该类会在Servlet启动时加载（当然也可以采用别的加载方法，比如采用扫描@Configuration注解类的方式等等）。
```java
//该类采用Java Configuration，来代替web.xml   
public class WebInitializer extends AbstractAnnotationConfigDispatcherServletInitializer {
    
    @Override
    protected Class<?>[] getRootConfigClasses() {
        return new Class[]{Config1.class, Config2.class, RedisHttpSessionConfig.class};
    }
	
	//......
}
```
 - 第四步，编写一个一个AbstractHttpSessionApplicationInitializer实现类，用于向Servlet容器中添加springSessionRepositoryFilter。
```java
import org.springframework.session.web.context.AbstractHttpSessionApplicationInitializer;  
  
public class SpringSessionInitializer extends AbstractHttpSessionApplicationInitializer {  
} 
```

Java中的Lock和synchronized两种锁定机制的对比
-----------------------------------------
参考资料https://www.cnblogs.com/xrq730/p/4979021.html
总的来说，Lock接口是对synchronized语法的一个扩展，它支持更加灵活的锁定范围，以及一些灵活的锁定方式，例如允许**并发读**。
直接复制的JDK1.6官方文档相关部分：
synchronized 方法或语句的使用提供了对与每个对象相关的隐式监视器锁的访问，但却强制所有锁获取和释放均要出现在一个块结构中：当获取了多个锁时，它们必须以相反的顺序释放，且必须在与所有锁被获取时相同的词法范围内释放所有锁。

虽然 synchronized 方法和语句的范围机制使得使用监视器锁编程方便了很多，而且还帮助避免了很多涉及到锁的常见编程错误，但有时也需要以**更为灵活**的方式使用锁。例如，某些遍历并发访问的数据结果的算法要求使用 "hand-over-hand" 或 "chain locking"：获取节点 A 的锁，然后再获取节点 B 的锁，然后释放 A 并获取 C，然后释放 B 并获取 D，依此类推。Lock 接口的实现允许锁在不同的作用范围内获取和释放，并允许以任何顺序获取和释放多个锁，从而支持使用这种技术。

随着灵活性的增加，也带来了更多的责任。不使用块结构锁就失去了使用 synchronized 方法和语句时会出现的锁自动释放功能。在大多数情况下，应该使用以下语句：
```java
Lock l = ...; 
     l.lock();
     try {
         // access the resource protected by this lock
     } finally {
         l.unlock();
     }
 
```
Lock 实现提供了使用 synchronized 方法和语句所没有的其他功能，包括提供了一个非块结构的获取锁尝试 (tryLock())、一个获取可中断锁的尝试 (lockInterruptibly()) 和一个获取超时失效锁的尝试 (tryLock(long, TimeUnit))。

Spring中ApplicationContext和beanfactory区别
---------------------------------------
org.springframework.beans  and  org.springframework.context  是springIOC容器的基础。BeanFactory  接口提供了一套完备的配置机制，使得它可以管理任何类型的对象。  ApplicationContext  是BeanFactory  的一个子接口。  ApplicationContext子接口实现了与AOP的宽松集成：消息处理与集成；web应用中的应用层context，比如说WebApplicationContext  。
简单来说，BeanFactory  提供框架级别的配置以及基础的功能，在此基础上，ApplicationContext添加了更多针对企业级的特定功能。Applicationcontext是beanfactory的的一个完整扩展集。如果要使用beanfactory而不是applicationcontext，可以参考spring的官方文档  Section  7.16, “The BeanFactory”.
在spring中，那些由IOC容器管理的对象组成了应用的骨架，这些对象被称为beans。

设计模式之六大原则
---------
 1. 单一职责原则(Single Responsibility Principle,
    SRP)：一个类只负责一个功能领域中的相应职责，或者可以定义为：就一个类而言，应该只有一个引起它变化的原因。

 2. 开闭原则(Open-Closed Principle, OCP)：一个软件实体应当对扩展开放，对修改关闭。即软件实体应尽量在不修改原有代码的情况下进行扩展。

 3. 里氏代换原则(Liskov Substitution Principle, LSP)：所有引用基类（父类）的地方必须能透明地使用其子类的对象。

 4. 依赖倒转原则(Dependency Inversion  Principle,
    DIP)：抽象不应该依赖于细节，细节应当依赖于抽象。换言之，要针对接口编程，而不是针对实现编程。

 5. 接口隔离原则(Interface  Segregation Principle,
    ISP)：使用多个专门的接口，而不使用单一的总接口，即客户端不应该依赖那些它不需要的接口。

 6. 迪米特法则(Law of  Demeter, LoD)：一个软件实体应当尽可能少地与其他实体发生相互作用。
 

linux中可以同时查看一个文件的前几行和末尾几行
-------------------------
可以使用head（查看前几行）、tail（查看末尾几行）两个命令。
例如：
查看/etc/profile的前10行内容，应该是：
 head -n 10 /etc/profile
查看/etc/profile的最后5行内容，应该是：
 tail  -n 5 /etc/profile
如果想同时查看可以将前10行和后5行的显示信息通过输出重定向的方法保存到一个文档，这样查看文档即可一目了然。
例如：
将内容输出到/home/test文件中
 head -n 10 /etc/profile >>/home/test
 tail  -n 5 /etc/profile>>/home/test
查看的话只需要打开test文件即可。
cat /home/test

linux文本内容查找
-----------
从文件内容查找匹配指定字符串的行：
$ grep "被查找的字符串" 文件名
从文件内容查找与正则表达式匹配的行：
$ grep –e “正则表达式” 文件名
查找时不区分大小写：
$ grep –i "被查找的字符串" 文件名
查找匹配的行数：
$ grep -c "被查找的字符串" 文件名
从文件内容查找不匹配指定字符串的行：
$ grep –v "被查找的字符串" 文件名

从根目录开始查找所有扩展名为.log的文本文件，并找出包含”ERROR”的行
find / -type f -name "*.log" | xargs grep "ERROR"

系统查找到httpd.conf文件后即时在屏幕上显示httpd.conf文件信息。 
find/-name"httpd.conf"-ls

在根目录下查找某个文件
find . -name "test"

在某个目录下查找包含某个字符串的文件

grep -r "zh_CN" ./

violatile关键字
------------
参考链接http://www.cnblogs.com/dolphin0520/p/3920373.html
由于所有的程序代码执行都是通过在CPU中执行线程来实现的，一般运算过程中的临时数据存储在物理内存（主存）中，但是主存的速度远低于CPU，这时CPU就需要使用告诉缓存（cache）。
在CPU进行计算时，直接从cache中读取数据，而不是主存。
对共享变量，在单线程程序中是没有问题的，但是在多线程程序中就会出现问题，因为各个线程读取的cache可能不同步。
如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。

　　为了解决缓存不一致性问题，通常来说有以下2种解决方法：
 1. 通过在总线加LOCK#锁的方式
 2. 通过缓存一致性协议
两种方式都是通过硬件层面进行控制。
在早期的CPU当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。

　　但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。

　　所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。
　　

volatile的原理和实现机制是什么？
--------------------
下面这段话摘自《深入理解Java虚拟机》：

　　“观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令”

　　lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：

　　1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成；

　　2）它会强制将对缓存的修改操作立即写入主存；

　　3）如果是写操作，它会导致其他CPU中对应的缓存行无效。

　　

并发编程中的原子性、可见性、有序性
-----------------

 - 原子性

某一个操作，或者一系列操作，要么成功。要么失败。
例如i = 9;就是一个院子操作。
假若一个线程执行到这个语句时，我暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。
那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。

 - 可见性
对于共享变量，一个线程修改了它，其他线程能够立马读取到修改后的值。
　
 - 有序性

即程序执行的顺序按照代码的先后顺序执行。
一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。因为处理器（**不是jvm**）在进行指令重排序之前，需要考虑数据之间的以来关系。
**虽然**指令重排序不会影响**单线程**程序的执行结果，但是对于多线程程序，就可能产生影响。比如下面的代码：
```java
//线程1:
context = loadContext();   //语句1
inited = true;             //语句2
 
//线程2:
while(!inited ){
  sleep()
}
doSomethingwithconfig(context);
```
上面的代码，如果线程2执行的时候，线程1还未对context进行初始化，那么就会出现错误。
**总之**，对于并发编程，原子性、可见性、有序性缺一不可。

Java内存模型
--------
Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。
那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？

 - 原子性
在Java中，对**基本数据类型**的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。
请分析以下哪些操作是原子性操作：
```java
x = 10;         //语句1
y = x;         //语句2
x++;           //语句3
x = x + 1;     //语句4
```
上面语句，只有语句1是原子操作，其他的都不是。
从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。
 - 可见性
Java提供了volatile关键字来保证可见性。
另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。
 - 有序性
 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。

jvm性能分析
-------
linux系统，一般的逻辑是使用ps命令查看Java的进程id，然后使用
```linux
jmap -dump:format=b,file=3.dump 11459
```
这个命令可以将堆栈的信息以文件的方式输出，然后可以使用MAT工具打开。
如果要查看当前PID的每个线程的状态，也可以使用jstack命令
```linux
audi@audi-PC:~/Desktop$ jstack -l 21703
Picked up _JAVA_OPTIONS:   -Dawt.useSystemAAFontSettings=gasp
2017-11-29 23:35:09
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.151-b12 mixed mode):

"Attach Listener" #9 daemon prio=9 os_prio=0 tid=0x00007fa9c0001000 nid=0x5544 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Service Thread" #8 daemon prio=9 os_prio=0 tid=0x00007fa9f80cb000 nid=0x54db runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C1 CompilerThread2" #7 daemon prio=9 os_prio=0 tid=0x00007fa9f80be000 nid=0x54da waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread1" #6 daemon prio=9 os_prio=0 tid=0x00007fa9f80bc000 nid=0x54d9 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread0" #5 daemon prio=9 os_prio=0 tid=0x00007fa9f80b9000 nid=0x54d8 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Signal Dispatcher" #4 daemon prio=9 os_prio=0 tid=0x00007fa9f80b7800 nid=0x54d7 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007fa9f8085000 nid=0x54d6 in Object.wait() [0x00007fa9e6b0c000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000ff988ec8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x00000000ff988ec8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

   Locked ownable synchronizers:
	- None

"Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007fa9f8080000 nid=0x54d5 in Object.wait() [0x00007fa9e6b4d000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000ff986b68> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x00000000ff986b68> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

   Locked ownable synchronizers:
	- None

"main" #1 prio=5 os_prio=0 tid=0x00007fa9f800b000 nid=0x54c9 runnable [0x00007fa9fed81000]
   java.lang.Thread.State: RUNNABLE
	at com.audi.jvm.HeapOOM.main(HeapOOM.java:13)

   Locked ownable synchronizers:
	- None

"VM Thread" os_prio=0 tid=0x00007fa9f8078800 nid=0x54d4 runnable 

"GC task thread#0 (ParallelGC)" os_prio=0 tid=0x00007fa9f8020000 nid=0x54d0 runnable 

"GC task thread#1 (ParallelGC)" os_prio=0 tid=0x00007fa9f8021800 nid=0x54d1 runnable 

"GC task thread#2 (ParallelGC)" os_prio=0 tid=0x00007fa9f8023800 nid=0x54d2 runnable 

"GC task thread#3 (ParallelGC)" os_prio=0 tid=0x00007fa9f8025000 nid=0x54d3 runnable 

"VM Periodic Task Thread" os_prio=0 tid=0x00007fa9f80ce000 nid=0x54dc waiting on condition 

JNI global references: 6

```

JVM性能瓶颈定位
---------

 1. 使用top命令找到最耗CPU的进程ID。
```linux

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                       
23729 audi      20   0 2325496  23864  16096 S 100.0  0.3   9:41.56 java                          
  538 root      20   0  472828 159524 111776 S  25.0  2.0  12:59.41 Xorg                          
 1108 audi      20   0 1257172 174420  88856 S   6.2  2.2  17:03.47 deepin-wm                     
 1649 audi      20   0    9160   6504   1920 S   6.2  0.1  22:32.07 wineserver.real               
 2447 audi      20   0 1467844 276216 148520 S   6.2  3.4   4:59.35 chrome                        
 5558 audi      20   0  606828  41644  31080 S   6.2  0.5   0:45.75 deepin-terminal               
    1 root      20   0  212524   7592   5896 S   0.0  0.1   0:01.59 systemd                       
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.01 kthreadd                      
    3 root      20   0       0      0      0 S   0.0  0.0   0:01.50 ksoftirqd/0                   
    5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H                  
    7 root      20   0       0      0      0 S   0.0  0.0   0:09.79 rcu_preempt                   
    8 root      20   0       0      0      0 S   0.0  0.0   0:00.00 rcu_sched                     
    9 root      20   0       0      0      0 S   0.0  0.0   0:00.00 rcu_bh                        
   10 root      rt   0       0      0      0 S   0.0  0.0   0:00.17 migration/0                   
   11 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 lru-add-drain   
```

 2. 得到最耗CPU的进程ID以后，继续查找，在该进程下，哪一个线程最耗费CPU资源。
```linux
audi@audi-PC:~/Desktop$ ps p 23729 -L -o pcpu,pid,tid,time,tname,cmd
%CPU   PID   TID     TIME TTY      CMD
 0.0 23729 23729 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
99.8 23729 23732 00:11:55 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23737 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23738 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23739 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23740 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23742 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23743 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23744 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23745 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23746 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23747 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23748 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23749 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23750 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256

```
从TID这一列可以看出来，最耗费CPU的线程是**23732**，转换成16进制是0x5CB4，这时候就需要使用jstack命令来查看进程**23729**的内部线程的运行情况：
```linux
audi@audi-PC:~/Desktop$ jstack -l 23729
Picked up _JAVA_OPTIONS:   -Dawt.useSystemAAFontSettings=gasp
2017-11-29 23:57:06
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.151-b12 mixed mode):

"Attach Listener" #9 daemon prio=9 os_prio=0 tid=0x00007fb1cc001000 nid=0x6d71 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Service Thread" #8 daemon prio=9 os_prio=0 tid=0x00007fb2040d3000 nid=0x5cc5 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C1 CompilerThread2" #7 daemon prio=9 os_prio=0 tid=0x00007fb2040be000 nid=0x5cc4 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread1" #6 daemon prio=9 os_prio=0 tid=0x00007fb2040bc000 nid=0x5cc3 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread0" #5 daemon prio=9 os_prio=0 tid=0x00007fb2040b9000 nid=0x5cc2 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Signal Dispatcher" #4 daemon prio=9 os_prio=0 tid=0x00007fb2040b7800 nid=0x5cc1 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007fb204085000 nid=0x5cc0 in Object.wait() [0x00007fb1f26cd000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000ff988ec8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x00000000ff988ec8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

   Locked ownable synchronizers:
	- None

"Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007fb204080000 nid=0x5cbf in Object.wait() [0x00007fb1f270e000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000ff986b68> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x00000000ff986b68> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

   Locked ownable synchronizers:
	- None

"main" #1 prio=5 os_prio=0 tid=0x00007fb20400b000 nid=0x5cb4 runnable [0x00007fb20a949000]
   java.lang.Thread.State: RUNNABLE
	at com.audi.jvm.HeapOOM.main(HeapOOM.java:13)

   Locked ownable synchronizers:
	- None

"VM Thread" os_prio=0 tid=0x00007fb204078800 nid=0x5cbe runnable 

"GC task thread#0 (ParallelGC)" os_prio=0 tid=0x00007fb204020000 nid=0x5cb9 runnable 

"GC task thread#1 (ParallelGC)" os_prio=0 tid=0x00007fb204021800 nid=0x5cba runnable 

"GC task thread#2 (ParallelGC)" os_prio=0 tid=0x00007fb204023800 nid=0x5cbb runnable 

"GC task thread#3 (ParallelGC)" os_prio=0 tid=0x00007fb204025000 nid=0x5cbc runnable 

"VM Periodic Task Thread" os_prio=0 tid=0x00007fb2040d6000 nid=0x5cc6 waiting on condition 

JNI global references: 6

```
注意上面的，nid=0x5cb4，就是我们需要定位的线程：
```linux
"main" #1 prio=5 os_prio=0 tid=0x00007fb20400b000 nid=0x5cb4 runnable [0x00007fb20a949000]
   java.lang.Thread.State: RUNNABLE
	at com.audi.jvm.HeapOOM.main(HeapOOM.java:13)

```
显示是在HeapOOM.java:13的第十三行出了问题，结果也确实是，我在那里写了while死循环。

Redis、Memcache和MongoDB的区别
-------------------------

 
参考链接：http://www.cnblogs.com/tuyile006/p/6382062.html
 - Memcached
============

 - Memcached的优点：
Memcached可以利用多核优势，单实例吞吐量极高，可以达到几十万QPS（取决于key、value的字节大小以及服务器硬件性能，日常环境中QPS高峰大约在4-6w左右）。适用于最大程度扛量。支持直接配置为session handle。

 - Memcached的局限性：
只支持简单的key/value数据结构，不像Redis可以支持丰富的数据类型。
无法进行持久化，数据不能备份，只能用于缓存使用，且重启后数据全部丢失。
无法进行数据同步，不能将MC中的数据迁移到其他MC实例中。
Memcached内存分配采用Slab Allocation机制管理内存，value大小分布差异较大时会造成内存利用率降低，并引发低利用率时依然出现踢出等问题。需要用户注重value设计。

 

 - Redis
=======
 - Redis的优点：
支持多种数据结构，如 string（字符串）、 list(双向链表)、dict(hash表)、set(集合）、zset(排序set)、hyperloglog（基数估算）
支持持久化操作，可以进行aof及rdb数据持久化到磁盘，从而进行数据备份或数据恢复等操作，较好的防止数据丢失的手段。
支持通过Replication进行数据复制，通过master-slave机制，可以实时进行数据的同步复制，支持多级复制和增量复制，master-slave机制是Redis进行HA的重要手段。
单线程请求，所有命令串行执行，并发情况下不需要考虑数据一致性问题。
支持pub/sub消息订阅机制，可以用来进行消息订阅与通知。
支持简单的事务需求，但业界使用场景很少，并不成熟。
 - Redis的局限性：
Redis只能使用单线程，性能受限于CPU性能，故单实例CPU最高才可能达到5-6wQPS每秒（取决于数据结构，数据大小以及服务器硬件性能，日常环境中QPS高峰大约在1-2w左右）。
支持简单的事务需求，但业界使用场景很少，并不成熟，既是优点也是缺点。
Redis在string类型上会消耗较多内存，可以使用dict（hash表）压缩存储以降低内存耗用。

Mc和Redis都是Key-Value类型，不适合在不同数据集之间建立关系，也不适合进行查询搜索。比如redis的keys pattern这种匹配操作，对redis的性能是灾难。

 - mongoDB
mongoDB 是一种文档性的数据库。先解释一下文档的数据库，即可以存放xml、json、bson类型系那个的数据。
这些数据具备自述性（self-describing），呈现分层的树状数据结构。redis可以用hash存放简单关系型数据。
mongoDB 存放**json**格式数据。
适合场景：事件记录、内容管理或者博客平台，比如评论系统。 
 - mongodb持久化原理
mongodb与mysql不同，mysql的每一次更新操作都会直接写入硬盘，但是mongo不会，做为内存型数据库，数据操作会先写入内存，然后再会持久化到硬盘中去，那么mongo是如何持久化的呢
mongodb在启动时，专门初始化一个线程不断循环（除非应用crash掉），用于在一定时间周期内来从defer队列中获取要持久化的数据并写入到磁盘的journal(日志)和mongofile(数据)处，当然因为它不是在用户添加记录时就写到磁盘上，所以按mongodb开发者说，它不会造成性能上的损耗，因为看过代码发现，当进行CUD操作时，记录(Record类型)都被放入到defer队列中以供延时批量（groupcommit）提交写入，但相信其中时间周期参数是个要认真考量的参数，系统为90毫秒，如果该值更低的话，可能会造成频繁磁盘操作，过高又会造成系统宕机时数据丢失过。
 - 什么是NoSQL数据库？NoSQL和RDBMS有什么区别？在哪些情况下使用和不使用NoSQL数据库？
NoSQL是非关系型数据库，NoSQL = Not Only SQL。
关系型数据库采用的结构化的数据，NoSQL采用的是键值对的方式存储数据。
在处理非结构化/半结构化的大数据时；在水平方向上进行扩展时；随时应对动态增加的数据项时可以优先考虑使用NoSQL数据库。
在考虑数据库的成熟度；支持；分析和商业智能；管理及专业性等问题时，应优先考虑关系型数据库。 
 - MySQL和MongoDB之间最基本的区别是什么？
关系型数据库与非关系型数据库的区别，即数据存储结构的不同。
 - MongoDB的特点是什么？
（1）面向文档（2）高性能（3）高可用（4）易扩展（5）丰富的查询语言 
 - MongoDB支持存储过程吗？如果支持的话，怎么用？
MongoDB支持存储过程，它是javascript写的，保存在db.system.js表中。
 - 如何理解MongoDB中的GridFS机制，MongoDB为何使用GridFS来存储文件？
GridFS是一种将大型文件存储在MongoDB中的文件规范。使用GridFS可以将大文件分隔成多个小文档存放，这样我们能够有效的保存大文档，而且解决了BSON对象有限制的问题。 
 - 为什么MongoDB的数据文件很大？
MongoDB采用的预分配空间的方式来防止文件碎片。
 - 当更新一个正在被迁移的块（Chunk）上的文档时会发生什么？
更新操作会立即发生在旧的块（Chunk）上，然后更改才会在所有权转移前复制到新的分片上。
 - MongoDB在A:{B,C}上建立索引，查询A:{B,C}和A:{C,B}都会使用索引吗？
不会，只会在A:{B,C}上使用索引。
 - 如果一个分片（Shard）停止或很慢的时候，发起一个查询会怎样？
如果一个分片停止了，除非查询设置了“Partial”选项，否则查询会返回一个错误。如果一个分片响应很慢，MongoDB会等待它的响应。

Redis、Memcache和MongoDB的区别
=========================
从以下几个维度，对redis、memcache、mongoDB 做了对比，

1、性能

都比较高，性能对我们来说应该都不是瓶颈

总体来讲，TPS方面redis和memcache差不多，要大于mongodb

2、操作的便利性

memcache数据结构单一

redis丰富一些，数据操作方面，redis更好一些，较少的网络IO次数

mongodb支持丰富的数据表达，索引，最类似关系型数据库，支持的查询语言非常丰富

3、内存空间的大小和数据量的大小

redis在2.0版本后增加了自己的VM特性，突破物理内存的限制；可以对key value设置过期时间（类似memcache）

memcache可以修改最大可用内存,采用LRU算法

mongoDB适合大数据量的存储，依赖操作系统VM做内存管理，吃内存也比较厉害，服务不要和别的服务在一起

4、可用性（单点问题）

对于单点问题，

redis，依赖客户端来实现分布式读写；主从复制时，每次从节点重新连接主节点都要依赖整个快照,无增量复制，因性能和效率问题，

所以单点问题比较复杂；不支持自动sharding,需要依赖程序设定一致hash 机制。

一种替代方案是，不用redis本身的复制机制，采用自己做主动复制（多份存储），或者改成增量复制的方式（需要自己实现），一致性问题和性能的权衡

Memcache本身没有数据冗余机制，也没必要；对于故障预防，采用依赖成熟的hash或者环状的算法，解决单点故障引起的抖动问题。

mongoDB支持master-slave,replicaset（内部采用paxos选举算法，自动故障恢复）,auto sharding机制，对客户端屏蔽了故障转移和切分机制。

5、可靠性（持久化）

对于数据持久化和数据恢复，

redis支持（快照、AOF）：依赖快照进行持久化，aof增强了可靠性的同时，对性能有所影响

memcache不支持，通常用在做缓存,提升性能；

MongoDB从1.8版本开始采用binlog方式支持持久化的可靠性

6、数据一致性（事务支持）

Memcache 在并发场景下，用cas保证一致性

redis事务支持比较弱，只能保证事务中的每个操作连续执行

mongoDB不支持事务

7、数据分析

mongoDB内置了数据分析的功能(mapreduce),其他不支持

8、应用场景

redis：数据量较小的更性能操作和运算上

memcache：用于在动态系统中减少数据库负载，提升性能;做缓存，提高性能（适合读多写少，对于数据量比较大，可以采用sharding）

MongoDB:主要解决海量数据的访问效率问题

Java的threadlocal类
-----------------
java的threadlocal类，该类提供了线程局部 (thread-local) 变量。这些变量不同于它们的普通对应物，因为访问某个变量（通过其 get 或 set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。ThreadLocal 实例通常是类中的 private static 字段，它们希望将状态与某一个线程（例如，用户 ID 或事务 ID）相关联。——出自JDK1.6文档。
Thread类中有这么一个变量，threadLocals，这个属性就是ThreadLocalMap类型的。
```java
/* ThreadLocal values pertaining to this thread. This map is maintained
 * by the ThreadLocal class. */
ThreadLocal.ThreadLocalMap threadLocals = null;
```
threadLocals属于当前线程实例，即，每一个线程都有一个自己的threadLocals属性。
ThreadLocalMap中底层实现，是以ThreadLocal类型实例对象为key的hashmap。比如下面代码，key就是name，value就是haha（通过set方法实现设置）。
```java
ThreadLocal<String> name = new ThreadLocal<>();
name.set("haha");
System.out.println(name.get());
```
下面的例子出自《疯狂java讲义》
```java
package com.audi;

/**
 * Description:
 * <br/>网站: <a href="http://www.crazyit.org">疯狂Java联盟</a>
 * <br/>Copyright (C), 2001-2016, Yeeku.H.Lee
 * <br/>This program is protected by copyright laws.
 * <br/>Program Name:
 * <br/>Date:
 * @author Yeeku.H.Lee kongyeeku@163.com
 * @version 1.0
 */
class Account
{
	/* 定义一个ThreadLocal类型的变量，该变量将是一个线程局部变量
	每个线程都会保留该变量的一个副本 */
	private ThreadLocal<String> name = new ThreadLocal<>();
	// 定义一个初始化name成员变量的构造器
	public Account(String str)
	{
		this.name.set(str);
		// 下面代码用于访问当前线程的name副本的值
		System.out.println("---" + this.name.get());
	}
	// name的setter和getter方法
	public String getName()
	{
		return name.get();
	}
	public void setName(String str)
	{
		this.name.set(str);
	}
}
class MyTest extends Thread
{
	// 定义一个Account类型的成员变量
	private Account account;
	public MyTest(Account account, String name)
	{
		super(name);
		this.account = account;
	}
	public void run()
	{
		// 循环10次
		for (int i = 0 ; i < 10 ; i++)
		{
			// 当i == 6时输出将账户名替换成当前线程名
			if (i == 6)
			{
				account.setName(getName());
			}
			// 输出同一个账户的账户名和循环变量
			System.out.println(account.getName()
				+ " 账户的i值：" + i);
		}
	}
}
public class ThreadLocalTest
{
	public static void main(String[] args)
	{
		// 启动两条线程，两条线程共享同一个Account
		Account at = new Account("初始名");
		/*
		虽然两条线程共享同一个账户，即只有一个账户名
		但由于账户名是ThreadLocal类型的，所以每条线程
		都完全拥有各自的账户名副本，所以从i == 6之后，将看到两条
		线程访问同一个账户时看到不同的账户名。
		*/
		new MyTest(at , "线程甲").start();
		new MyTest(at , "线程乙").start ();
	}
}
```
代码输出结果如下：
```java
---初始名
null 账户的i值：0
null 账户的i值：1
null 账户的i值：2
null 账户的i值：3
null 账户的i值：4
null 账户的i值：5
线程甲 账户的i值：6
线程甲 账户的i值：7
线程甲 账户的i值：8
线程甲 账户的i值：9
null 账户的i值：0
null 账户的i值：1
null 账户的i值：2
null 账户的i值：3
null 账户的i值：4
null 账户的i值：5
线程乙 账户的i值：6
线程乙 账户的i值：7
线程乙 账户的i值：8
线程乙 账户的i值：9
```

JDK动态代理与CGLIB动态代理
-----------------

jdk动态代理
=======
jdk动态需要被代理的对象有接口，至于为什么，这是跟JDK动态代理的实现过程有关的。
一般的JDK动态代理都使用JAVA提供的**Proxy**类的
```java
public static Object newProxyInstance(ClassLoader loader,
                                      Class<?>[] interfaces,
                                      InvocationHandler h)
                               throws IllegalArgumentException
```
来返回一个代理对象，其中第二个参数就是被代理对象的接口名称，所以JDK 的动态代理是一定要有接口的。与之相对的，CGLIB动态代理则不需要。
JDK动态代理需要实现java.lang.reflect.InvocationHandler接口，通过实现接口的invoke方法实现动态代理。
```java
Object invoke(Object proxy,
              Method method,
              Object[] args)
              throws Throwable
```

CGLIB动态代理
=========
使用cglib实现动态代理，首先通过net.sf.cglib.proxy.Enhancer的create方法获得代理对象 ，然后通过实现net.sf.cglib.proxy.MethodInterceptor接口的intecept方法，从而实现动态代理。
为什么spring的DAO只使用接口就可以调用mybatis的xml文件
--------------------------
关键在于动态代理，而且使用的还是JDK动态代理。
mybatis会根据相应的接口声明，使用sqlsession.getMapper(xxxDao.class)方法，通过使用JDK动态代理生成一个DAO实例对象。当我们使用DAO接口的某一个方法时，mybatis会根据这个方法的方法名和参数类型，确定xml文件中的statement ID，最终调用sqlSession.select(statement ID,...)来执行sql。

mybatis的执行过程
------------
```c
                                应用程序
配置文件config.xml    →          Configuration
        ↑                           ↓
        ↑                           ↓
映射文件sqlMapConfig.xml        SqlSessionFactory
                                    ↓
                                SqlSession      →  Mapped Statement
                                    ↓
                                transaction
```             

什么是restful架构？
-------------

 1. 每一个URI代表一种资源
 2. customer和server之间，传递这种资源的某种表现层；
 3. 客户端通过4个http动词，对服务器资源进行操作
 

http的get、post、delete、put
------------------------

 1. get的后退按钮无害，post的后退，数据会被重新提交；
 2. get的编码类型application/x-www-form-uri，post编码类型encodedapplication/x-www-form-urlencoded或multipart/form-data
 3. get历史数据保留在浏览器历史中，post的则不会保存
 4. get对数据长度有限制，一般为1-2KB，只允许ASCII码，post没有限制
 5. get安全性较差，不适合发送密码等数据，post更安全
 6. get/postbenzhix本质上都是TCP链接，get产生一个TCP数据包，post产生两个TCP数据包。get请求，浏览器会把http header data一并发送出去，响应为200。post请求，浏览器先发送header，服务器响应100后，浏览器继续发送data，然后返回200.
 

hash索引和BTree索引
--------------
 
 1. 大量不同数据等值精确查询，hash索引更快；
 2. hash索引不支持复合索引的最左匹配规则，即(where a=? and b=?）,index(a,b,c)相当于范围查找，不走索引。
 3. hash索引不支持排序，hash索引不支持模糊查找
 +++++++++++++++++++++++++++++++++++++++++++++
存储引擎                |索引类型
 +++++++++++++++++++++++++++++++++++++++++++++
InnoDB                  |BTree、hash
 +++++++++++++++++++++++++++++++++++++++++++++
MyISAM                  |BTree
 +++++++++++++++++++++++++++++++++++++++++++++
memory/heap             |BTree、hash
 +++++++++++++++++++++++++++++++++++++++++++++
NDB                     |Hash、BTree
 +++++++++++++++++++++++++++++++++++++++++++++
InnoDB存储引擎支持的hash index是自适应的，InnoDB会根据表的使用情况，自动为表生成索引，不能认为干预是否在一张表中生成hash index。

String类的hashcode()是怎么实现的？
-------------------------
先上JDK8的源码：
```java
   /**
     * Returns a hash code for this string. The hash code for a
     * {@code String} object is computed as
     * <blockquote><pre>
     * s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]
     * </pre></blockquote>
     * using {@code int} arithmetic, where {@code s[i]} is the
     * <i>i</i>th character of the string, {@code n} is the length of
     * the string, and {@code ^} indicates exponentiation.
     * (The hash value of the empty string is zero.)
     *
     * @return  a hash code value for this object.
     */
    public int hashCode() {
        int h = hash;
        if (h == 0 && value.length > 0) {
            char val[] = value;

            for (int i = 0; i < value.length; i++) {
                h = 31 * h + val[i];
            }
            hash = h;
        }
        return h;
    }
```
其实计算公式就是：
s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]
至于为什么选31呢？
是因为：
31可以表示成11111，If it were even and the multiplication overflowed, information would be lost, as multiplication by 2 is equivalent to shifting。
另外一个原因是： 31的乘法可以由i*31== (i<<5)-i来表示（也就是32×i-i），现在很多虚拟机里面都有做相关优化，使用31的原因可能是为了更好的分配hash地址，并且31只占用5bits！

spring的AOP使用了什么设计模式
-------------------
spring的AOP使用了代理模式和策略模式（策略是指使用JDK动态代理hai）。

spring的filter使用了什么设计模式
----------------------
参考链接http://www.flyne.org/article/693
使用的是责任链设计模式。

drools中的有状态session和无状态的session
------------------------------

 - 无状态的session（stateless Kie Session）

可以理解成一个函数。但是又有一些细微的差别，它的输入参数的限定比较宽松，多次调用session产生的结果之间相互不会影响。
其实，StatelessKieSession是StatefulKieSession的子类。无状态session规则的匹配通过execute()一个函数就可以实现。技术上来说，无状态的session完全可以使用有状态的session来进行替代。但是无状态session更加强调规则的匹配是one-shot evaluation。
无状态session适合于**数据验证，计算（比如风险评估、按揭利率）、数据过滤、消息路由**等。
无状态session不需要进行dispose。after  each invocation of any of the execute() methods，the resources used for thr executtion are freed。 at this point，the same stateless kie session is ready for another execution round if required。each executio() invocation will then be independent。

 - 有状态session（stateful Kie Session）
drools默认的session类型就是有状态的。有状态session与无状态session的一个显著区别就是，我们不用等到全部输入参数到达才进行规则匹配，可以在一部分参数到达以后就进行计算，另外一部分参数到达以后继续进行运算，两次运算看成实在一个session中完成的，相互之间具有关联关系。
有状态session使用完成以后必须进行释放。
有状态session的运算需要先使用insert函数插入fact，然后调用fireAllRules方法进行规则运算。

nginx负载均衡
---------
参考链接：http://blog.csdn.net/tjcyjd/article/details/50695922
nginx负载均衡有5种配置方式：

 1. 轮询（默认）
 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。
 
 2. weight
指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。
例如：
upstream bakend {
server 192.168.0.14 weight=10;
server 192.168.0.15 weight=90;
}
 3. ip_hash
每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。
例如：
upstream bakend {
ip_hash;
server 192.168.0.14:88;
server 192.168.0.15:80;
}
 4. url_hash（第三方）
 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。
 例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法
upstream backend {
server squid1:3128;
server squid2:3128;
hash $request_uri;
hash_method crc32;
}
 5. fair（第三方）
按后端服务器的响应时间来分配请求，响应时间短的优先分配。
upstream backend {
server server1;
server server2;
fair;
}

Java锁的类型
--------
参考
 1. 公平锁/非公平锁
 公平锁是指多个线程按照申请锁的顺序来获取锁。
 非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。
 对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。
对于Synchronized而言，也是一种非公平锁，由于其并不像ReentrantLock是通过AQS（AbstractQueuedSynchronized）的来实现线程调度，所以并没有任何办法使其变成公平锁。
 2. 可重入锁
 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。说的有点抽象，下面会有一个代码的示例。
 对于Java ReentrantLock而言, 他的名字就可以看出是一个可重入锁，其名字是Re entrant Lock重新进入锁。
对于Synchronized而言,也是一个可重入锁。可重入锁的一个好处是可一定程度避免死锁。
```java
synchronized void setA() throws Exception{
    Thread.sleep(1000);
    setB();
}

synchronized void setB() throws Exception{
    Thread.sleep(1000);
}
```
上面的代码就是一个可重入锁的一个特点，如果不是可重入锁的话，setB可能不会被当前线程执行，可能造成死锁。
 1. 独享锁/共享锁
 独享锁是指该锁一次只能被一个线程所持有。
共享锁是指该锁可被多个线程所持有。
对于Java ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。
读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。
独享锁与共享锁也是通过AQS（AbstractQueuedSynchronized）来实现的，通过实现不同的方法，来实现独享或者共享。
对于Synchronized而言，当然是独享锁。
 2. 互斥锁/读写锁
 上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。
互斥锁在Java中的具体实现就是ReentrantLock
读写锁在Java中的具体实现就是ReadWriteLock
 3. 乐观锁/悲观锁
 乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。
悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。
乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。
从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。
悲观锁在Java中的使用，就是利用各种锁。
乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。
 4. 分段锁
 分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。
我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7与JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是一个ReentrantLock（Segment继承了ReentrantLock)。
当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。
但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。
分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。
 5. 自旋锁
 在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。
 

Java中的AQS
---------
https://www.cnblogs.com/waterystone/p/4920797.html
谈到并发，不得不谈ReentrantLock；而谈到ReentrantLock，不得不谈AbstractQueuedSynchronized（AQS）！
它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。如下图所示：
![此处输入图片的描述][1]


  [1]: http://images2015.cnblogs.com/blog/721070/201705/721070-20170504110246211-10684485.png
  
  

java实现冒泡排序和快速排序
---------------
```java
package com.audi.mySort;

public class Sort
{
	public static void bubbleSort(int a[])
	{
		if (a == null|| a.length==0 ||a.length==1)
		{
			return;
		}
		int temp=0;
		for (int i = 0; i < a.length-1; i++)
		{
			for (int j = 0; j < a.length-i-1; j++)
			{
				if (a[j]>a[j+1])
				{
					temp=a[j];
					a[j]=a[j+1];
					a[j+1]=temp;
				}
			}
		}
	}

	//看不懂的时候看一下这个http://developer.51cto.com/art/201403/430986.htm
	public static void quickSort(int a[],int low,int high)
	{
		if (a==null|| a.length==0 ||a.length==1)
		{
			return;
		}
		int i,j,pivoit,temp;
		if (low<high)
		{
			pivoit = a[low];
			i=low;
			j=high;
			while(i<j)
			{
				while(i<j&&a[j]>=pivoit)
					j--;
				while(i<j&&a[i]<=pivoit)
					i++;
				if (i<j)
				{
					temp=a[i];
					a[i]=a[j];
					a[j]=temp;
				}
			}
			a[low]=a[i];
			a[i]=pivoit;
			quickSort(a, low, i-1);
			quickSort(a, i+1, high);
		}
	}
	
	public static void main(String[] args)
	{
		int a[]= {1,34,0,21,342,2,90};
		System.out.println("冒泡排序：");
		Sort.bubbleSort(a);
		for(int i:a)
		{
			System.out.println(i);
		}
		int b[]= {1,34,0,21,342,2,90};
		System.out.println("快速排序：");
		Sort.quickSort(b, 0, b.length-1);
		for(int i:b)
		{
			System.out.println(i);
		}
	}
}

```
为什么快速排序都要先从右边开始？
参考链接http://blog.csdn.net/w282529350/article/details/50982650
考虑数组6 1 2 7 9
6在左，9在右  我们将6作为基数。
假设从左边开始（与正确程序正好相反）
于是i 就会移动到现在的 数字 7 那个位置停下来，而  j 原来在 数字 9 那个位置 ，因为
while(arr[j]>=temp&&i<j)
于是，j 也会停留在数字7 那个位置，于是问题来了。当你最后交换基数6与7时，不对呀！！。
问题在于当我们先从在边开始时，那么 i 所停留的那个位置肯定是大于基数6的，而在上述例子中，为了满足 i<j 于是 j也停留在7的位置
但最后交换回去的时候，7就到了左边，不行，因为我们原本 交换后数字6在边应该是全部小于6，右边全部大于6.但现在不行了。
于是，我们必须从右边开始，也就是从基数的对面开始。