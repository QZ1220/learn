# 笔记整理

标签（空格分隔）： 笔记

---

涉及的东西还是很多很广的，总体而言感觉技术实力还是很不错的。

问题
=====

> 大致的一些内容包括java、jvm、*多线程、线程池、进程与线程*、数据结构、*set的底层实现*、*Java的深复制浅复制*、*main函数执行所发生的一系列动作*、*sql优化、什么情况导致不走索引*、*数据库的隔离性的4个等级*、*常用的设计模式*、*单例的实现及多种实现方式*、*GC回收算法、现在的JDK使用什么GC算法*、*Java的reader和writer使用了什么设计模式*、spring的前后端交互时所经历的一系列过程、object怎么转换成json串的、*hashmap遇到key重复会怎样、怎么取出重复位置指定的value、collection与collections的区别*、*java中length,length(),size()区别*，*quartz的任务调度怎么进行的以及如何选择节点来执行定时任务*、*hashcode函数与equals方法*、偏向锁、数据库索引优化、查询优化和存储优化、*string类的源码分析、为什么是不可变的*、*jvm新生代1和新生代2的区别*、*什么是Servlet，Servlet的作用，生命周期，如何创建、配置Servlet*、*Spring事务的传播特性和隔离级别*、bean的生命周期、applicationContext和beanFactory的关系、*concurrentHashMap的底层实现*、*java类的加载过程*、分布式的基本知识、*violatile关键字*、*为什么spring的DAO只使用接口就可以调用xml*、spring的filter使用了什么设计模式、spring的AOP使用了什么设计模式、*Java的threadlocal类*、*JDK动态代理与CGLIB动态代理*、*nginx负载均衡、Java锁的类型、java实现冒泡排序和快速排序*、*CMS和G1、jvm内存泄漏检测、jdk1.8舍去了永久区*、*Java中的Error能不能被Catch*、位图索引、随机森林、聚类、分类算法、线程调度相关的queue、hadoop简介、分布式事务一致性、分布式锁、Integer包装类、hashmap插入null的具体操作、stringbuffer和stringbuilder的扩容函数的具体实现、统计出某个log文件中ip出现次数最多的ip、找到当前系统的最大的文件、当fixedThreadPool的等待队列满了以后会怎样、Paxos算法

技术栈
======

：前端angular、vue、react，后端微服务、MVC、spring、springMVC、springBoot、springCloud，分布式Docker、hadoop+spark

collection与collections的区别
-------------------------
/home/audi/Pictures/collection.png
首先，Collection是接口，Collections是类。
Collection是set和list的父接口，需要注意的是map也是一个接口，它和collection没有继承派生的关系。
Collections是针对集合类的一个工具类，提供了操作集合的工具方法：一系列静态方法实现对各种集合的搜索、排序、线程安全化(比如Collections.synchronizedList使链表安全)等操作。它通过将**构造函数私有化**来避免创建对象。

java集合架构图如下所示：

![此处输入图片的描述][1]

hashmap遇到key重复会怎样、怎么取出重复位置指定的value
----------------------------------
首先看一篇参考笔记：
文章写的很好，以JDK1.8为例，详细的讲述了hashmap中几个重要的函数，get\put\resize等。
hashmap的key重复的话，那么必然会导致记得得到的hash（key）重复，存储的位置也会相同，所以是value会产生**覆盖**的效果。
hashmap的所有节点元素都是一个Node<k,v>，Node节点的源代码（链接上的哥们儿给的注释，挺详细的）如下所示：
```java
    //Node是单向链表，它实现了Map.Entry接口
    static class Node<k,v> implements Map.Entry<k,v> {
        final int hash; K key;
        V value;
        Node<k,v> next;
        //构造函数Hash值 键 值 下一个节点
        Node(int hash, K key, V value, Node<k,v> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }
     
        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + = + value; }
     
        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }
     
        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }
        //判断两个node是否相等,若key和value都相等，返回true。可以与自身比较为true
        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<!--?,?--> e = (Map.Entry<!--?,?-->)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
```
hashmap的构造函数可以指定初始容量（注意不一定要是2的幂次，是基数也可以，但是tableSizeFor函数会自动调整为最近的2的幂次），也可以不指定，此时大小为默认值16，也可以指定hahmap的装载比，源码如下：
```java
//构造函数1
public HashMap(int initialCapacity, float loadFactor) {
    //指定的初始容量非负
    if (initialCapacity < 0)
        throw new IllegalArgumentException(Illegal initial capacity:  +
                                           initialCapacity);
    //如果指定的初始容量大于最大容量,置为最大容量
    if (initialCapacity > MAXIMUM_CAPACITY)
        initialCapacity = MAXIMUM_CAPACITY;
    //填充比为正
    if (loadFactor <= 0 || Float.isNaN(loadFactor))
        throw new IllegalArgumentException(Illegal load factor:  +
                                           loadFactor);
    this.loadFactor = loadFactor;
    this.threshold = tableSizeFor(initialCapacity);//新的扩容临界值
}
 
//构造函数2
public HashMap(int initialCapacity) {
    this(initialCapacity, DEFAULT_LOAD_FACTOR);
}
 
//构造函数3
public HashMap() {
    this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
}
 
//构造函数4用m的元素初始化散列映射
public HashMap(Map<!--? extends K, ? extends V--> m) {
    this.loadFactor = DEFAULT_LOAD_FACTOR;
    putMapEntries(m, false);
}
```
hashmap获取value的机制：使用hash（key）和n-1（n=tab.length）进行位与操作，避免出现越界的情况。
取元素的时候，首先根据hash（key）定位元素的位置，然后首个元素是不是需要的元素（判断的原则是hash值相等，且key相等），如果不是就继续取元素，取的时候分为链表遍历和红黑树遍历。
```java
public V get(Object key) {
        Node<K,V> e;
        return (e = getNode(hash(key), key)) == null ? null : e.value;
    }
	  /**
     * Implements Map.get and related methods
     *
     * @param hash hash for key
     * @param key the key
     * @return the node, or null if none
     */
	final Node<K,V> getNode(int hash, Object key) {
        Node<K,V>[] tab;//Entry对象数组
	Node<K,V> first,e; //在tab数组中经过散列的第一个位置
	int n;
	K k;
	/*找到插入的第一个Node，方法是hash值和n-1相与，tab[(n - 1) & hash]*/
	//也就是说在一条链上的hash值相同的
        if ((tab = table) != null && (n = tab.length) > 0 &&(first = tab[(n - 1) & hash]) != null) {
	/*检查第一个Node是不是要找的Node*/
            if (first.hash == hash && // always check first node
                ((k = first.key) == key || (key != null && key.equals(k))))//判断条件是hash值要相同，key值要相同
                return first;
	  /*检查first后面的node*/
            if ((e = first.next) != null) {
                if (first instanceof TreeNode)
                    return ((TreeNode<K,V>)first).getTreeNode(hash, key);
				/*遍历后面的链表，找到key值和hash值都相同的Node*/
                do {
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        return e;
                } while ((e = e.next) != null);
            }
        }
        return null;
    }
```
hashmap存储Node的机制：下面是源码。
put的时候首先判断hash表是否创建了，没有就创建一个；
然后使用hash（key）&（n-1）来确定该put的位置，如果要put的位置没有元素，那么直接put；否则，
如果key相同，那么就执行替换操作；
如果不同，且是treeNode，那么就按照红黑树的put方式放入元素；
是链表，就在链表的尾部put元素。
```java
public V put(K key, V value) {
        return putVal(hash(key), key, value, false, true);
    }
	 /**
     * Implements Map.put and related methods
     *
     * @param hash hash for key
     * @param key the key
     * @param value the value to put
     * @param onlyIfAbsent if true, don't change existing value
     * @param evict if false, the table is in creation mode.
     * @return previous value, or null if none 注意这个返回值情况，因为在set的add方法中会调用这个putval方法，并且通过返回值来判断是否加入成功的
     * putval函数会在执行完put操作以后进行判断，是否需要扩容resize
     */
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; 
	Node<K,V> p; 
	int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;
	/*如果table的在（n-1）&hash的值是空，就新建一个节点插入在该位置*/
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
	/*表示有冲突,开始处理冲突*/
        else {
            Node<K,V> e; 
	    K k;
	/*检查第一个Node，p是不是要找的值*/
            if (p.hash == hash &&((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                for (int binCount = 0; ; ++binCount) {
		/*指针为空就挂在后面*/
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
		       //如果冲突的节点数已经达到8个，看是否需要改变冲突节点的存储结构，　　　　　　　　　　　　　
　　　　　　　　　　　　//treeifyBin首先判断当前hashMap的长度，如果不足64，只进行
                        //resize，扩容table，如果达到64，那么将冲突的存储结构为红黑树
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
		/*如果有相同的key值就结束遍历*/
                    if (e.hash == hash &&((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
	/*就是链表上有相同的key值*/
            if (e != null) { // existing mapping for key，就是key的Value存在
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;//返回存在的Value值
            }
        }
        ++modCount;
     /*如果当前大小大于门限，门限原本是初始容量*0.75*/
        if (++size > threshold)
            resize();//扩容两倍
        afterNodeInsertion(evict);
        return null;
    }
```
hashmap的resize()函数
构造hash表时，如果不指明初始大小，默认大小为16（即Node数组大小16），如果Node[]数组中的元素达到（填充比*Node.length）重新调整HashMap大小 变为原来2倍大小,**扩容很耗时**
resize函数不仅仅要完成map的扩容，还要完成hash表元素位置的迁移。
resize的时候，可能有2种情况。
一是hash表本就是空的，且门限值thresHold>0,此时需要将 
```java
newCap = oldThr;  
```
如果门限值thresHold<=0,需要按照默认大小进行hash表的创建
二是hash表不为空，此时需要进行2倍扩容和Node元素存储位置更改，元素位置的移动要参考http://www.importnew.com/20386.html，注意其中resize的部分

下面是resize函数的源码：
```java
  /**
     * Initializes or doubles table size.  If null, allocates in
     * accord with initial capacity target held in field threshold.
     * Otherwise, because we are using power-of-two expansion, the
     * elements from each bin must either stay at same index, or move
     * with a power of two offset in the new table.
     *
     * @return the table
     */
    final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
		
	/*如果旧表的长度不是空*/
        if (oldCap > 0) {
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
	/*把新表的长度设置为旧表长度的两倍，newCap=2*oldCap*/
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
	      /*把新表的门限设置为旧表门限的两倍，newThr=oldThr*2*/
                newThr = oldThr << 1; // double threshold
        }
     /*如果旧表的长度的是0，就是说第一次初始化表*/
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
		
		
		
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;//新表长度乘以加载因子
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
	/*下面开始构造新表，初始化表中的数据*/
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;//把新表赋值给table
        if (oldTab != null) {//原表不是空要把原表中数据移动到新表中	
            /*遍历原来的旧表*/		
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)//说明这个node没有链表直接放在新表的e.hash & (newCap - 1)位置
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
	/*如果e后边有链表,到这里表示e后面带着个单链表，需要遍历单链表，将每个结点重*/
                    else { // preserve order保证顺序
					////新计算在新表的位置，并进行搬运
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
						
                        do {
                            next = e.next;//记录下一个结点
			  //新表是旧表的两倍容量，实例上就把单链表拆分为两队，
　　　　　　　　　　　　　　//e.hash&oldCap为偶数一队，e.hash&oldCap为奇数一对
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
						
                        if (loTail != null) {//lo队不为null，放在新表原位置
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {//hi队不为null，放在新表j+oldCap位置
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    }
```
如果同一个entryset位置存储的元素>=8时，hashmap就会采用红黑树的方式来存储元素，可以查找的时间复杂度从O(N)降为O(logN)。

JDK1.8HashMap的红黑树是这样解决的：
------------------------

如果某个桶中的记录过大的话（当前是TREEIFY_THRESHOLD = 8），HashMap会动态的使用一个专门的treemap实现来替换掉它。这样做的结果会更好，是O(logn)，而不是糟糕的O(n)。
它是如何工作的？前面产生冲突的那些KEY对应的记录只是简单的追加到一个链表后面，这些记录只能通过遍历来进行查找。但是超过这个阈值后HashMap开始将列表升级成一个二叉树，使用哈希值作为树的分支变量，如果两个哈希值不等，但指向同一个桶的话，较大的那个会插入到右子树里。如果哈希值相等，HashMap希望key值最好是实现了Comparable接口的，这样它可以按照顺序来进行插入。这对HashMap的key来说并不是必须的，不过如果实现了当然最好。如果没有实现这个接口，在出现严重的哈希碰撞的时候，你就并别指望能获得性能提升了。

java中length,length(),size()区别
-----------------------------
java中的length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了length这个属性.
java中的length()方法是针对字符串String说的,如果想看这个字符串的长度则用到length()这个方法，需要注意的是，它计算长度是unicode编码格式的，也就是说1个中文字符算1个，而不是2个。
下面是string类的length()方法的源码：
```java
 /**
     * Returns the length of this string.
     * The length is equal to the number of <a href="Character.html#unicode">Unicode
     * code units</a> in the string.
     *
     * @return  the length of the sequence of characters represented by this
     *          object.
     */
    public int length() {
        return value.length;
    }
```
java中的size()方法是针对泛型集合（Map，Collection）说的,如果想看这个泛型有多少个元素,就调用此方法来查看
map接口中的size()源码：
```java
    /**
     * Returns the number of key-value mappings in this map.  If the
     * map contains more than <tt>Integer.MAX_VALUE</tt> elements, returns
     * <tt>Integer.MAX_VALUE</tt>.
     *
     * @return the number of key-value mappings in this map
     */
    int size();
```
collection接口中的size()源码：
```java
/**
     * Returns the number of elements in this collection.  If this collection
     * contains more than <tt>Integer.MAX_VALUE</tt> elements, returns
     * <tt>Integer.MAX_VALUE</tt>.
     *
     * @return the number of elements in this collection
     */
    int size();
```

进程与线程 区别
--------
**进程**：cpu资源分配的最小单位，拥有独立的内存单元，可以拥有一个或者多个线程。多个线程共享内存。共享复杂，但是同步简单，需要使用IPC（Inter-Process Communication）。

**线程**：cpu调度的最小单位，线程间共享进程的资源，共享简单，但是同步复杂，需要加锁。

**同一进程间的线程究竟共享哪些资源呢，而又各自独享哪些资源呢？**

 - 共享的资源有

**a. 堆**  由于堆是在进程空间中开辟出来的，所以它是理所当然地被共享的；因此new出来的都是共享的（16位平台上分全局堆和局部堆，局部堆是独享的）

**b. 全局变量** 它是与具体某一函数无关的，所以也与特定线程无关；因此也是共享的

**c. 静态变量** 虽然对于局部变量来说，它在代码中是“放”在某一函数中的，但是其存放位置和全局变量一样，存于堆中开辟的.bss和.data段，是共享的

**d. 文件等公用资源**  这个是共享的，使用这些公共资源的线程必须同步。Win32 提供了几种同步资源的方式，包括信号、临界区、事件和互斥体。

 - 独享的资源有

**a. 栈** 栈（虚拟机栈）是独享的

**b. 寄存器**  这个可能会误解，因为电脑的寄存器是物理的，每个线程去取值难道不一样吗？其实线程里存放的是副本，包括程序计数器PC

**c.程序计数器**它可以看做是当前线程所执行的字节码的行号指示器。

进程间通信方式
---------

 - 管道(pipe)

    管道是一种具有两个端点的通信通道，一个管道实际上就是只存在在内存中的**文件**，**对这个文件操作需要两个已经打开文件进行，他们代表管道的两端，也叫两个句柄**，管道是一种特殊的文件，不属于一种文件系统，而是一种独立的文件系统，有自己的数据结构，根据管道的使用范围划分为无名管道和命名管道。
**无名管道**用于父进程和子进程之间，通常父进程创建管道，然后由通信的子进程继承父进程的读端点句柄和写端点句柄，或者父进程有读写句柄的子进程，这些子进程可以使用管道直接通信，不需要通过父进程。
**命名管道**，命名管道是为了解决无名管道只能在父子进程间通信而设计的，命名管道是建立在实际的磁盘介质或文件系统(而不是只存在内存中)，任何进程可以通过文件名或路径建立与该文件的联系，命名管道需要一种FIFO文件(有先进先出的原则)，虽然FIFO文件的inode节点在磁盘上，但仅是一个节点而已，文件的数据还是存在于内存缓冲页面中，和普通管道相同。

 - 信号量( semophore ) ：

 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

 - 消息队列( message queue ) ：

 消息队列是消息的链表，包括Posix消息队列和system v消息队列(Posix常用于线程，system常用于进程)，有权限的进程可以向消息队列中添加消息，有读权限的进程可以读走消息队列的消息。
消息队列克服了信号承载信息量少，管道只能承载无格式字节流及缓冲区大小受限等缺陷。

 - 共享内存( shared memory )

 ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是**最快**的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。传递文件最好用共享内存的方式。 
套接字( socket ) ： 套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于**不同机器**间的进程通信。

多线程之生产者消费者
----------
注意，下面的代码中的if判断，应该改为while循环。
下面的代码创建了2个消费者线程，这样会出现数据越界的错误，就是因为没有使用while的原因。
```java
/*
    生产和消费
*/
package multiThread;

class SynStack 
{
    private char[] data = new char[6];
    private int cnt = 0; //表示数组有效元素的个数
    
    public synchronized void push(char ch)
    {
        if (cnt >= data.length)
        {
            try
            {
                System.out.println("生产线程"+Thread.currentThread().getName()+"准备休眠");
                this.wait();
                System.out.println("生产线程"+Thread.currentThread().getName()+"休眠结束了");
            }
            catch (Exception e)
            {
                e.printStackTrace();
            }
        }
        this.notify(); 
        data[cnt] = ch;
        ++cnt;
        System.out.printf("生产线程"+Thread.currentThread().getName()+"正在生产第%d个产品，该产品是: %c\n", cnt, ch);
    }
    
    public synchronized char pop()
    {
        char ch;
        if (cnt <= 0)
        {
            try
            {
                System.out.println("消费线程"+Thread.currentThread().getName()+"准备休眠");
                this.wait();
                System.out.println("消费线程"+Thread.currentThread().getName()+"休眠结束了");
            }
            catch (Exception e)
            {
                e.printStackTrace();
            }
        }
        this.notify();
        ch = data[cnt-1];
        System.out.printf("消费线程"+Thread.currentThread().getName()+"正在消费第%d个产品，该产品是: %c\n", cnt, ch);
        --cnt;
        return ch;        
    }    
}

class Producer implements Runnable
{
    private SynStack ss = null;
    public Producer(SynStack ss)
    {
        this.ss = ss;
    }
    
    public void run()
    {
        char ch;
        for (int i=0; i<10; ++i)
        {
//            try{
//            Thread.sleep(100);
//            }
//            catch (Exception e){            
//            }
                
            ch = (char)('a'+i);
            ss.push(ch);
        }
    }
}

class Consumer implements Runnable
{
    private SynStack ss = null;
    
    public Consumer(SynStack ss)
    {
        this.ss = ss;
    }
    
    public void run()
    {
        for (int i=0; i<10; ++i)
        {
            /*try{
            Thread.sleep(100);
            }
            catch (Exception e){            
            }*/
            
            //System.out.printf("%c\n", ss.pop());
            ss.pop();
        }
    }
}


public class TestPC2
{
    public static void main(String[] args)
    {
        SynStack ss = new SynStack();
        Producer p = new Producer(ss);
        Consumer c = new Consumer(ss);
        
        
        Thread t1 = new Thread(p);
        t1.setName("1号");
        t1.start();
        /*Thread t2 = new Thread(p);
        t2.setName("2号");
        t2.start();*/
                
        Thread t6 = new Thread(c);
        t6.setName("6号");
        t6.start();
        Thread t7 = new Thread(c);
        t7.setName("7号");
        t7.start();
    }
}
```

Java中的访问控制符号
------------
需要特别说明“无修饰符”这个情况，子类能否访问父类中无修饰符的变量/方法，取决于**子类的位置**。如果**子类和父类在同一个包**中，那么子类可以访问父类中的无修饰符的变量/方法，否则不行。
```java
/*
		 * ----------
           			private default protected   public
		----------
		同一个类    	√       √       √           √
		----------	
		同一个包    			√		√			√
		----------
		子类							√			√
		----------
		全局										√
		----------
		 */
```


Java中set的底层实现
-------------
Set是对数学上集的抽象,Set中不包含重复的元素.如何界定是否是重复元素?Set最多可含一个null元素;对于任意的非null元素e1和e2,都满足e1.equals(e2)==false.
**Set实现**
HashSet是使用一个哈希表存储元素的,是非排序的,可以随机访问,是Set的最优性能实现.TreeSet实现了treeMap接口,使用一个红黑树（如果出现相等的key，那么新的元素直接替换旧的元素）来存储元素,提供了元素的有序存储和访问.
```java
// Dummy value to associate with an Object in the backing Map
// 意思就是为了和hashmap的key-value方式保持一致，set这里假定了一个value值，没有实际意义
    private static final Object PRESENT = new Object();
/**
     * Constructs a new, empty set; the backing <tt>HashMap</tt> instance has
     * default initial capacity (16) and load factor (0.75).
     *   可以看出hashset的底层是使用hashmap来实现的
     */
    public HashSet() {
        map = new HashMap<>();
    }

    /**
     * Constructs a new set containing the elements in the specified
     * collection.  The <tt>HashMap</tt> is created with default load factor
     * (0.75) and an initial capacity sufficient to contain the elements in
     * the specified collection.
     *
     * @param c the collection whose elements are to be placed into this set
     * @throws NullPointerException if the specified collection is null
     * 注意下面构造函数的+1操作，因为是int格式转换的时候浮点数是向下转的，也就是说6.66f会转为6
     */
    public HashSet(Collection<? extends E> c) {
        map = new HashMap<>(Math.max((int) (c.size()/.75f) + 1, 16));
        addAll(c);
    }

    /**
     * Constructs a new, empty set; the backing <tt>HashMap</tt> instance has
     * the specified initial capacity and the specified load factor.
     *
     * @param      initialCapacity   the initial capacity of the hash map
     * @param      loadFactor        the load factor of the hash map
     * @throws     IllegalArgumentException if the initial capacity is less
     *             than zero, or if the load factor is nonpositive
     */
    public HashSet(int initialCapacity, float loadFactor) {
        map = new HashMap<>(initialCapacity, loadFactor);
    }

    /**
     * Constructs a new, empty set; the backing <tt>HashMap</tt> instance has
     * the specified initial capacity and default load factor (0.75).
     *
     * @param      initialCapacity   the initial capacity of the hash table
     * @throws     IllegalArgumentException if the initial capacity is less
     *             than zero
     */
    public HashSet(int initialCapacity) {
        map = new HashMap<>(initialCapacity);
    }

    /**
     * Constructs a new, empty linked hash set.  (This package private
     * constructor is only used by LinkedHashSet.) The backing
     * HashMap instance is a LinkedHashMap with the specified initial
     * capacity and the specified load factor.
     *
     * @param      initialCapacity   the initial capacity of the hash map
     * @param      loadFactor        the load factor of the hash map
     * @param      dummy             ignored (distinguishes this
     *             constructor from other int, float constructor.)
     * @throws     IllegalArgumentException if the initial capacity is less
     *             than zero, or if the load factor is nonpositive
     * 下满函数是default访问权限的，作域在当前类或者同包中
     */
    HashSet(int initialCapacity, float loadFactor, boolean dummy) {
        map = new LinkedHashMap<>(initialCapacity, loadFactor);
    }
```
由于set要求内部所有的元素都不重复，那么我们就来看一下它的contains函数和add函数以及remove函数的实现过程。
```java
/**
     * Returns <tt>true</tt> if this set contains the specified element.
     * More formally, returns <tt>true</tt> if and only if this set
     * contains an element <tt>e</tt> such that
     * <tt>(o==null&nbsp;?&nbsp;e==null&nbsp;:&nbsp;o.equals(e))</tt>.
     *
     * @param o element whose presence in this set is to be tested
     * @return <tt>true</tt> if this set contains the specified element
     * 这里contains方法调用了hashmap的containsKey方法，因为hashmap的key总是唯一的，不存在重复的情况。
     */
    public boolean contains(Object o) {
        return map.containsKey(o);
    }

    /**
     * Adds the specified element to this set if it is not already present.
     * More formally, adds the specified element <tt>e</tt> to this set if
     * this set contains no element <tt>e2</tt> such that
     * <tt>(e==null&nbsp;?&nbsp;e2==null&nbsp;:&nbsp;e.equals(e2))</tt>.
     * If this set already contains the element, the call leaves the set
     * unchanged and returns <tt>false</tt>.
     *
     * @param e element to be added to this set
     * @return <tt>true</tt> if this set did not already contain the specified
     * element
     * 如果元素存在，那么不对set做任何更改，并且返回false
     */
    public boolean add(E e) {
        return map.put(e, PRESENT)==null;
    }

    /**
     * Removes the specified element from this set if it is present.
     * More formally, removes an element <tt>e</tt> such that
     * <tt>(o==null&nbsp;?&nbsp;e==null&nbsp;:&nbsp;o.equals(e))</tt>,
     * if this set contains such an element.  Returns <tt>true</tt> if
     * this set contained the element (or equivalently, if this set
     * changed as a result of the call).  (This set will not contain the
     * element once the call returns.)
     *
     * @param o object to be removed from this set, if present
     * @return <tt>true</tt> if the set contained the specified element
     * map.remove函数如果成功remove一个存在的元素，那么会返回remove的元素的value 
     */
    public boolean remove(Object o) {
        return map.remove(o)==PRESENT;
    }
```
下面看下TreeSet的实现过程，从源码中可以看出treeset的实现是通过treemap来实现的,TreeMap是NavigableMap的子类,NavigableMap是SortedMap的子类，SortedMap实现了Map接口  
下面是源码：
```java
/**
     * Constructs a set backed by the specified navigable map.
     *  TreeMap是NavigableMap的子类
     */
    TreeSet(NavigableMap<E,Object> m) {
        this.m = m;
    }

    /**
     * Constructs a new, empty tree set, sorted according to the
     * natural ordering of its elements.  All elements inserted into
     * the set must implement the {@link Comparable} interface.
     * Furthermore, all such elements must be <i>mutually
     * comparable</i>: {@code e1.compareTo(e2)} must not throw a
     * {@code ClassCastException} for any elements {@code e1} and
     * {@code e2} in the set.  If the user attempts to add an element
     * to the set that violates this constraint (for example, the user
     * attempts to add a string element to a set whose elements are
     * integers), the {@code add} call will throw a
     * {@code ClassCastException}.
     */
    public TreeSet() {
        this(new TreeMap<E,Object>());
    }

    /**
     * Constructs a new, empty tree set, sorted according to the specified
     * comparator.  All elements inserted into the set must be <i>mutually
     * comparable</i> by the specified comparator: {@code comparator.compare(e1,
     * e2)} must not throw a {@code ClassCastException} for any elements
     * {@code e1} and {@code e2} in the set.  If the user attempts to add
     * an element to the set that violates this constraint, the
     * {@code add} call will throw a {@code ClassCastException}.
     *
     * @param comparator the comparator that will be used to order this set.
     *        If {@code null}, the {@linkplain Comparable natural
     *        ordering} of the elements will be used.
     */
    public TreeSet(Comparator<? super E> comparator) {
        this(new TreeMap<>(comparator));
    }

    /**
     * Constructs a new tree set containing the elements in the specified
     * collection, sorted according to the <i>natural ordering</i> of its
     * elements.  All elements inserted into the set must implement the
     * {@link Comparable} interface.  Furthermore, all such elements must be
     * <i>mutually comparable</i>: {@code e1.compareTo(e2)} must not throw a
     * {@code ClassCastException} for any elements {@code e1} and
     * {@code e2} in the set.
     *
     * @param c collection whose elements will comprise the new set
     * @throws ClassCastException if the elements in {@code c} are
     *         not {@link Comparable}, or are not mutually comparable
     * @throws NullPointerException if the specified collection is null
     */
    public TreeSet(Collection<? extends E> c) {
        this();
        addAll(c);
    }

    /**
     * Constructs a new tree set containing the same elements and
     * using the same ordering as the specified sorted set.
     *
     * @param s sorted set whose elements will comprise the new set
     * @throws NullPointerException if the specified sorted set is null
     */
    public TreeSet(SortedSet<E> s) {
        this(s.comparator());
        addAll(s);
    }
```

为什么需要重写hashcode方法与equals方法
--------------------------
这个主要是考虑到java集合中的set特性，它要求元素不重复，那么需要hashcode和equals都相等才可以。这里需要注意，hashset的底层也是使用hashmap来实现的，hashmap在put元素的时候，就需要先判断hashcode，如果相等，再判断equals，如果还相等，那么证明元素相等，就进行**替换**。
另外，如果不进行重写，对于我们自定义的对象，程序也不知道我们判断两个对象相等的条件是（比如People对象，身份证号码（ID）相等就是相等），所以必须进行重写，否则根本无法判断对象是否重复。
看一下Object.hashCode的通用约定（摘自《Effective Java》第45页）
    

 - 在一个应用程序执行期间，如果一个对象的equals方法做比较所用到的信息没有被修改的话，那么，对该对象调用hashCode方法多次，它必须始终如一地返回 同一个整数。在同一个应用程序的多次执行过程中，这个整数可以不同，即这个应用程序这次执行返回的整数与下一次执行返回的整数可以不一致。
    

 - 如果两个对象根据equals(Object)方法是相等的，那么调用这两个对象中任一个对象的hashCode方法必须产生同样的整数结果。
  

 - 如果两个对象根据equals(Object)方法是不相等的，那么调用这两个对象中任一个对象的hashCode方法，不要求必须产生不同的整数结果。然而，程序员应该意识到这样的事实，对于不相等的对象产生截然不同的整数结果，有可能提高散列表（hash
   table）的性能。

     如果只重写了equals方法而没有重写hashCode方法的话，则会违反约定的第二条：相等的对象必须具有相等的散列码（hashCode）
     同时对于HashSet和HashMap这些基于散列值（hash）实现的类。HashMap的底层处理机制是以数组的方法保存放入的数据的(Node<K,V>[] table)，其中的关键是数组下标的处理。数组的下标是根据传入的元素hashCode方法的返回值再和特定的值异或决定的。如果该数组位置上已经有放入的值了，且传入的键值相等则不处理，若不相等则覆盖原来的值，如果数组位置没有条目，则插入，并加入到相应的链表中。检查键是否存在也是根据hashCode值来确定的。所以如果不重写hashCode的话，可能导致HashSet、HashMap不能正常的运作、
  如果我们将某个自定义对象存到HashMap或者HashSet及其类似实现类中的时候，如果该对象的属性参与了hashCode的计算，那么就不能修改该对象参数hashCode计算的属性了。有可能会移除不了元素，导致内存泄漏。

Java的深复制浅复制
-----------
參考文章http://blog.csdn.net/zhangjg_blog/article/details/18369201
 - 浅复制
对于简单对象，都是浅复制。对于引用对象，浅复制就是栈指针指向堆中同一地址空间。
 - 深复制
 对于堆中的对象，在堆中产生一份相同的拷贝，并在栈中建立一个指向这个地址空间的引用。
Java当中，一般可以通过new关键字或者clone方法创建或者复制一个对象。
程序执行到new操作符时，首先去看new操作符后面的类型，因为知道了类型，才能知道要分配多大的内存空间。分配完内存之后，再调用构造函数，填充对象的各个域，这一步叫做对象的初始化，构造方法返回后，一个对象创建完毕，可以把他的引用（地址）发布到外部，在外部就可以使用这个引用操纵这个对象。
而clone在第一步是和new相似的，都是分配内存，调用clone方法时，分配的内存和源对象（即调用clone方法的对象）相同，然后再使用原对象中对应的各个域，填充新对象的域，填充完成之后，clone方法返回，一个新的相同的对象被创建，同样可以把这个新对象的引用发布到外部。 
下面展示一个完全深拷贝的例子，尤其要注意类中嵌套类（区别于简单对象）的形式。
```java
package com.audi;

public class TestClone
{
	static class Body implements Cloneable
	{
		public Head head;

		public Body()
		{
		}

		public Body(Head head)
		{
			this.head = head;
		}

		@Override
		protected Object clone() throws CloneNotSupportedException
		{
			Body newBody = (Body) super.clone();
			newBody.head = (Head) head.clone();
			return newBody;
		}

	}

	static class Head implements Cloneable
	{
		public Face face;

		public Head()
		{
		}

		public Head(Face face)
		{
			this.face = face;
		}

		@Override
		protected Object clone() throws CloneNotSupportedException
		{
			//return super.clone();
			
			// 实现完全的深拷贝
			Head newHead = (Head) super.clone();  
	        newHead.face = (Face) this.face.clone();  
	        return newHead;
		}
	}

	// static class Face{}

	// 为了实现完全的深拷贝  所以Face类也需要实现cloneable接口
	static class Face implements Cloneable
	{
		@Override
		protected Object clone() throws CloneNotSupportedException
		{
			return super.clone();
		}
	}

	public static void main(String[] args) throws CloneNotSupportedException
	{
		Body body = new Body(new Head(new Face()));
		Body body1 = (Body) body.clone();

		System.out.println("body == body1 : " + (body == body1));
		System.out.println("body.head == body1.head : " + (body.head == body1.head));
		System.out.println("body.head.face == body1.head.face : " + (body.head.face == body1.head.face));
	}
}

```

string类的源码分析，为什么是不可变的
---------------------
string类之所以是immutable的，是因为value字符数组它一旦初始化，就无法更改，且没有提供相应的set方法来修改。
```java
/** The value is used for character storage. */
    private final char value[];
```
那么string类的replace方法是如何进行替换的呢？源代码中很明显的指出：如果存在可以替换的字符，那么就替换字符，并且使用string的构造函数新建一个对象进行返回；
如果不存在替换的字符，那么直接返回原字符串对象。
上源码：
```java
/**
     * Returns a string resulting from replacing all occurrences of
     * {@code oldChar} in this string with {@code newChar}.
     * <p>
     * If the character {@code oldChar} does not occur in the
     * character sequence represented by this {@code String} object,
     * then a reference to this {@code String} object is returned.
     * Otherwise, a {@code String} object is returned that
     * represents a character sequence identical to the character sequence
     * represented by this {@code String} object, except that every
     * occurrence of {@code oldChar} is replaced by an occurrence
     * of {@code newChar}.
     * <p>
     * Examples:
     * <blockquote><pre>
     * "mesquite in your cellar".replace('e', 'o')
     *         returns "mosquito in your collar"
     * "the war of baronets".replace('r', 'y')
     *         returns "the way of bayonets"
     * "sparring with a purple porpoise".replace('p', 't')
     *         returns "starring with a turtle tortoise"
     * "JonL".replace('q', 'x') returns "JonL" (no change)
     * </pre></blockquote>
     *
     * @param   oldChar   the old character.
     * @param   newChar   the new character.
     * @return  a string derived from this string by replacing every
     *          occurrence of {@code oldChar} with {@code newChar}.
     */
    public String replace(char oldChar, char newChar) {
        if (oldChar != newChar) {
            int len = value.length;
            int i = -1;
            char[] val = value; /* avoid getfield opcode */

            while (++i < len) {
                if (val[i] == oldChar) {
                    break;
                }
            }
            if (i < len) {
                char buf[] = new char[len];
                for (int j = 0; j < i; j++) {
                    buf[j] = val[j];
                }
                while (i < len) {
                    char c = val[i];
                    buf[i] = (c == oldChar) ? newChar : c;
                    i++;
                }
                return new String(buf, true);
            }
        }
        return this;
    }
```
对于不可变对象，如果使用反射的方法，是可以改变某些值的。

quartz的任务调度怎么进行的以及如何选择节点来执行定时任务
-------------------------------
下面是spring中一种典型的quartz集成配置方式
```xml
<!------------ 配置调度程序quartz ，其中配置JobDetail有两种方式-------------->    
    <!--方式一：使用JobDetailBean，任务类必须实现Job接口 -->     
    <bean id="myjob" class="org.springframework.scheduling.quartz.JobDetailBean">    
     <property name="name" value="exampleJob"></property>    
     <property name="jobClass" value="com.ncs.hj.SpringQtz"></property>   
     <property name="jobDataAsMap">  
<map>  
    <entry key="service"><value>simple is the beat</value></entry>  
</map>  
;/property>  
    </bean>   
    <!--运行时请将方式一注释掉！ -->    
    <!-- 方式二：使用MethodInvokingJobDetailFactoryBean，任务类可以不实现Job接口，通过targetMethod指定调用方法-->    
    <!-- 定义目标bean和bean中的方法 -->  
    <!-- SpringQtzJob就是定时任务到的时候需要调用知道的bean，execute就是这个bean内部的一个方法 -->  
    <bean id="SpringQtzJob" class="com.ncs.hj.SpringQtz"/>  
    <bean id="SpringQtzJobMethod" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean">  
    <property name="targetObject">  
        <ref bean="SpringQtzJob"/>  
    </property>  
    <property name="targetMethod">  <!-- 要执行的方法名称 -->  
        <value>execute</value>  
    </property>  
</bean>  
  
<!-- ======================== 调度触发器 ======================== -->  
<bean id="CronTriggerBean" class="org.springframework.scheduling.quartz.CronTriggerBean">  
    <property name="jobDetail" ref="SpringQtzJobMethod"></property>  
    <property name="cronExpression" value="0/5 * * * * ?"></property>  
</bean>  
  
<!-- ======================== 调度工厂 ======================== -->  
<bean id="SpringJobSchedulerFactoryBean" class="org.springframework.scheduling.quartz.SchedulerFactoryBean">  
    <property name="triggers">  
        <list>  
            <ref bean="CronTriggerBean"/>  
        </list>  
    </property>  
</bean>    
```

任务调度的机制
-------
参考链接：https://www.cnblogs.com/zhangchengzhangtuo/p/5705672.html
在这里将几个重要的类调用的过程以序列图的形式展现出来，上半部分展现的是启动过程，下半部分展现的是任务调度的过程。

步骤1.用户首先需要生成一个调度器工厂SchedulerFactory，可以用下面的方式实现自己的定制化：
步骤2.然后通过getScheduler()方法从调度器工厂里得到调度器实例，首先查找有没有这样的调度器，没有的话，就生成一个，有的话直接返回。所以得到的一般是单例，即默认的调度器。
步骤3.Scheduler有一个QuartzSchedulerThread（Thread的子类）属性，在scheduler实例化的时候，实例化了一个对象，并用ThreadExecutor启动该线程对象。该线程就是调度线程，主要任务就是不停的从JobStore中获取即将被触发的触发器（默认30s调度一次）。在这个时候调度线程虽然启动，但是处于pause状态。
步骤4.接下来是任务调度的部分：
client通过scheduleJob()方法将任务和触发器存储在JobStore中，通过start()方法将QuartzSchedulerThread的pause状态设为false，通知调度线程执行任务，此后调度线程不停的从JobStore中去取即将触发的任务。  
上半部分展现的是任务执行之前准备工作的时序，下半部分展现的是任务执行的时序。

步骤1.调度线程首先去线程池中获取可用的线程，如果没有的话，就阻塞。
步骤2.从JobStore(从存储介质中获取触发器，存储介质可以是内存也可以是数据库)获取（接下来30s内的）触发器，然后等待该触发器触发。
步骤3.调度线程创建一个JobRunShell(就是一个Runnable)，然后从线程池中调用线程执行该任务。
接下来就是任务执行的时序：
步骤4.获取trigger、JobDetail以及生成Job实例，然后执行job的execute接口函数。

sql优化、什么情况导致不走索引
----------------
主要原则就是应尽量避免全表扫描，应该考虑在where及order by 涉及的列上建立索引。

 - 一个表的索引不是越多越好，也没有一个具体的数字，根据以往的经验，一个表的索引最多不能超过**6个**，因为索引越多，对update和insert操作也会有性能的影响，涉及到索引的新建和重建操作。
 - 建立索引的方法论为：
1、多数查询经常使用的列；
2、很少进行修改操作的列；
3、索引需要建立在数据差异化大的列上
 - 对应的sql优化的方法有
1、建立复合索引；
2、like语句优化，比如下面的语句，前后都加了%，该查询必然会走全表扫描
```sql
SELECT id FROM A WHERE name like '%abc%'
```
3、在where子句中使用 ！= 或 <>操作符，索引将被放弃使用，会进行全表查询。
4、in和not in 也要慎用，否则也会导致全表扫描。

　　 方案一：between替换in

　　 如SQL:SELECT id FROM A WHERE num in(1,2,3) 优化成：SELECT id FROM A WHERE num between 1 and 3

　　 方案二：exist替换in

　　 如SQL:SELECT id FROM A WHERE num in(select num from b ) 优化成：SELECT num FROM A WHERE num exists(select 1 from B where B.num = A.num)

　　 方案三：left join替换in

　　 如SQL:SELECT id FROM A WHERE num in(select num from B) 优化成：SELECT id FROM A LEFT JOIN B ON A.num = B.num
5、不要在where子句中的“=”**左边**进行函数、算数运算或其他表达式运算，否则系统将可能无法正确使用索引。

　　 如SQL:SELECT id FROM A WHERE num/2 = 100 优化成：SELECT id FROM A WHERE num = 100*2

　　 如SQL:SELECT id FROM A WHERE substring(name,1,3) = 'abc' 优化成：SELECT id FROM A WHERE LIKE 'abc%'

　　 如SQL:SELECT id FROM A WHERE datediff(day,createdate,'2016-11-30')=0 优化成：SELECT id FROM A WHERE createdate>='2016-11-30' and createdate<'2016-12-1'

　　 如SQL:SELECT id FROM A WHERE year(addate) <2016 优化成：SELECT id FROM A where addate<'2016-01-01'
6、任何地方都不要用 select * from table ，用具体的字段列表替换"*"，不要返回用不到的字段　
7、使用“临时表”暂存中间结果
8、limit分页优化
mysql的limit函数语法：
```sql
SELECT * FROM table  LIMIT [offset,] rows | rows OFFSET offset
```
当偏移量特别时，limit效率会非常低

　　　　SELECT id FROM A LIMIT 1000,10   很快

　　　　SELECT id FROM A LIMIT 90000,10 很慢

　　　　优化方法：

　　　　方法一：select id from A **order by** id limit 90000,10; 很快，0.04秒就OK。 因为用了id主键做索引当然快

　　　　方法二：select id,title from A where id>=(select id from collect order by id limit 90000,1) limit 10;

     　　 方法三：select id from A order by id  between 10000000 and 10000010;
9、尽量不要使用 BY RAND()命令
BY RAND()是随机显示结果，这个函数可能会为表中每一个独立的行执行BY RAND()命令，这个会消耗处理器的处理能力。
10、排序的索引问题　
Mysql查询**只是用一个索引**，因此如果where子句中**已经**使用了索引的话，那么order by中的列是**不会使用索引**的。因此数据库默认排序可以符合要求情况下不要使用排序操作；
尽量不要包含多个列的排序，如果需要最好给这些列**创建复合索引**。
11、尽量用 union add 替换 union
union和union all的差异主要是前者需要将两个（或者多个）结果集合并后再进行唯一性过滤操作，这就会涉及到排序，增加大量的cpu运算，加大资源消耗及延迟。所以当我们可以确认不可能出现重复结果集或者不在乎重复结果集的时候，尽量使用union all而不是union
12、Inner join 和 left join、right join、子查询
第一：inner join内连接也叫等值连接是，left/rightjoin是外连接。

　　　　 SELECT A.id,A.name,B.id,B.name FROM A LEFT JOIN B ON A.id =B.id;

　　　　 SELECT A.id,A.name,B.id,B.name FROM A RIGHT JOIN ON B A.id= B.id;

　　　　 SELECT A.id,A.name,B.id,B.name FROM A INNER JOIN ON A.id =B.id;

　　 　　经过来自多方面的证实inner join性能比较快，因为inner join是等值连接，或许返回的行数比较少。但是我们要记得有些语句隐形的用到了等值连接，如：

 　　　　SELECT A.id,A.name,B.id,B.name FROM A,B WHERE A.id = B.id;

 　　　　推荐：能用inner join连接尽量使用inner join连接

　　 第二：**子查询的性能又比外连接性能慢**，尽量用外连接来替换子查询。

　　　　Select* from A where exists (select * from B where id>=3000 and A.uuid=B.uuid);

　　　　A表的数据为十万级表，B表为百万级表，在本机执行差不多用2秒左右，我们可以通过explain可以查看到子查询是一个相关子查询(DEPENDENCE SUBQUERY);Mysql是先对外表A执行全表查询，然后根据uuid逐次执行子查询，如果外层表是一个很大的表，我们可以想象查询性能会表现比这个更加糟糕。

      　　一种简单的优化就是用innerjoin的方法来代替子查询，查询语句改为：

  　　　Select* from A inner join B ON A.uuid=B.uuid using(uuid) where b.uuid>=3000;  这个语句执行测试不到一秒；

　　第三：使用JOIN时候，应该用小的结果驱动大的结果（left join 左边表结果尽量小，如果有条件应该放到左边先处理，right join同理反向），同时尽量把牵涉到多表联合的查询拆分多个query	(多个表查询效率低，容易锁表和阻塞)。如：

	　　Select * from A left join B A.id=B.ref_id where  A.id>10;可以以优化为：
	　　select * from (select * from A wehre id >10) T1 left join B on T1.id=B.ref_id;
13、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描
select id from t where num is null 
可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： 
select id from t where num=0 
14、尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。
15、尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 
16、尽量避免大事务操作，提高系统并发能力。



**所以，sql查询不走索引大致可以分为如下几个原因：**

 - 使用了不等号<>作为查询条件
 - 使用了null作为查询的判断条件，形如is null
 - 使用了in作为查询条件（不是绝对的，部分in条件是可以走索引的）
 - 查询条件中的“=”左边进行了运算
 - like查询前都使用了%，形如like '%abc'，但是like 'abc%'还是可以走索引的
 - 使用复合索引时，索引的第一个字段没有作为查询条件
 

navicat查看mysql执行计划各参数意义
-----------------------

 1. ID：Query Optimizer 所选定的执行计划中查询的序列号；
 2. Select_type：所使用的查询类型，主要有以下这几种查询类型
 
 ◇ DEPENDENT SUBQUERY：子查询中内层的第一个SELECT，依赖于外部查询的结果集；

◇ DEPENDENT UNION：子查询中的UNION，且为UNION 中从第二个SELECT
开始的后面所有SELECT，同样依赖于外部查询的结果集；
◇ PRIMARY：子查询中的最外层查询，注意并不是主键查询；

◇ SIMPLE：除子查询或者UNION 之外的其他查询；

◇ SUBQUERY：子查询内层查询的第一个SELECT，结果不依赖于外部查询结果集；

◇ UNCACHEABLE SUBQUERY：结果集无法缓存的子查询；

◇ UNION：UNION 语句中第二个SELECT 开始的后面所有SELECT，第一个SELECT 为PRIMARY

◇ UNION RESULT：UNION 中的合并结果；

 3. Table：显示这一步所访问的数据库中的表的名称；
 4. Type：告诉我们对表所使用的访问方式，主要包含如下集中类型：
 
 ◇ all：全表扫描

◇ const：读常量，且最多只会有一条记录匹配，由于是常量，所以实际上只需要读一次；

◇ eq_ref：最多只会有一条匹配结果，一般是通过主键或者唯一键索引来访问；

◇ fulltext：

◇ index：全索引扫描；

◇ index_merge：查询中同时使用两个（或更多）索引，然后对索引结果进行merge 之后再读
取表数据；

◇ index_subquery：子查询中的返回结果字段组合是一个索引（或索引组合），但不是一个
主键或者唯一索引；

◇ rang：索引范围扫描；

◇ ref：Join 语句中被驱动表索引引用查询；

◇ ref_or_null：与ref 的唯一区别就是在使用索引引用查询之外再增加一个空值的查询；

◇ system：系统表，表中只有一行数据；

◇ unique_subquery：子查询中的返回结果字段组合是主键或者唯一约束；

 5. Possible_keys：该查询可以利用的索引. 如果没有任何索引可以使用，就会显示成null，这一项内容对于优化时候索引的调整非常重要；
 6. Key：MySQL Query Optimizer 从possible_keys 中所选择使用的索引（**注意**，mysql默认一次查询只会走一个索引）；
 7. Key_len：被选中使用索引的索引键长度；
 8. Ref：列出是通过常量（const），还是某个表的某个字段（如果是join）来过滤（通过key）的；
 9. Rows：MySQL Query Optimizer 通过系统收集到的统计信息估算出来的结果集记录条数；
 10. Extra：查询中每一步实现的额外细节信息，主要可能会是以下内容：
 
 ◇ Distinct：查找distinct 值，所以当mysql 找到了第一条匹配的结果后，将停止该值的查询而转为后面其他值的查询；
◇ Full scan on NULL key：子查询中的一种优化方式，主要在遇到无法通过索引访问null
值的使用使用；

◇ Impossible WHERE noticed after reading const tables：MySQL Query Optimizer 通过收集到的统计信息判断出不可能存在结果；

◇ No tables：Query 语句中使用FROM DUAL 或者不包含任何FROM 子句；

◇ Not exists：在某些左连接中MySQL Query Optimizer 所通过改变原有Query 的组成而使用的优化方法，可以部分减少数据访问次数；

◇ Range checked for each record (index map: N)：通过MySQL 官方手册的描述，当MySQL Query Optimizer没有发现好的可以使用的索引的时候，如果发现如果来自前面的表的列值已知，可能部分索引可以使用。对前面的表的每个行组合，MySQL 检查是否可以使用range 或index_merge 访问方法来索取行。

◇ Select tables optimized away：当我们使用某些聚合函数来访问存在索引的某个字段的
时候，MySQL Query Optimizer会通过索引而直接一次定位到所需的数据行完成整个查询。当然，前提是在Query 中不能有GROUP BY 操作。如使用MIN()或者MAX（）的时候；

◇ Using filesort：当我们的Query 中包含ORDER BY操作，而且无法利用索引完成排序操作的时候，MySQL Query Optimizer 不得不选择相应的排序算法来实现。

◇ Using index：所需要的数据只需要在Index即可全部获得而不需要再到表中取数据；

◇ Using index for group-by：数据访问和Using index一样，所需数据只需要读取索引即可，而当Query 中使用了GROUP BY 或者DISTINCT 子句的时候，如果分组字段也在索引中，Extra 中的信息就会是Using index for group-by；

◇ Using temporary：当MySQL在某些操作中必须使用临时表的时候，在Extra 信息中就会出现Using temporary 。主要常见于GROUP BY 和ORDER BY等操作中。

◇ Using where：如果我们不是读取表的所有数据，或者不是仅仅通过索引就可以获取所有需要的数据，则会出现Using where 信息；

◇ Using where with pushed condition：这是一个仅仅在NDBCluster 存储引擎中才会出现的信息，而且还需要通过打开Condition Pushdown 优化功能才可能会被使用。控制参数为engine_condition_pushdown 。
 

数据库事务正确执行的四个基本要素
----------------
包含：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）

 - 原子性

    整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。

 - 一致性
以转账案例为例，假设有五个账户，每个账户余额是100元，那么五个账户总额是500元，如果在这个5个账户之间同时发生多个转账，无论并发多少个，比如在A与B账户之间转账5元，在C与D账户之间转账10元，在B与E之间转账15元，五个账户总额也应该还是500元，这就是保护性和不变性

 - 隔离性
事物相互之间不产生影响。

 - 持久性
在事务完成commit以后，该事务对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。

数据库的隔离性的4个等级
------------
参考链接http://blog.csdn.net/a1324204785/article/details/53156414
数据库事物有4个等级，由低到高，分别是：
Read uncommitted、Read committed、Repeatable read、Serializable、MVCC（这个好像不常用），这五个级别可以逐个解决脏读、不可重复读、幻读这几类问题。

 1. Read uncommitted（读未提交，存在脏读）
 例子：错发工资，空欢喜
         小明 辛苦搬砖一个月，领导给他账户存4000，此时领导发工资事物未提交，但小明查看工资发现账户多了4000，满心欢喜，不幸的是领导发现工资算错了，回滚事务，修改金额2000，提交事务。最终小明工资只有2000，空欢喜一场。
**总结**：此例子中，发生了**脏读**。即A事物修改，未提交，B事物先读到，A事物在修改提交。
 2. Read committed（读已提交，存在不可重复读）
 例子：例子:还是小明小华销售员，余票3张，A来小华那里请求3张订票单，小华接受订单，要卖出3张票，上面的销售步骤执行中的时候，B也来小明那里买票，由于小华的销售事务执行到一半，小明事务没有看到小华的事务执行，读到的票数是3，准备接受订单的时候，小华的销售事务完成了，此时小明的系统变成显示0张票，小明刚想按下鼠标点击接受订单的手又连忙缩了回去。
**总结**：此例子中，发生了**不可重复读**。即事物A查询，事物B修改并提交，事物A读发现修改了。
**oracle默认**系统事务隔离级别是READ COMMITTED,也就是读已提交。
 3. Repeatable read （可重复读，存在幻读）
 例子：老婆查账，打印完账单发现变化
        小明老婆银行工作，经常查看小明消费记录，有一次查看当月消费记录为80，此时小明在外胡吃海喝消费1000，并提交事物，随后老婆打印账单发现消费变成1080,，非常诧异以为出现幻觉。
总结：事物A查询，事物B修改，事物A继续查询，结果变了。
**mysql默认**使用的就是就是这个隔离级别，使用select @@tx_isolation;命令可以进行查询
 4. Serializable（序列化，性能低）
 串行化事物，避免脏读、不可重复读、幻读，但是性能花费太高。
 5. MVCC（Multi-Version Concurrency Control 多版本并发控制 ）
在Java concurrent包中，有copyonwrite系列的类，专门用于优化读远大于写的情况。而其优化的手段就是，在进行写操作时，将数据copy一份，不会影响原有数据，然后进行修改，修改完成后原子替换掉旧的数据，而读操作只会读取原有数据。通过这种方式实现写操作不会阻塞读操作，从而优化读效率。而写操作之间是要互斥的，并且每次写操作都会有一次copy，所以只适合读大于写的情况。
MVCC的原理与copyonwrite类似，全称是Multi-Version Concurrent Control，即多版本并发控制。在MVCC协议下，每个读操作会看到一个一致性的snapshot，并且可以实现非阻塞的读。MVCC允许数据具有多个版本，这个版本可以是时间戳或者是全局递增的事务ID，在同一个时间点，不同的事务看到的数据是不同的。
现在来看看MySQL数据库为我们提供的四种隔离级别：

　　① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。

　　② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。

　　③ Read committed (读已提交)：可避免脏读的发生。

　　④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。
整理成表格的形式就是：
```java
		/*
		 * ----------
           				脏读 	不可重复读 	幻读
		----------
		Read uncommitted √       √           √           
		----------	
		Read committed     		 √		 	 √			
		----------
		Repeatable read 					 √			
		----------
		Serializable 						
		----------
		 */
```

幻读和不可重复读的区别
-----------

 - 不可重复读
 强调在同一个事物中，两次读取同一数据，产生的结果不一致。可能的原因就是当前事物在执行的过程中，另外一个事物对当前事物读取的数据进行了修改。
 - 幻读
 同样的查询条件，不同的事物中分别先后执行，得到的结果不一致。

Spring事务的传播特性和隔离级别
------------------
**事务的几种传播特性**
1. PROPAGATION_REQUIRED: 如果存在一个事务，则支持当前事务。如果没有事务则开启
2. PROPAGATION_SUPPORTS: 如果存在一个事务，支持当前事务。如果没有事务，则非事务的执行
3. PROPAGATION_MANDATORY: 如果已经存在一个事务，支持当前事务。如果没有一个活动的事务，则抛出异常。
4. PROPAGATION_REQUIRES_NEW: 总是开启一个新的事务。如果一个事务已经存在，则将这个存在的事务挂起。
5. PROPAGATION_NOT_SUPPORTED: 总是非事务地执行，并挂起任何存在的事务。
6. PROPAGATION_NEVER: 总是非事务地执行，如果存在一个活动事务，则抛出异常
7. PROPAGATION_NESTED：如果一个活动的事务存在，则运行在一个嵌套的事务中. 如果没有活动事务,则按TransactionDefinition.PROPAGATION_REQUIRED 属性执行
**Spring事务的隔离级别**
1. ISOLATION_DEFAULT： 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别.
下面的2\3\4\5与JDBC的隔离级别相对应
2. ISOLATION_READ_UNCOMMITTED： 这是事务最低的隔离级别，它充许另外一个事务可以看到这个事务未提交的数据。
      这种隔离级别会产生脏读，不可重复读和幻像读。
3. ISOLATION_READ_COMMITTED： 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据
4. ISOLATION_REPEATABLE_READ： 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。
      它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了避免下面的情况产生(不可重复读)。
5. ISOLATION_SERIALIZABLE 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。
      除了防止脏读，不可重复读外，还避免了幻像读。 

Servlet的作用，生命周期，如何创建、配置Servlet
------------------------------
**servlet是什么**
servlet是一个基于java技术的WEB组件，运行在服务器端，我们利用 sevlet可以很轻松的扩展WEB服务器的功能，使它满足特定的应用需要。servlet由servlet容器管理，servlet容器也叫servlet引擎，是servlet的运行环境，给发送的请求和响应之上提供网络服务
**Servlet的生命周期**
servlet的生命周期是指：servlet由创建到销毁的过程。
生命周期涉及几个方法：构造器，init，service，destroy。servlet在请求时创建
构造器方法：只在第一次访问时调用一次，说明servlet是单例的。
init方法：只会在第一次访问servlet时调用一次，对servlet对象进行初始化。
service方法：每次访问时都调用一次，业务逻辑写在这个方法里。
destroy方法：在项目卸载的时候调用一次，即关闭服务器的时候调用一次。

常用设计模式
------
参考链接http://blog.csdn.net/xsl1990/article/details/16359289
创建型模式主要有简单工厂模式（并不是23种设计模式之一）、**工厂方法**、抽象工厂模式、**单例模式**、生成器模式和原型模式。
结构型模式主要有**适配器模式adapter**、**桥接模式bridge**、组合器模式component、装饰器模式decorator、门面模式、亨元模式flyweight和**代理模式proxy**。
行为型模式主要有**命令模式command**、解释器模式、**迭代器模式**、中介者模式、备忘录模式、**观察者模式**、状态模式state、策略模式、模板模式和**访问者模式**。

 - 单例模式
 单例模式。构造函数是私有的，通过一个共有的成员函数还调用这个构造函数，在多线程环境下，还需要对这个成员函数进行加锁。
下面是4种单例的创建方式，最安全也最好的是第4种，使用内部类的方式
```java
// 1、懒汉式单例，线程不安全的
public class Singleton {  
    private static Singleton instance;  
    private Singleton (){}  
  
    public static Singleton getInstance() {  
    if (instance == null) {  
        instance = new Singleton();  
    }  
    return instance;  
    }  
}  

// 2、懒汉式单例，线程安全的，这种写法能够在多线程中很好的工作，而且看起来它也具备很好的lazy loading，但是，遗憾的是，效率很低，99%情况下不需要同步。
public class Singleton {  
    private static Singleton instance;  
    private Singleton (){}  
    public static synchronized Singleton getInstance() {  
    if (instance == null) {  
        instance = new Singleton();  
    }  
    return instance;  
    }  
}  

// 3、饿汉式，线程安全，但不能保证是懒加载的模式
// 这种方式基于classloder机制避免了多线程的同步问题，不过，instance在类装载时就实例化，虽然导致类装载的原因有很多种，在单例模式中大多数都是调用getInstance方法会导致类加载，此时就是lazy loading。 
//但是也不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化instance显然没有达到lazy loading的效果。
public class Singleton {  
    private static Singleton instance = new Singleton();  
    private Singleton (){}  
    public static Singleton getInstance() {  
    return instance;  
    }  
} 

//4、静态内部类方式
//这种方式同样利用了classloder的机制来保证初始化instance时只有一个线程，它跟第三种方式不同的是（很细微的差别）：第三种方式是只要Singleton类被装载了，那么instance就会被实例化（没有达到lazy loading效果）。
//而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显式装载SingletonHolder类，从而实例化instance。
//想象一下，如果实例化instance很消耗资源，我想让他延迟加载，另外一方面，我不希望在Singleton类加载时就实例化，因为我不能确保Singleton类还可能在其他的地方被主动使用从而被加载，那么这个时候实例化instance显然是不合适的。这个时候，这种方式相比第三种方式就显得很合理。
public class Singleton
{
	// 私有的  静态的 
	private static class SingletonHolder
	{
		// 私有的 静态的  final类型的
		private static final Singleton INSTANCE = new Singleton();
	}

	private Singleton()
	{
	}

	public static final Singleton getInstance()
	{
		// 返回内部类的静态属性
		return SingletonHolder.INSTANCE;
	}
}
```
**工厂方法**
调用和被调用者之间不产生直接的以来关系，而是由工厂来负责对象的创建，一般是在工厂的方法中调用类的构造函数，单个或者批量的创建对象。
**代理模式(Proxy)**
代理模式是一种应用非常广泛的设计模式，当客户端代码需要调用某个对象时，客户端实际上不关心是否准确得到该对象，它只要一个能提供该功能的对象即可，此时我们就可返回该对象的代理（Proxy）。
代理就是一个Java对象代表另一个Java对象来采取行动。示例代码如下：
```java
public class ImageProxy implements Image
{
    //组合一个image实例，作为被代理的对象
    private Image image;
    //使用抽象实体来初始化代理对象
    public ImageProxy(Image image)
    {
       this.image = image;
    }
    /**
     * 重写Image接口的show()方法
     * 该方法用于控制对被代理对象的访问，
     * 并根据需要负责创建和删除被代理对象
     */
    public void show()
    {
       //只有当真正需要调用image的show方法时才创建被代理对象
       if (image == null)
       {
           image = new BigImage();
       }
       image.show();
    }
}

// 调用的时候，这么调用
Image image = new ImageProxy(null);
```

**命令模式(Command)**
某个方法需要完成某一个功能，完成这个功能的大部分步骤已经确定了，但可能有少量具体步骤无法确定，必须等到执行该方法时才可以确定。（在某些编程语言如Ruby、Perl里，允许传入一个代码块作为参数。但Java暂时还不支持代码块作为参数）。在Java中，传入该方法的是一个对象，该对象通常是某个接口的匿名实现类的实例，该接口通常被称为命令接口，这种设计方式也被称为命令模式。
下面是源代码：
```java
// Command接口
public interface Command
{
    //接口里定义的process方法用于封装“处理行为”
    void process(int[] target);
}

// ProcessArray

public class ProcessArray
{
    //定义一个each()方法，用于处理数组，
    public void each(int[] target , Command cmd)
    {
       cmd.process(target);
    }
}

// TestCommand 测试
public class TestCommand
{
    public static void main(String[] args)
    {
       ProcessArray pa = new ProcessArray();
       int[] target = {3, -4, 6, 4};
       //第一次处理数组，具体处理行为取决于Command对象
       pa.each(target , new Command()
       {
           //重写process()方法，决定具体的处理行为
           public void process(int[] target)
           {
              for (int tmp : target )
              {
                  System.out.println("迭代输出目标数组的元素:" + tmp);
              }
           }
       });
       System.out.println("------------------");
       //第二次处理数组，具体处理行为取决于Command对象
       pa.each(target , new Command()
       {
           //重写process方法，决定具体的处理行为
           public void process(int[] target)
           {
              int sum = 0;
              for (int tmp : target )
              {
                  sum += tmp;         
              }
              System.out.println("数组元素的总和是:" + sum);
           }
       });
    }
}
```
**策略模式(Strategy)**
策略模式用于封装系列的算法，这些算法通常被封装在一个被称为Context的类中，客户端程序可以自由选择其中一种算法，或让Context为客户端选择一种最佳算法——使用策略模式的优势是为了支持算法的自由切换。
 **门面模式(Facade)**
 将多个类的方法进行封装，对于一些固定的操作，封装成一个方法来执行。
 **桥接模式(Bridge)**
由于实际的需要，某个类具有两个以上的维度变化，如果只是使用继承将无法实现这种需要，或者使得设计变得相当臃肿。而桥接模式的做法是把变化部分抽象出来，使变化部分与主类分离开来，从而将多个的变化彻底分离。最后提供一个管理类来组合不同维度上的变化，通过这种组合来满足业务的需要。
**观察者模式(Observer)**
观察者模式定义了对象间的一对多依赖关系，让一个或多个观察者对象观察一个主题对象。当主题对象的状态发生变化时，系统能通知所有的依赖于此对象的观察者对象，从而使得观察者对象能够自动更新。

I/O中涉及到的设计模式
------------
 - Java的I/O库总体设计是符合**装饰者模式**（Decorator）跟**适配器模式**（Adapter）的。
　　１　装饰者模式：在由 InputStream，OutputStream，Reader和Writer代表的等级结构内部，有一些流处理器可以对另一些流处理器起到装饰作用，形成新的，具有改善了的功能的流处理器，比如bufferedxxxStream。装饰者模式是Java I/O库的整体设计模式。这样的一个原则是符合装饰者模式的。 

　　２　适配器模式：Reader和Writer代表的等级结构内部，实现了**字节流到字符流**的转换，比如InputStreamReader，实现了字节流到字符流的转换。这就是适配器模式的应用。 （需要注意的是，**不能**将字符流转换为字节流）

类的初始化过程
-------
参考链接http://blog.csdn.net/zjl477595675/article/details/48101611
类的初始化和对象初始化是两个不同的概念。
**类的初始化**是发生在类加载过程，是类加载过程的一个阶段，该阶段并不调用类的构造器。
**对象的初始化**是在类加载完成后为对象分配内存，实例变量的初始化，实例变量的赋值及调用类构造器完成对象的初始化过程。

 1. 类的初始化
 分为类的加载——>链接——>**对象初始化**
类的加载
类加载发生在以下几种情况： 
1）new生成新的对象实例。 
2）使用java.lang.reflect包的方法对类进行发射调用时。 
3）当子类进行加载或初始化时。当加载一个类时，如果发现其存在父类并且未被加载则会继续加载父类。（**注意**：接口的加载过程与类的加载过程稍有不同。接口中不能使用static{}块。当一个接口在初始化时，并不要求其父接口全部都完成了初始化，只有真正在使用到父接口时（例如引用接口中定义的常量）才会初始化。） 
4）虚拟机启动时，用户指定的执行主类（包含main()的执行入口类），虚拟机会加载加载该类。 
5）调用类变量（静态字段但非静态常量），类方法
注意： 
1. 调用类的静态字段，只有直接定义这个字段的类才会被加载和初始化。通过其子类来引用父类中定义的字段，只会触发父类的初始化而不会触发子类的初始化。 
2. 调用类的静态常量是不触发类的加载过程。如果在A类中调用B类的静态常量，那么在编译阶段会将该静态常量放到A的Class文件的静态常量池中，所以对该常量的调用不涉及B的加载。

```java
 class SuperClass{
    static{
        System.out.println("SuperClass 类初始化");
    }
    static int a=3;
    static final int b=4;
}
class SubClass extends SuperClass{
    static{
        System.out.println("SubClass 类初始化");
    }
}
public class Test {
    public static void main(String[] args) {
        System.out.println(SubClass.a);
    }
}
```
结果是：
```java
SuperClass 类初始化
3
```
如果Test这么写：
```java
public class Test
{
	public static void main(String[] args)
	{
//		System.out.println(SubClass.a);
		System.out.println(SubClass.b);
	}
}
```
那么输出结如下，两个类都没有进行初始化，因为此时b是类常量。
```java
4
```
类加载主要完成如下三件事情：
加载阶段虚拟机主要完成以下三件事： 
1）根据类的路径，定位并获取类的class文件 
2）通过加载器加载class文件，并将class文件里所代表的静态存储结构转化为方法区的运行数据结构 
3）在java堆中生成一个代表这个类的java.lang.Class对象，作为方法区这些数据的访问入口。
2、类的链接
链接又可以细分为验证——>准备——>解析

 - 验证阶段：

确保Class文件的字节流包含的信息符合当前虚拟机的要求，并不会危害虚拟机自身安全。该阶段包括四个部分： 
1.文件格式验证：验证字节流是否符合Class文件格式的规范 
2.元数据验证：对字节码描述的信息进行语义分析，以确保其描述的信息符合java语言规范要求 
3.字节码验证：进行数据流和控制流分析，保证校验类的方法在运行时不会做出危害虚拟机安全的行为。 
4.符号引用验证：在虚拟机中将符号引用转换为直接引用

 - 准备阶段
正式为**类变量**分配内存并设置类变量的默认值，这些内存在**方法区**中进行分配。内存分配仅针对类变量（static变量），不包括实例变量，实例变量是在对象实例化时和对象一起在堆中分配内存。 
类变量的默认值的设置和为了保证变量使用的安全性在对象实例化过程中虚拟机自动地对实例变量进行设置默认值是一样的。默认值的设置如下：
```java
/*数据类型		默认值
	int			0
	long		0L
	char		‘\u0000’
	byte		(byte)0
	boolean		false
	float		0.0f
	double		0.0d
	reference	null*/
```
 - 解析阶段
 将常量池内的符号引用替换为直接应用的过程
 - 对象初始化
 初始化是执行类构造器() (在准备阶段提供的代码，即执行所有类变量的赋值动作及静态代码块内容）。在执行类构造器()时保证父类的构造器()已经执行完毕。即一般我们会看到先执行父类的static方法块内容和static变量的赋值，然后再执行子类的static方法块内容和static变量赋值。

main函数执行所发生的一系列动作
-----------------
1、JVM启动
2、对main函数中需要使用的类，执行加载、链接、初始化
3、启动main线程
4、执行main函数
5、JVM退出

GC回收算法
------
JVM将堆分成了 二个大区  Young 和 Old 。

 - Young区

Young 区又分为 Eden、Survivor1、Survivor2, 两个Survivor 区相对地作为为From 和 To 逻辑区域, 当Servivor1作为 From 时 ， Servivor2 就作为 To,反之亦然。这是因为新生代区域使用的是**复制清理**的GC算法。
关于为什么要这样区分Young（将Young区分为Eden、Servivor1、Servivor2以及相对的From和To ），这要牵涉到JVM的垃圾回收算法的讨论。
1）因为引用计数法无法解决循环引用问题，JVM并没有采用这种算法来判断对象是否存活。
2）JVM一般采用GCRoots的方法，只要从任何一个GCRoots的对象可达，就是不被回收的对象
3）判断了对象生死，怎么进行内存的清理呢？
4）标记-清除算法，先标记那些要被回收的对象，然后进行清理，简单可行，但是**①**标记清除效率低，因为要一个一个标记和清除**②**造成大量不连续的内存碎片，空间碎片太多可能会导致当程序在以后的运行过程中需要分配较大对象的时候无法找到足够的连续内存而不得不触发另一次垃圾收集动作。
5）采用复制-清除算法：将可用内存按照容量分为大小相等的两块，每次只是使用其中的一块。当这一块的内存用完了，就将可用内存中存活的对象依次复制到空闲的内存区域，然后对之前的区域进行清除操作。

 - 那些对象可以作为GC Roots？

    虚拟机栈（栈帧中的本地变量表）中的引用的对象
    方法区中的类静态属性引用的对象
    方法区中的常量引用的对象
    本地方法栈中JNI（Native方法）的引用对象
    
新生代做的是复制-清理、from survivor、to survivor是干啥用的、老年代做的是标记-清理
**标记-清理的优缺点**
优点：好像没什么优点，或许优点是简单、理论上可用空间大等。
缺点：1、效率问题，标记和清理两个两个过程效率都不高。2、空间问题，标记清理完成以后会产生大量的不连续内存区域。可能导致大对象无法获得空间分配，从而触发不必要的GC操作。
**复制-清理的优缺点**
优点：不会产生碎片化问题，实现简单
缺点：会浪费掉一个部分内存区域

concurrentHashMap的底层实现
----------------------
concurrentHashMap在JDK1.8的实现和JDK1.7是不一样的，JDK8默认将table数组的每一项作为一个segment，而JDK7将table数组的几项作为一个segment。
JDK8直接采用transient volatile HashEntry<K,V>[] table保存数据，采用table数组元素作为锁，从而实现了对每一行数据进行加锁，进一步减少并发冲突的概率。

 - concurrentHashMap的get操作需要加锁吗？
没有加锁，但是put和remove操作有加锁。


分布式的基本知识
--------
CAP理论是由Eric Brewer提出的分布式系统中最为重要的理论之一：
Consistency：[强]一致性，事务保障，ACID模型。
Availiablity：[高]可用性，冗余以避免单点，至少做到柔性可用（服务降级）。
Partition tolerance：[高]可扩展性（分区容忍性）：一般要求系统能够自动按需扩展，比如HBase。

SpringMVC框架执行步骤（SpringMVC使用Servlet嵌入）：
--------------------------------------
SpringMVC的url请求的核心功能是使用dispatcherServlet来完成的。

1、客户端发出一个http请求给web服务器，web服务器对http请求进行解析，如果匹配DispatcherServlet的请求映射路径（在web.xml中指定），web容器将请求转交给DispatcherServlet.

2、DipatcherServlet接收到这个请求之后将根据请求的信息（包括URL、Http方法、请求报文头和请求参数Cookie等）以及HandlerMapping的配置找到处理请求的处理器（Handler）（也就是controller中对应的url处理类）。

3-4、DispatcherServlet根据HandlerMapping找到对应的Handler,将处理权交给Handler（Handler将具体的处理进行封装），再由具体的HandlerAdapter对Handler进行具体的调用（controller中具体的调用方法）。

5、Handler对数据处理完成以后将返回一个ModelAndView()对象给DispatcherServlet。

6、Handler返回的ModelAndView()只是一个逻辑视图并不是一个正式的视图，DispatcherSevlet通过ViewResolver将逻辑视图转化为真正的视图View。

7、Dispatcher通过model解析出ModelAndView()中的参数进行解析最终展现出完整的view并返回给客户端。

spring bean的生命周期
----------------
bean的生命周期大致会经历以下一些过程。

 1. bean实例化，寻找bean的定义信息并将其实例化。
 2. 设置bean的属性值。
 3. 调用BeanNameAware的setBeanName方法设置bean名称。
 4. 调用BeanFactoryAware的setBeanFactory方法。
 5. 调用BeanPostProcessor的**预初始化**方法。
 6. 调用InitializingBean的afterPropertiesSet方法。
 7. 调用bean的init-method方法
 8. 调用BeanPostProcessor的**后初始化**方法。
 9. bean初始化完成，可以在上下文环境中使用
 10. 容器关闭的时候，需要调用DisposableBean的destroy方法。
 11. 调用bean的destroy方法。
 
面试的时候，上面的步骤应该简化着来回答，一般可以概括为：实例化，初始init，接收请求service，销毁destroy；

1、**实例化**一个Bean－－也就是我们常说的**new**；

    2、按照Spring上下文对实例化的Bean进行配置－－也就是IOC注入；

    3、如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String)方法，此处传递的就是Spring配置文件中Bean的id值

    4、如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory(setBeanFactory(BeanFactory)传递的是Spring工厂自身（可以用这个方式来获取其它Bean，只需在Spring配置文件中配置一个普通的Bean就可以）；

    5、如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文（同样这个方式也可以实现步骤4的内容，但比4更好，因为ApplicationContext是BeanFactory的子接口，有更多的实现方法）；

    6、如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessBeforeInitialization(Object obj, String s)方法，BeanPostProcessor经常被用作是Bean内容的更改，并且由于这个是在Bean初始化结束时调用那个的方法，也可以被应用于内存或缓存技术；

    7、如果Bean在Spring配置文件中配置了init-method属性会自动调用其配置的初始化方法。

    8、如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法、；

    注：以上工作完成以后就可以应用这个Bean了，那这个Bean是一个Singleton的，所以一般情况下我们调用同一个id的Bean会是在内容地址相同的实例，当然在Spring配置文件中也可以配置非Singleton，这里我们不做赘述。

    9、当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用那个其实现的destroy()方法；

    10、最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。

一个文件中定义多个class
--------------
这个是可以的，只是**同一个文件只能有一个类定义成public**的
ArrayList的底层实现
------------
参考链接https://www.cnblogs.com/ITtangtang/p/3948555.html
 - ArrayList底层使用数组实现
 - 当不指定初始容量时，数组出事容量为10
 其中比较重要的函数，如下面源码所示：

根据下表index移除元素时，首先需要判断是否越界。然后才是删除元素，拷贝删除元素后面位置的数组，移除成功就返回移除的元素。
```java
// 移除此列表中指定位置上的元素。  
 public E remove(int index) {  
 // 如果index越界，抛出IndexOutOfBoundsException异常
    RangeCheck(index);  
  
    modCount++;  
    E oldValue = (E) elementData[index];  
  
    int numMoved = size - index - 1;  
    if (numMoved > 0)  
        System.arraycopy(elementData, index+1, elementData, index, numMoved);  
    elementData[--size] = null; // clear to let GC do its work
  
    return oldValue;  
 }
```

根据指定元素o，移除成功返回true，否则返回false
remove(Object o)中通过遍历element寻找是否存在传入对象，一旦找到就调用fastRemove移除对象。为什么找到了元素就知道了index，不通过remove(index)来移除元素呢？因为fastRemove跳过了判断边界的处理，因为找到元素就相当于确定了index不会超过边界，而且fastRemove并不返回被移除的元素。
```java
// 移除此列表中首次出现的指定元素（如果存在）。这是应为ArrayList中允许存放重复的元素。  
 public boolean remove(Object o) {  
    // 由于ArrayList中允许存放null，因此下面通过两种情况来分别处理。  
    if (o == null) {  
        for (int index = 0; index < size; index++)  
            if (elementData[index] == null) {  
                // 类似remove(int index)，移除列表中指定位置上的元素。  
                fastRemove(index);  
                return true;  
            }  
    } else {  
        for (int index = 0; index < size; index++)  
            if (o.equals(elementData[index])) {  
                fastRemove(index);  
                return true;  
            }  
        }  
        return false;  
    } 
}
```

```java
/**
     * Increases the capacity of this <tt>ArrayList</tt> instance, if
     * necessary, to ensure that it can hold at least the number of elements
     * specified by the minimum capacity argument.
     *
     * @param   minCapacity   the desired minimum capacity
     */
    public void ensureCapacity(int minCapacity) {
        int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)
            // any size if not default element table
            ? 0
            // larger than default for default empty table. It's already
            // supposed to be at default size.
            : DEFAULT_CAPACITY;

        if (minCapacity > minExpand) {
            ensureExplicitCapacity(minCapacity);
        }
    }
    
    // ensureExplicitCapacity方法
        private void ensureExplicitCapacity(int minCapacity) {
        modCount++;

        // overflow-conscious code
        if (minCapacity - elementData.length > 0)
            grow(minCapacity);
    }
    
    // grow方法进行实际数组的扩容，先按照2倍进行扩容，如果不够，就按照指定的minCapacity容量进行扩容
    /**
     * Increases the capacity to ensure that it can hold at least the
     * number of elements specified by the minimum capacity argument.
     *
     * @param minCapacity the desired minimum capacity
     */
    private void grow(int minCapacity) {
        // overflow-conscious code
        int oldCapacity = elementData.length;
        int newCapacity = oldCapacity + (oldCapacity >> 1);
        if (newCapacity - minCapacity < 0)
            newCapacity = minCapacity;
        if (newCapacity - MAX_ARRAY_SIZE > 0)
            newCapacity = hugeCapacity(minCapacity);
        // minCapacity is usually close to size, so this is a win:
        elementData = Arrays.copyOf(elementData, newCapacity);
    }
```

Fail-Fast机制： 
ArrayList也采用了快速失败的机制，通过记录modCount参数来实现。在面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险。具体介绍请参考这篇文章深入Java集合学习系列：HashMap的实现原理 中的Fail-Fast机制。
ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法。Arrays.copyof()内部最终也调用了System.arraycopy()，只是调用之前先判断数组类型是否一致，再选择创建相应类型的数组（不懂可以看源码），再调用System.arraycopy()这个native方法进行数组的拷贝。

Java Array和Arrays
-----------------
Array是数组类，Arrays是一个工具类，类似于collection和collections的区别。

分布式如何实现session共享
----------------
参考资料http://blog.csdn.net/sxiaobei/article/details/57086489
 1. 服务器实现的session复制或session共享，这类型的共享session是和服务器紧密相关的，比如webSphere或JBOSS在搭建集群时候可以配置实现session复制或session共享，但是这种方式有一个致命的缺点，就是不好扩展和移植，比如我们更换服务器，那么就要修改服务器配置（因为会涉及到服务器之间的直接http请求，因此会将服务器的IP写死在相应的配置文件中）。
 2. 利用成熟的技术做session复制，比如12306使用的gemfire，比如常见的内存数据库如redis或memorycache，这类方案虽然比较普适，但是严重依赖于第三方，这样当第三方服务器出现问题的时候，那么将是应用的灾难。
 3. 将session维护在客户端，很容易想到就是利用cookie，但是客户端存在风险，数据不安全，而且可以存放的数据量比较小，所以将session维护在客户端还要对session中的信息加密。
 4. 使用Spring Session
Spring Session是Spring的项目之一，GitHub地址：https://github.com/spring-projects/spring-session。
Spring Session提供了一套创建和管理Servlet HttpSession的方案。Spring Session提供了集群Session（Clustered Sessions）功能，默认采用外置的Redis来存储Session数据，以此来解决Session共享的问题。
这是官网的介绍：
Spring Session provides an API and implementations for managing a user’s session information.

Features

Spring Session provides the following features:

API and implementations for managing a user's session
HttpSession - allows replacing the HttpSession in an application container (i.e. Tomcat) neutral way
Clustered Sessions - Spring Session makes it trivial to support clustered sessions without being tied to an application container specific solution.
Multiple Browser Sessions - Spring Session supports managing multiple users' sessions in a single browser instance (i.e. multiple authenticated accounts similar to Google).
RESTful APIs - Spring Session allows providing session ids in headers to work with RESTful APIs
WebSocket - provides the ability to keep the HttpSession alive when receiving WebSocket messages
使用spring-session需要进行如下一些配置：

 - 添加spring-session-data-redis依赖
 
 ```html
 <dependency>  
    <groupId>org.springframework.session</groupId>  
    <artifactId>spring-session-data-redis</artifactId>  
    <version>1.0.1.RELEASE</version>  
</dependency>  
 ```
 
 - 第二步，编写一个配置类，用来启用RedisHttpSession功能，并向Spring容器中注册一个RedisConnectionFactory。
```java
import org.springframework.context.annotation.Bean;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.connection.jedis.JedisConnectionFactory;
import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession;

@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 7200)
public class RedisHttpSessionConfig {

    @Bean
    public RedisConnectionFactory connectionFactory() {
        JedisConnectionFactory connectionFactory = new JedisConnectionFactory();
        connectionFactory.setPort(6379);
        connectionFactory.setHostName("10.18.15.190");
        return connectionFactory;
    }
}
```

 - 第三步，将RedisHttpSessionConfig加入到WebInitializer#getRootConfigClasses()中，让Spring容器加载RedisHttpSessionConfig类。WebInitializer是一个自定义的AbstractAnnotationConfigDispatcherServletInitializer实现类，该类会在Servlet启动时加载（当然也可以采用别的加载方法，比如采用扫描@Configuration注解类的方式等等）。
```java
//该类采用Java Configuration，来代替web.xml   
public class WebInitializer extends AbstractAnnotationConfigDispatcherServletInitializer {
    
    @Override
    protected Class<?>[] getRootConfigClasses() {
        return new Class[]{Config1.class, Config2.class, RedisHttpSessionConfig.class};
    }
	
	//......
}
```
 - 第四步，编写一个一个AbstractHttpSessionApplicationInitializer实现类，用于向Servlet容器中添加springSessionRepositoryFilter。
```java
import org.springframework.session.web.context.AbstractHttpSessionApplicationInitializer;  
  
public class SpringSessionInitializer extends AbstractHttpSessionApplicationInitializer {  
} 
```

Java中的Lock和synchronized两种锁定机制的对比
-----------------------------------------
参考资料https://www.cnblogs.com/xrq730/p/4979021.html
总的来说，Lock接口是对synchronized语法的一个扩展，它支持更加灵活的锁定范围，以及一些灵活的锁定方式，例如允许**并发读**。
直接复制的JDK1.6官方文档相关部分：
synchronized 方法或语句的使用提供了对与每个对象相关的隐式监视器锁的访问，但却强制所有锁获取和释放均要出现在一个块结构中：当获取了多个锁时，它们必须以相反的顺序释放，且必须在与所有锁被获取时相同的词法范围内释放所有锁。

虽然 synchronized 方法和语句的范围机制使得使用监视器锁编程方便了很多，而且还帮助避免了很多涉及到锁的常见编程错误，但有时也需要以**更为灵活**的方式使用锁。例如，某些遍历并发访问的数据结果的算法要求使用 "hand-over-hand" 或 "chain locking"：获取节点 A 的锁，然后再获取节点 B 的锁，然后释放 A 并获取 C，然后释放 B 并获取 D，依此类推。Lock 接口的实现允许锁在不同的作用范围内获取和释放，并允许以任何顺序获取和释放多个锁，从而支持使用这种技术。

随着灵活性的增加，也带来了更多的责任。不使用块结构锁就失去了使用 synchronized 方法和语句时会出现的锁自动释放功能。在大多数情况下，应该使用以下语句：
```java
Lock l = ...; 
     l.lock();
     try {
         // access the resource protected by this lock
     } finally {
         l.unlock();
     }
 
```
Lock 实现提供了使用 synchronized 方法和语句所没有的其他功能，包括提供了一个非块结构的获取锁尝试 (tryLock())、一个获取可中断锁的尝试 (lockInterruptibly()) 和一个获取超时失效锁的尝试 (tryLock(long, TimeUnit))。

Spring中ApplicationContext和beanfactory区别
---------------------------------------
org.springframework.beans  and  org.springframework.context  是springIOC容器的基础。BeanFactory  接口提供了一套完备的配置机制，使得它可以管理任何类型的对象。  ApplicationContext  是BeanFactory  的一个子接口。  ApplicationContext子接口实现了与AOP的宽松集成：消息处理与集成；web应用中的应用层context，比如说WebApplicationContext  。
简单来说，BeanFactory  提供框架级别的配置以及基础的功能，在此基础上，ApplicationContext添加了更多针对企业级的特定功能。Applicationcontext是beanfactory的的一个完整扩展集。如果要使用beanfactory而不是applicationcontext，可以参考spring的官方文档  Section  7.16, “The BeanFactory”.
在spring中，那些由IOC容器管理的对象组成了应用的骨架，这些对象被称为beans。

设计模式之六大原则
---------
 1. 单一职责原则(Single Responsibility Principle,
    SRP)：一个类只负责一个功能领域中的相应职责，或者可以定义为：就一个类而言，应该只有一个引起它变化的原因。

 2. 开闭原则(Open-Closed Principle, OCP)：一个软件实体应当对扩展开放，对修改关闭。即软件实体应尽量在不修改原有代码的情况下进行扩展。

 3. 里氏代换原则(Liskov Substitution Principle, LSP)：所有引用基类（父类）的地方必须能透明地使用其子类的对象。

 4. 依赖倒转原则(Dependency Inversion  Principle,
    DIP)：抽象不应该依赖于细节，细节应当依赖于抽象。换言之，要针对接口编程，而不是针对实现编程。

 5. 接口隔离原则(Interface  Segregation Principle,
    ISP)：使用多个专门的接口，而不使用单一的总接口，即客户端不应该依赖那些它不需要的接口。

 6. 迪米特法则(Law of  Demeter, LoD)：一个软件实体应当尽可能少地与其他实体发生相互作用。
 

linux中可以同时查看一个文件的前几行和末尾几行
-------------------------
可以使用head（查看前几行）、tail（查看末尾几行）两个命令。

例如：

查看/etc/profile的前10行内容，应该是：

 head -n 10 /etc/profile
 
查看/etc/profile的最后5行内容，应该是：

 tail  -n 5 /etc/profile
 
如果想同时查看可以将前10行和后5行的显示信息通过输出重定向的方法保存到一个文档，这样查看文档即可一目了然。

例如：

将内容输出到/home/test文件中

 head -n 10 /etc/profile >>/home/test
 
 tail  -n 5 /etc/profile>>/home/test
 
查看的话只需要打开test文件即可。

cat /home/test

linux文本内容查找
-----------
从文件内容查找匹配指定字符串的行：
$ grep "被查找的字符串" 文件名
从文件内容查找与正则表达式匹配的行：
$ grep –e “正则表达式” 文件名
查找时不区分大小写：
$ grep –i "被查找的字符串" 文件名
查找匹配的行数：
$ grep -c "被查找的字符串" 文件名
从文件内容查找不匹配指定字符串的行：
$ grep –v "被查找的字符串" 文件名

从根目录开始查找所有扩展名为.log的文本文件，并找出包含”ERROR”的行
find / -type f -name "*.log" | xargs grep "ERROR"

上面的**xargs**参数，参考链接http://czmmiao.iteye.com/blog/1949225，它的作用就是多个命令，或者引用参考博客里的一段话，“之所以能用到这个命令，关键是由于很多命令不支持|管道来传递参数，而日常工作中有有这个必要，所以就有了xargs命令”。

系统查找到httpd.conf文件后即时在屏幕上显示httpd.conf文件信息。 
find/-name"httpd.conf"-ls

在根目录下查找某个文件
find . -name "test"

在某个目录下查找包含某个字符串的文件

grep -r "zh_CN" ./

violatile关键字
------------
参考链接http://www.cnblogs.com/dolphin0520/p/3920373.html
由于所有的程序代码执行都是通过在CPU中执行线程来实现的，一般运算过程中的临时数据存储在物理内存（主存）中，但是主存的速度远低于CPU，这时CPU就需要使用告诉缓存（cache）。
在CPU进行计算时，直接从cache中读取数据，而不是主存。
对共享变量，在单线程程序中是没有问题的，但是在多线程程序中就会出现问题，因为各个线程读取的cache可能不同步。
如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。

　　为了解决缓存不一致性问题，通常来说有以下2种解决方法：
 1. 通过在总线加LOCK#锁的方式
 2. 通过缓存一致性协议
两种方式都是通过硬件层面进行控制。
在早期的CPU当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。

　　但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。

　　所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。
　　

volatile的原理和实现机制是什么？
--------------------
下面这段话摘自《深入理解Java虚拟机》：

　　“观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令”

　　lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：

　　1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成；

　　2）它会强制将对缓存的修改操作立即写入主存；

　　3）如果是写操作，它会导致其他CPU中对应的缓存行无效。

　　

并发编程中的原子性、可见性、有序性
-----------------

 - 原子性

某一个操作，或者一系列操作，要么成功。要么失败。
例如i = 9;就是一个院子操作。
假若一个线程执行到这个语句时，我暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。
那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。

 - 可见性
对于共享变量，一个线程修改了它，其他线程能够立马读取到修改后的值。
　
 - 有序性

即程序执行的顺序按照代码的先后顺序执行。
一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。因为处理器（**不是jvm**）在进行指令重排序之前，需要考虑数据之间的以来关系。
**虽然**指令重排序不会影响**单线程**程序的执行结果，但是对于多线程程序，就可能产生影响。比如下面的代码：
```java
//线程1:
context = loadContext();   //语句1
inited = true;             //语句2
 
//线程2:
while(!inited ){
  sleep()
}
doSomethingwithconfig(context);
```
上面的代码，如果线程2执行的时候，线程1还未对context进行初始化，那么就会出现错误。
**总之**，对于并发编程，原子性、可见性、有序性缺一不可。

Java内存模型
--------
Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。
那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？

 - 原子性
在Java中，对**基本数据类型**的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。
请分析以下哪些操作是原子性操作：
```java
x = 10;         //语句1
y = x;         //语句2
x++;           //语句3
x = x + 1;     //语句4
```
上面语句，只有语句1是原子操作，其他的都不是。
从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。
 - 可见性
Java提供了volatile关键字来保证可见性。
另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。
 - 有序性
 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。

jvm性能分析
-------
linux系统，一般的逻辑是使用ps命令查看Java的进程id，然后使用
```linux
jmap -dump:format=b,file=3.dump 11459
```
这个命令可以将堆栈的信息以文件的方式输出，然后可以使用MAT工具打开。
如果要查看当前PID的每个线程的状态，也可以使用jstack命令
```linux
audi@audi-PC:~/Desktop$ jstack -l 21703
Picked up _JAVA_OPTIONS:   -Dawt.useSystemAAFontSettings=gasp
2017-11-29 23:35:09
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.151-b12 mixed mode):

"Attach Listener" #9 daemon prio=9 os_prio=0 tid=0x00007fa9c0001000 nid=0x5544 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Service Thread" #8 daemon prio=9 os_prio=0 tid=0x00007fa9f80cb000 nid=0x54db runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C1 CompilerThread2" #7 daemon prio=9 os_prio=0 tid=0x00007fa9f80be000 nid=0x54da waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread1" #6 daemon prio=9 os_prio=0 tid=0x00007fa9f80bc000 nid=0x54d9 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread0" #5 daemon prio=9 os_prio=0 tid=0x00007fa9f80b9000 nid=0x54d8 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Signal Dispatcher" #4 daemon prio=9 os_prio=0 tid=0x00007fa9f80b7800 nid=0x54d7 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007fa9f8085000 nid=0x54d6 in Object.wait() [0x00007fa9e6b0c000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000ff988ec8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x00000000ff988ec8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

   Locked ownable synchronizers:
	- None

"Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007fa9f8080000 nid=0x54d5 in Object.wait() [0x00007fa9e6b4d000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000ff986b68> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x00000000ff986b68> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

   Locked ownable synchronizers:
	- None

"main" #1 prio=5 os_prio=0 tid=0x00007fa9f800b000 nid=0x54c9 runnable [0x00007fa9fed81000]
   java.lang.Thread.State: RUNNABLE
	at com.audi.jvm.HeapOOM.main(HeapOOM.java:13)

   Locked ownable synchronizers:
	- None

"VM Thread" os_prio=0 tid=0x00007fa9f8078800 nid=0x54d4 runnable 

"GC task thread#0 (ParallelGC)" os_prio=0 tid=0x00007fa9f8020000 nid=0x54d0 runnable 

"GC task thread#1 (ParallelGC)" os_prio=0 tid=0x00007fa9f8021800 nid=0x54d1 runnable 

"GC task thread#2 (ParallelGC)" os_prio=0 tid=0x00007fa9f8023800 nid=0x54d2 runnable 

"GC task thread#3 (ParallelGC)" os_prio=0 tid=0x00007fa9f8025000 nid=0x54d3 runnable 

"VM Periodic Task Thread" os_prio=0 tid=0x00007fa9f80ce000 nid=0x54dc waiting on condition 

JNI global references: 6

```

JVM性能瓶颈定位（CPU占用异常高、但内存使用正常）
---------

测试代码如下（测试代码新加的，和后面的问题定位的行数可能不一致，但是方法是通用的，不影响我们的分析）：
```java
package com.audi.jvm;

import java.util.ArrayList;
import java.util.List;

public class Dump
{
	static class OOMObject{}
	
	public static void main(String[] args)
	{
		List<OOMObject> list = new ArrayList<>();
		
		while (true)
		{
			//list.add(new OOMObject());
		}
	}
}

```
 1. 使用top命令找到最耗CPU的进程ID。
```linux

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                       
23729 audi      20   0 2325496  23864  16096 S 100.0  0.3   9:41.56 java                          
  538 root      20   0  472828 159524 111776 S  25.0  2.0  12:59.41 Xorg                          
 1108 audi      20   0 1257172 174420  88856 S   6.2  2.2  17:03.47 deepin-wm                     
 1649 audi      20   0    9160   6504   1920 S   6.2  0.1  22:32.07 wineserver.real               
 2447 audi      20   0 1467844 276216 148520 S   6.2  3.4   4:59.35 chrome                        
 5558 audi      20   0  606828  41644  31080 S   6.2  0.5   0:45.75 deepin-terminal               
    1 root      20   0  212524   7592   5896 S   0.0  0.1   0:01.59 systemd                       
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.01 kthreadd                      
    3 root      20   0       0      0      0 S   0.0  0.0   0:01.50 ksoftirqd/0                   
    5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H                  
    7 root      20   0       0      0      0 S   0.0  0.0   0:09.79 rcu_preempt                   
    8 root      20   0       0      0      0 S   0.0  0.0   0:00.00 rcu_sched                     
    9 root      20   0       0      0      0 S   0.0  0.0   0:00.00 rcu_bh                        
   10 root      rt   0       0      0      0 S   0.0  0.0   0:00.17 migration/0                   
   11 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 lru-add-drain   
```

 2. 得到最耗CPU的进程ID以后，继续查找，在该进程下，哪一个线程最耗费CPU资源。
```linux
audi@audi-PC:~/Desktop$ ps p 23729 -L -o pcpu,pid,tid,time,tname,cmd
%CPU   PID   TID     TIME TTY      CMD
 0.0 23729 23729 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
99.8 23729 23732 00:11:55 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23737 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23738 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23739 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23740 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23742 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23743 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23744 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23745 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23746 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23747 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23748 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23749 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256
 0.0 23729 23750 00:00:00 ?        /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Xms20m -Xmx20m -Xss256

```
从TID这一列可以看出来，最耗费CPU的线程是**23732**，转换成16进制是0x5CB4，这时候就需要使用jstack命令来查看进程**23729**的内部线程的运行情况：
```linux
audi@audi-PC:~/Desktop$ jstack -l 23729
Picked up _JAVA_OPTIONS:   -Dawt.useSystemAAFontSettings=gasp
2017-11-29 23:57:06
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.151-b12 mixed mode):

"Attach Listener" #9 daemon prio=9 os_prio=0 tid=0x00007fb1cc001000 nid=0x6d71 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Service Thread" #8 daemon prio=9 os_prio=0 tid=0x00007fb2040d3000 nid=0x5cc5 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C1 CompilerThread2" #7 daemon prio=9 os_prio=0 tid=0x00007fb2040be000 nid=0x5cc4 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread1" #6 daemon prio=9 os_prio=0 tid=0x00007fb2040bc000 nid=0x5cc3 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread0" #5 daemon prio=9 os_prio=0 tid=0x00007fb2040b9000 nid=0x5cc2 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Signal Dispatcher" #4 daemon prio=9 os_prio=0 tid=0x00007fb2040b7800 nid=0x5cc1 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007fb204085000 nid=0x5cc0 in Object.wait() [0x00007fb1f26cd000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000ff988ec8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x00000000ff988ec8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

   Locked ownable synchronizers:
	- None

"Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007fb204080000 nid=0x5cbf in Object.wait() [0x00007fb1f270e000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000ff986b68> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x00000000ff986b68> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

   Locked ownable synchronizers:
	- None

"main" #1 prio=5 os_prio=0 tid=0x00007fb20400b000 nid=0x5cb4 runnable [0x00007fb20a949000]
   java.lang.Thread.State: RUNNABLE
	at com.audi.jvm.HeapOOM.main(HeapOOM.java:13)

   Locked ownable synchronizers:
	- None

"VM Thread" os_prio=0 tid=0x00007fb204078800 nid=0x5cbe runnable 

"GC task thread#0 (ParallelGC)" os_prio=0 tid=0x00007fb204020000 nid=0x5cb9 runnable 

"GC task thread#1 (ParallelGC)" os_prio=0 tid=0x00007fb204021800 nid=0x5cba runnable 

"GC task thread#2 (ParallelGC)" os_prio=0 tid=0x00007fb204023800 nid=0x5cbb runnable 

"GC task thread#3 (ParallelGC)" os_prio=0 tid=0x00007fb204025000 nid=0x5cbc runnable 

"VM Periodic Task Thread" os_prio=0 tid=0x00007fb2040d6000 nid=0x5cc6 waiting on condition 

JNI global references: 6

```
注意上面的，nid=0x5cb4，就是我们需要定位的线程：
```linux
"main" #1 prio=5 os_prio=0 tid=0x00007fb20400b000 nid=0x5cb4 runnable [0x00007fb20a949000]
   java.lang.Thread.State: RUNNABLE
	at com.audi.jvm.HeapOOM.main(HeapOOM.java:13)

```
显示是在HeapOOM.java:13的第十三行出了问题，结果也确实是，我在那里写了while死循环。

jvm内存泄漏检测（CPU使用正常，但是内存占用持续增高）
---------

测试代码如下：
```java
package com.audi.jvm;

import java.util.ArrayList;
import java.util.List;

/**
 * @author Administrator
 * VM参数：-Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError
 *
 */
public class Dump
{
	static class OOMObject{}
	
	public static void main(String[] args)
	{
		List<OOMObject> list = new ArrayList<>();
		
		while (true)
		{
			list.add(new OOMObject());
		}
	}
}
```

出现内存泄漏，一般在没有出现OOM之前，内存占用率会一直上升，除了可以使用jconsle以及visual VM等工具进行在线查看以外，还可以使用-XX:+HeapDumpOnOutOfMemoryError参数配置虚拟机，使虚拟机在发生OOM的时候生成*.hprof堆快照文件；类似的使用jmap命令：
```linux
jmap -dump:format=b,file=3.hprof 11459
```
也可以生成堆快照文件。不过它不需要在OOM的时候才产生堆快照文件，调用这个命令一次就产生一个快照文件（注意修改文件名以便于产生不同的快照文件）。
然后使用eclipse的MAT工具打开生成的堆快照文件，选择Leak Suspects Report：
![Leak Suspects Report][2]
在上面的测试代码中加入线程休眠：
```java
Thread.sleep(3);
```
延迟OOM出现的时间，方便我们在各个时刻生成亏按照文件。
实际中我生成了两个hprof文件。分别为5.hprof和6.hprof。依次使用mat打开，结果如下：
稍早的快照文件：
![此处输入图片的描述][3]
稍后的快照文件：
![此处输入图片的描述][4]
从两张图片，基本可以看出问题出在图中深蓝色的区域，因为只有这个区域随着时间的增长在持续增长，点击上图中左下角的
Dominator Tree: List the biggest objects and what they keep alive.，查看大对象在堆中的内存占用情况，结果如下图所示。
![此处输入图片的描述][5]
上图中我手动标红的部分就是我们发生线程泄漏的地方，因为它就占用了34%左右的堆内存。
下图是更具体的内存泄漏的地方：
![此处输入图片的描述][6]
**注意**这里只是简单的模拟了一下内存泄漏，实际生产如果发生内存泄漏肯定比这个复杂的多，需要工作中进行积累与思考。

Redis、Memcache和MongoDB的区别
-------------------------

 
参考链接：http://www.cnblogs.com/tuyile006/p/6382062.html
 - Memcached
============

 - Memcached的优点：
Memcached可以利用多核优势，单实例吞吐量极高，可以达到几十万QPS（取决于key、value的字节大小以及服务器硬件性能，日常环境中QPS高峰大约在4-6w左右）。适用于最大程度扛量。支持直接配置为session handle。

 - Memcached的局限性：
只支持简单的key/value数据结构，不像Redis可以支持丰富的数据类型。
无法进行持久化，数据不能备份，只能用于缓存使用，且重启后数据全部丢失。
无法进行数据同步，不能将MC中的数据迁移到其他MC实例中。
Memcached内存分配采用Slab Allocation机制管理内存，value大小分布差异较大时会造成内存利用率降低，并引发低利用率时依然出现踢出等问题。需要用户注重value设计。

 

 - Redis
=======
 - Redis的优点：
支持多种数据结构，如 string（字符串）、 list(双向链表)、dict(hash表)、set(集合）、zset(排序set)、hyperloglog（基数估算）
支持持久化操作，可以进行aof及rdb数据持久化到磁盘，从而进行数据备份或数据恢复等操作，较好的防止数据丢失的手段。
支持通过Replication进行数据复制，通过master-slave机制，可以实时进行数据的同步复制，支持多级复制和增量复制，master-slave机制是Redis进行HA的重要手段。
单线程请求，所有命令串行执行，并发情况下不需要考虑数据一致性问题。
支持pub/sub消息订阅机制，可以用来进行消息订阅与通知。
支持简单的事务需求，但业界使用场景很少，并不成熟。
 - Redis的局限性：
Redis只能使用单线程，性能受限于CPU性能，故单实例CPU最高才可能达到5-6wQPS每秒（取决于数据结构，数据大小以及服务器硬件性能，日常环境中QPS高峰大约在1-2w左右）。
支持简单的事务需求，但业界使用场景很少，并不成熟，既是优点也是缺点。
Redis在string类型上会消耗较多内存，可以使用dict（hash表）压缩存储以降低内存耗用。

Mc和Redis都是Key-Value类型，不适合在不同数据集之间建立关系，也不适合进行查询搜索。比如redis的keys pattern这种匹配操作，对redis的性能是灾难。

 - mongoDB
mongoDB 是一种文档性的数据库。先解释一下文档的数据库，即可以存放xml、json、bson类型系那个的数据。
这些数据具备自述性（self-describing），呈现分层的树状数据结构。redis可以用hash存放简单关系型数据。
mongoDB 存放**json**格式数据。
适合场景：事件记录、内容管理或者博客平台，比如评论系统。 
 - mongodb持久化原理
mongodb与mysql不同，mysql的每一次更新操作都会直接写入硬盘，但是mongo不会，做为内存型数据库，数据操作会先写入内存，然后再会持久化到硬盘中去，那么mongo是如何持久化的呢
mongodb在启动时，专门初始化一个线程不断循环（除非应用crash掉），用于在一定时间周期内来从defer队列中获取要持久化的数据并写入到磁盘的journal(日志)和mongofile(数据)处，当然因为它不是在用户添加记录时就写到磁盘上，所以按mongodb开发者说，它不会造成性能上的损耗，因为看过代码发现，当进行CUD操作时，记录(Record类型)都被放入到defer队列中以供延时批量（groupcommit）提交写入，但相信其中时间周期参数是个要认真考量的参数，系统为90毫秒，如果该值更低的话，可能会造成频繁磁盘操作，过高又会造成系统宕机时数据丢失过。
 - 什么是NoSQL数据库？NoSQL和RDBMS有什么区别？在哪些情况下使用和不使用NoSQL数据库？
NoSQL是非关系型数据库，NoSQL = Not Only SQL。
关系型数据库采用的结构化的数据，NoSQL采用的是键值对的方式存储数据。
在处理非结构化/半结构化的大数据时；在水平方向上进行扩展时；随时应对动态增加的数据项时可以优先考虑使用NoSQL数据库。
在考虑数据库的成熟度；支持；分析和商业智能；管理及专业性等问题时，应优先考虑关系型数据库。 
 - MySQL和MongoDB之间最基本的区别是什么？
关系型数据库与非关系型数据库的区别，即数据存储结构的不同。
 - MongoDB的特点是什么？
（1）面向文档（2）高性能（3）高可用（4）易扩展（5）丰富的查询语言 
 - MongoDB支持存储过程吗？如果支持的话，怎么用？
MongoDB支持存储过程，它是javascript写的，保存在db.system.js表中。
 - 如何理解MongoDB中的GridFS机制，MongoDB为何使用GridFS来存储文件？
GridFS是一种将大型文件存储在MongoDB中的文件规范。使用GridFS可以将大文件分隔成多个小文档存放，这样我们能够有效的保存大文档，而且解决了BSON对象有限制的问题。 
 - 为什么MongoDB的数据文件很大？
MongoDB采用的预分配空间的方式来防止文件碎片。
 - 当更新一个正在被迁移的块（Chunk）上的文档时会发生什么？
更新操作会立即发生在旧的块（Chunk）上，然后更改才会在所有权转移前复制到新的分片上。
 - MongoDB在A:{B,C}上建立索引，查询A:{B,C}和A:{C,B}都会使用索引吗？
不会，只会在A:{B,C}上使用索引。
 - 如果一个分片（Shard）停止或很慢的时候，发起一个查询会怎样？
如果一个分片停止了，除非查询设置了“Partial”选项，否则查询会返回一个错误。如果一个分片响应很慢，MongoDB会等待它的响应。

Redis、Memcache和MongoDB的区别
=========================
从以下几个维度，对redis、memcache、mongoDB 做了对比，

1、性能

都比较高，性能对我们来说应该都不是瓶颈

总体来讲，TPS方面redis和memcache差不多，要大于mongodb

2、操作的便利性

memcache数据结构单一

redis丰富一些，数据操作方面，redis更好一些，较少的网络IO次数

mongodb支持丰富的数据表达，索引，最类似关系型数据库，支持的查询语言非常丰富

3、内存空间的大小和数据量的大小

redis在2.0版本后增加了自己的VM特性，突破物理内存的限制；可以对key value设置过期时间（类似memcache）

memcache可以修改最大可用内存,采用LRU算法

mongoDB适合大数据量的存储，依赖操作系统VM做内存管理，吃内存也比较厉害，服务不要和别的服务在一起

4、可用性（单点问题）

对于单点问题，

redis，依赖客户端来实现分布式读写；主从复制时，每次从节点重新连接主节点都要依赖整个快照,无增量复制，因性能和效率问题，

所以单点问题比较复杂；不支持自动sharding,需要依赖程序设定一致hash 机制。

一种替代方案是，不用redis本身的复制机制，采用自己做主动复制（多份存储），或者改成增量复制的方式（需要自己实现），一致性问题和性能的权衡

Memcache本身没有数据冗余机制，也没必要；对于故障预防，采用依赖成熟的hash或者环状的算法，解决单点故障引起的抖动问题。

mongoDB支持master-slave,replicaset（内部采用paxos选举算法，自动故障恢复）,auto sharding机制，对客户端屏蔽了故障转移和切分机制。

5、可靠性（持久化）

对于数据持久化和数据恢复，

redis支持（快照、AOF）：依赖快照进行持久化，aof增强了可靠性的同时，对性能有所影响

memcache不支持，通常用在做缓存,提升性能；

MongoDB从1.8版本开始采用binlog方式支持持久化的可靠性

6、数据一致性（事务支持）

Memcache 在并发场景下，用cas保证一致性

redis事务支持比较弱，只能保证事务中的每个操作连续执行

mongoDB不支持事务

7、数据分析

mongoDB内置了数据分析的功能(mapreduce),其他不支持

8、应用场景

redis：数据量较小的更性能操作和运算上

memcache：用于在动态系统中减少数据库负载，提升性能;做缓存，提高性能（适合读多写少，对于数据量比较大，可以采用sharding）

MongoDB:主要解决海量数据的访问效率问题

Java的threadlocal类
-----------------
java的threadlocal类，该类提供了线程局部 (thread-local) 变量。这些变量不同于它们的普通对应物，因为访问某个变量（通过其 get 或 set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。ThreadLocal 实例通常是类中的 private static 字段，它们希望将状态与某一个线程（例如，用户 ID 或事务 ID）相关联。——出自JDK1.6文档。
Thread类中有这么一个变量，threadLocals，这个属性就是ThreadLocalMap类型的。
```java
/* ThreadLocal values pertaining to this thread. This map is maintained
 * by the ThreadLocal class. */
ThreadLocal.ThreadLocalMap threadLocals = null;
```
threadLocals属于当前线程实例，即，每一个线程都有一个自己的threadLocals属性。
ThreadLocalMap中底层实现，是以ThreadLocal类型实例对象为key的hashmap。比如下面代码，key就是name，value就是haha（通过set方法实现设置）。
```java
ThreadLocal<String> name = new ThreadLocal<>();
name.set("haha");
System.out.println(name.get());
```
下面的例子出自《疯狂java讲义》
```java
package com.audi;

/**
 * Description:
 * <br/>网站: <a href="http://www.crazyit.org">疯狂Java联盟</a>
 * <br/>Copyright (C), 2001-2016, Yeeku.H.Lee
 * <br/>This program is protected by copyright laws.
 * <br/>Program Name:
 * <br/>Date:
 * @author Yeeku.H.Lee kongyeeku@163.com
 * @version 1.0
 */
class Account
{
	/* 定义一个ThreadLocal类型的变量，该变量将是一个线程局部变量
	每个线程都会保留该变量的一个副本 */
	private ThreadLocal<String> name = new ThreadLocal<>();
	// 定义一个初始化name成员变量的构造器
	public Account(String str)
	{
		this.name.set(str);
		// 下面代码用于访问当前线程的name副本的值
		System.out.println("---" + this.name.get());
	}
	// name的setter和getter方法
	public String getName()
	{
		return name.get();
	}
	public void setName(String str)
	{
		this.name.set(str);
	}
}
class MyTest extends Thread
{
	// 定义一个Account类型的成员变量
	private Account account;
	public MyTest(Account account, String name)
	{
		super(name);
		this.account = account;
	}
	public void run()
	{
		// 循环10次
		for (int i = 0 ; i < 10 ; i++)
		{
			// 当i == 6时输出将账户名替换成当前线程名
			if (i == 6)
			{
				account.setName(getName());
			}
			// 输出同一个账户的账户名和循环变量
			System.out.println(account.getName()
				+ " 账户的i值：" + i);
		}
	}
}
public class ThreadLocalTest
{
	public static void main(String[] args)
	{
		// 启动两条线程，两条线程共享同一个Account
		Account at = new Account("初始名");
		/*
		虽然两条线程共享同一个账户，即只有一个账户名
		但由于账户名是ThreadLocal类型的，所以每条线程
		都完全拥有各自的账户名副本，所以从i == 6之后，将看到两条
		线程访问同一个账户时看到不同的账户名。
		*/
		new MyTest(at , "线程甲").start();
		new MyTest(at , "线程乙").start ();
	}
}
```
代码输出结果如下：
```java
---初始名
null 账户的i值：0
null 账户的i值：1
null 账户的i值：2
null 账户的i值：3
null 账户的i值：4
null 账户的i值：5
线程甲 账户的i值：6
线程甲 账户的i值：7
线程甲 账户的i值：8
线程甲 账户的i值：9
null 账户的i值：0
null 账户的i值：1
null 账户的i值：2
null 账户的i值：3
null 账户的i值：4
null 账户的i值：5
线程乙 账户的i值：6
线程乙 账户的i值：7
线程乙 账户的i值：8
线程乙 账户的i值：9
```

JDK动态代理与CGLIB动态代理
-----------------

jdk动态代理
=======
jdk动态需要被代理的对象有接口，至于为什么，这是跟JDK动态代理的实现过程有关的。
一般的JDK动态代理都使用JAVA提供的**Proxy**类的
```java
public static Object newProxyInstance(ClassLoader loader,
                                      Class<?>[] interfaces,
                                      InvocationHandler h)
                               throws IllegalArgumentException
```
来返回一个代理对象，其中第二个参数就是被代理对象的接口名称，所以JDK 的动态代理是一定要有接口的。与之相对的，CGLIB动态代理则不需要。
JDK动态代理需要实现java.lang.reflect.InvocationHandler接口，通过实现接口的invoke方法实现动态代理。
```java
Object invoke(Object proxy,
              Method method,
              Object[] args)
              throws Throwable
```

摘抄一个《深入浅出Mybatis技术原理与实战》上的JDK动态代理的例子：
首先是接口HelloService：
```java
package com.audi.proxy.jdkProxy;

public interface HelloService
{
	public void sayHello(String name);
}

```
然后是接口的实现类HelloServiceImpl：
```java
package com.audi.proxy.jdkProxy;

public class HelloServiceImpl implements HelloService
{

	@Override
	public void sayHello(String name)
	{
		System.out.println("Hello "+name);
	}

}
```
然后是代理类：
```java
package com.audi.proxy.jdkProxy;

import java.lang.reflect.InvocationHandler;
import java.lang.reflect.Method;
import java.lang.reflect.Proxy;

public class HelloServiceProxy implements InvocationHandler
{
	/** 
	* @Fields target : 真实的服务对象,也就是HelloServiceImpl类
	*/ 
	private Object target;

	/** 
	* @Title: bind 
	* @Description: 绑定委托对象target，并返回一个Proxy类型的代理
	*/
	public Object bind(Object target)
	{
		this.target=target;
		System.out.println("取得代理对象");
		return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), this);
	}

	/** 
	* @Title: invoke 
	* @Description: TODO
	*/
	@Override
	public Object invoke(Object proxy, Method method, Object[] args) throws Throwable
	{
		System.out.println("jdk动态代理，进入invoke方法");
		Object result = null;
		System.out.println("反射方法调用前。。。");
		
		// 执行方法，相当于调用 HelloServiceImpl类的sayHello方法
		result = method.invoke(target, args);
		System.out.println("反射方法调用后。。。");
		return result;
	}

}
```
下面是jdk动态代理的测试代码：
```java
package com.audi.proxy.jdkProxy;

public class HelloServiceMain
{
	public static void main(String[] args)
	{
		HelloServiceProxy helloHandler = new HelloServiceProxy();
		HelloService proxy = (HelloService) helloHandler.bind(new HelloServiceImpl());
		proxy.sayHello("张三");
	}
}
```

 - 下面对jdk的源码实现过程进行一个简单的介绍
 首先是Proxy类的静态方法newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), this);
下面是JDK1.8中newProxyInstance方法的实现源码，注意其中的中文注释：
```java
/**
     * Returns an instance of a proxy class for the specified interfaces
     * that dispatches method invocations to the specified invocation
     * handler.
     *
     * <p>{@code Proxy.newProxyInstance} throws
     * {@code IllegalArgumentException} for the same reasons that
     * {@code Proxy.getProxyClass} does.
     *
     * @param   loader the class loader to define the proxy class
     * @param   interfaces the list of interfaces for the proxy class
     *          to implement
     * @param   h the invocation handler to dispatch method invocations to
     * @return  a proxy instance with the specified invocation handler of a
     *          proxy class that is defined by the specified class loader
     *          and that implements the specified interfaces
     * @throws  IllegalArgumentException if any of the restrictions on the
     *          parameters that may be passed to {@code getProxyClass}
     *          are violated
     * @throws  SecurityException if a security manager, <em>s</em>, is present
     *          and any of the following conditions is met:
     *          <ul>
     *          <li> the given {@code loader} is {@code null} and
     *               the caller's class loader is not {@code null} and the
     *               invocation of {@link SecurityManager#checkPermission
     *               s.checkPermission} with
     *               {@code RuntimePermission("getClassLoader")} permission
     *               denies access;</li>
     *          <li> for each proxy interface, {@code intf},
     *               the caller's class loader is not the same as or an
     *               ancestor of the class loader for {@code intf} and
     *               invocation of {@link SecurityManager#checkPackageAccess
     *               s.checkPackageAccess()} denies access to {@code intf};</li>
     *          <li> any of the given proxy interfaces is non-public and the
     *               caller class is not in the same {@linkplain Package runtime package}
     *               as the non-public interface and the invocation of
     *               {@link SecurityManager#checkPermission s.checkPermission} with
     *               {@code ReflectPermission("newProxyInPackage.{package name}")}
     *               permission denies access.</li>
     *          </ul>
     * @throws  NullPointerException if the {@code interfaces} array
     *          argument or any of its elements are {@code null}, or
     *          if the invocation handler, {@code h}, is
     *          {@code null}
     */
    @CallerSensitive
    public static Object newProxyInstance(ClassLoader loader,
                                          Class<?>[] interfaces,
                                          InvocationHandler h)
        throws IllegalArgumentException
    {
        Objects.requireNonNull(h);

        final Class<?>[] intfs = interfaces.clone();
        final SecurityManager sm = System.getSecurityManager();
        if (sm != null) {
            checkProxyAccess(Reflection.getCallerClass(), loader, intfs);
        }

        /*
         * Look up or generate the designated proxy class.
         *
         */
        Class<?> cl = getProxyClass0(loader, intfs);

        /*
         * Invoke its constructor with the designated invocation handler.
         */
        try {
            if (sm != null) {
                checkNewProxyPermission(Reflection.getCallerClass(), cl);
            }

            final Constructor<?> cons = cl.getConstructor(constructorParams);
            final InvocationHandler ih = h;
            if (!Modifier.isPublic(cl.getModifiers())) {
                AccessController.doPrivileged(new PrivilegedAction<Void>() {
                    public Void run() {
                        cons.setAccessible(true);
                        return null;
                    }
                });
            }
            return cons.newInstance(new Object[]{h});
        } catch (IllegalAccessException|InstantiationException e) {
            throw new InternalError(e.toString(), e);
        } catch (InvocationTargetException e) {
            Throwable t = e.getCause();
            if (t instanceof RuntimeException) {
                throw (RuntimeException) t;
            } else {
                throw new InternalError(t.toString(), t);
            }
        } catch (NoSuchMethodException e) {
            throw new InternalError(e.toString(), e);
        }
    }
```

CGLIB动态代理
=========
使用cglib实现动态代理，首先通过net.sf.cglib.proxy.Enhancer的create方法获得代理对象 ，然后通过实现net.sf.cglib.proxy.MethodInterceptor接口的intecept方法，从而实现动态代理。
首先，是代理类ClassHasNoInterface的实现过程：
```java
package cglib;

public class ClassHasNoInterface {
	
	public void  method(){
		System.out.println("建立自己的知识体系还是很重要的，尽管觉得麻烦");
	}
	public void function(){
		System.out.println("如果我只停留在使用的别人开发的工具阶段，那么再过5年我也对不起程序员这个称呼");
	}
}
```
然后是代理类：
```java
package cglib;

import java.lang.reflect.Method;

import net.sf.cglib.proxy.Enhancer;
import net.sf.cglib.proxy.MethodInterceptor;
import net.sf.cglib.proxy.MethodProxy;

public class CglibTs implements MethodInterceptor{

	private Enhancer enhancer = new Enhancer();
	
	
	public Object getProxy(Class clazz){
		//生成指定类对象的子类,也就是重写类中的业务函数，在重写中加入intercept()函数而已。
		enhancer.setSuperclass(clazz);
		//这里是回调函数，enhancer中肯定有个MethodInterceptor属性。
		//回调函数是在setSuperclass中的那些重写的方法中调用---猜想
		enhancer.setCallback(this);
		//创建这个子类对象
		return enhancer.create();
	}
	public Object intercept(Object obj, Method method, Object[] args,
			MethodProxy proxy) throws Throwable {
		System.out.println(method.getName()+"执行之前做一些准备工作");
		//一不小心写成下面被注释一行代码了。 StackOverflowError
		//Object result = method.invoke(obj, args); 想不通
		Object result = proxy.invokeSuper(obj,args);
		System.out.println(method.getName()+"执行之后做一些准备的工作");
		return result;
	}
	
}
```

cglib的测试代码：
```java
package cglib;

public class MainTest {
	public static void main(String[] args) {
		CglibTs ct = new CglibTs();
		ClassHasNoInterface chni = (ClassHasNoInterface) ct.getProxy(ClassHasNoInterface.class);
		chni.method();
		chni.function();
	}
}
```




为什么spring的DAO只使用接口就可以调用mybatis的xml文件
--------------------------
关键在于动态代理，而且使用的还是JDK动态代理。
mybatis会根据相应的接口声明，使用sqlsession.getMapper(xxxDao.class)方法，通过使用JDK动态代理生成一个DAO实例对象。当我们使用DAO接口的某一个方法时，mybatis会根据这个方法的方法名和参数类型，确定xml文件中的statement ID，最终调用sqlSession.select(statement ID,...)来执行sql。

mybatis的执行过程
------------
```c
                                应用程序
配置文件config.xml    →          Configuration
        ↑                           ↓
        ↑                           ↓
映射文件sqlMapConfig.xml        SqlSessionFactory
                                    ↓
                                SqlSession      →  Mapped Statement
                                    ↓
                                transaction
```             

什么是restful架构？
-------------

 1. 每一个URI代表一种资源
 2. customer和server之间，传递这种资源的某种表现层；
 3. 客户端通过4个http动词，对服务器资源进行操作
 

http的get、post、delete、put
------------------------

 1. get的后退按钮无害，post的后退，数据会被重新提交；
 2. get的编码类型application/x-www-form-uri，post编码类型encodedapplication/x-www-form-urlencoded或multipart/form-data
 3. get历史数据保留在浏览器历史中，post的则不会保存
 4. get对数据长度有限制，一般为1-2KB，只允许ASCII码，post没有限制
 5. get安全性较差，不适合发送密码等数据，post更安全
 6. get/postbenzhix本质上都是TCP链接，get产生一个TCP数据包，post产生两个TCP数据包。get请求，浏览器会把http header data一并发送出去，响应为200。post请求，浏览器先发送header，服务器响应100后，浏览器继续发送data，然后返回200.
 

hash索引和BTree索引和位图索引
--------------
 

 - **hash索引**
**hash索引的底层数据结构：**
hash索引底层使用的是hashmap来存储数据，一般来讲hashmap节点会存储key、value、hash(key)、next指针（预防hash冲突）。
表的索引字段将作为key，使用hash(key)确认存储位置，表的**行**数据实际存储的物理地址作为value，如果出现hash冲突那么就使用next指针以链表的形式进行存储（这是我的**个人推断**，目前暂时没有在其他地方明确看到hash索引怎么存储表数据的）。
  1. 大量不同数据等值精确查询，hash索引更快；
 2. hash索引不支持复合索引的最左匹配规则，即(where a=? and b=?）,index(a,b,c)相当于范围查找，不走索引（**因为hash索引需要对key进行hash**，如果复合索引底层是hashmap的数据结构，复合索引的索引字段不写全是无法计算hash值的）。
 3. hash索引不支持排序，hash索引不支持模糊查找（**因为hash索引需要对key进行hash**，模糊以后是无法计算hash值的）
 

| 存储引擎        | 索引类型   |
| --------   | -----:  |
| InnoDB     | BTree、hash |
| MyISAM        |   BTree   |
| memory/heap        |    BTree、hash    |
| NDB        |    Hash、BTree    |


InnoDB存储引擎支持的hash index是自适应的，InnoDB会根据表的使用情况，自动为表生成索引，不能人为干预是否在一张表中生成hash index。
 - **BTree索引**
 参考网址https://www.cnblogs.com/tgycoder/p/5410057.html，
https://dev.mysql.com/doc/refman/5.7/en/innodb-index-types.html
mysql的Btree索引在MyISAM和InnoDB两个存储引擎中实现的方式是不一样的，主要体现在Btree**存储的数据**不一样。
下面照抄博客的内容：
MyISAM引擎使用B+Tree作为索引结构，叶结点的data域存放的是数据记录的地址。下面是MyISAM索引的原理图：

![此处输入图片的描述][7]

从图中可以看出主索引的B+Tree的key存储的索引列的值，而value部分存储的是行数据在磁盘上的物理地址。
辅助索引与主索引的结构类似，value部分也是保存物理地址。
示意图（以表的第二列为索引）如下所示：

![此处输入图片的描述][8]

MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。

下面介绍InnoDB存储引擎的B+Tree索引实现。
虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。

第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶结点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引(上面这段话摘自博客，起初我也这段话的正确性表示怀疑，直到看了mysql的官方reference)。
https://dev.mysql.com/doc/refman/5.7/en/innodb-index-types.html，

![此处输入图片的描述][9]

主索引存储方式：key是索引列的值，value是数据库表的该行除索引以外的其他数据。这也叫**聚集索引**。

![此处输入图片的描述][10]

注意，此时的副索引存储的key依然存储的是索引列的值，value存储的是主索引的key，而不再是数据。
所以，如果主索引的key过场，那么相应的副索引的存储空间就会上升，索引我们一般都要求主索引的字段长度尽量短。

 - **位图索引**
参考链接http://blog.csdn.net/zhou920786312/article/details/72790171，https://www.cnblogs.com/LBSer/p/3322630.html

首先，**mysql是不支持位图索引的**。oracle才支持。如果强行创建，那么会得到如下错误提示：
```sql
mysql> create bitmap index idx_gender_city on student(gender,city);

ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'bitmap index idx_gender_city on student(gender,city)' at line 1
mysql> 

```

位图索引需要注意几点：
1、在表中**放置单独的位图索引是没有意义的**，只有**多个列建立位图索引**，系统才能有效的利用位图索引提高查询速度. 
2、因为位图索引不能是唯一索引，也**不能对其进行键压缩** 
3、位图索引的作用源于与其他位图索引的结合，当位图索引的多个列进行查询时，oracle对这些上的位图索引进行布尔and和or运行，最终返回结果.

下面借博客的内容举个例子：

![此处输入图片的描述][11]

假设有这样一张表，对于性别和婚姻状况这两列，它的状态是有限的，我们可以针对这两列建立位图索引。
对于性别列建立位图索引：

![此处输入图片的描述][12]

针对婚姻状况列建立位图索引：

![此处输入图片的描述][13]

说明一下，对于状况对于2种的情况，比如婚姻状况，它是需要为每一种情况都建立位图索引的。
然后下图展示是匹配的过程，使用查询语句“select * from table where Gender=‘男’ and Marital=“未婚”;”的时候 首先取出男向量10100...，然后取出未婚向量00100...，将两个向量做and操作，这时生成新向量00100...，可以发现第三位为1，表示该表的**第三行**数据就是我们需要查询的结果。 

![此处输入图片的描述][14]

为什么mysql使用B+Tree而不是Btree来实现索引？
------------------------------
参考链接：http://blog.csdn.net/mr253727942/article/details/50813283

 - 为什么使用Btree而不是普通的二叉排序树
 
 因为Btree的深度更浅，单次数据的查找更快。减少了磁盘的IO。

 - 为什么使用B+tree而不是Btree
 - 
B-tree索引存储的是key-data形式，而B+tree存储的key形式，没有data。 

1.B+-tree内部节点没有存储具体data，所以一个盘块中可以存储更多的指针，依次指向子节点。把同一节点的data放在同一盘块中，能降低IO读写次数。

2.B+-tree查询效率更加稳定，由于任何关键字查找必须走从根节点到叶子节点，效率会稳定。

3.B+-tree的叶子节点是链表有序，在做范围查询的时候速度非常快。 例如上图中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点就可以。


String类的hashcode()是怎么实现的？
-------------------------
先上JDK8的源码：
```java
   /**
     * Returns a hash code for this string. The hash code for a
     * {@code String} object is computed as
     * <blockquote><pre>
     * s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]
     * </pre></blockquote>
     * using {@code int} arithmetic, where {@code s[i]} is the
     * <i>i</i>th character of the string, {@code n} is the length of
     * the string, and {@code ^} indicates exponentiation.
     * (The hash value of the empty string is zero.)
     *
     * @return  a hash code value for this object.
     */
    public int hashCode() {
        int h = hash;
        if (h == 0 && value.length > 0) {
            char val[] = value;

            for (int i = 0; i < value.length; i++) {
                h = 31 * h + val[i];
            }
            hash = h;
        }
        return h;
    }
```
其实计算公式就是：
s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]
至于为什么选31呢？
是因为：
31可以表示成11111，If it were even and the multiplication overflowed, information would be lost, as multiplication by 2 is equivalent to shifting。
另外一个原因是： 31的乘法可以由i*31== (i<<5)-i来表示（也就是32×i-i），现在很多虚拟机里面都有做相关优化，使用31的原因可能是为了更好的分配hash地址，并且31只占用5bits！

spring的AOP使用了什么设计模式
-------------------
spring的AOP使用了代理模式和策略模式（策略是指使用JDK动态代理hai）。

spring的filter使用了什么设计模式
----------------------
参考链接http://www.flyne.org/article/693
使用的是责任链设计模式。

drools中的有状态session和无状态的session
------------------------------

 - 无状态的session（stateless Kie Session）

可以理解成一个函数。但是又有一些细微的差别，它的输入参数的限定比较宽松，多次调用session产生的结果之间相互不会影响。
其实，StatelessKieSession是StatefulKieSession的子类。无状态session规则的匹配通过execute()一个函数就可以实现。技术上来说，无状态的session完全可以使用有状态的session来进行替代。但是无状态session更加强调规则的匹配是one-shot evaluation。
无状态session适合于**数据验证，计算（比如风险评估、按揭利率）、数据过滤、消息路由**等。
无状态session不需要进行dispose。after  each invocation of any of the execute() methods，the resources used for thr executtion are freed。 at this point，the same stateless kie session is ready for another execution round if required。each executio() invocation will then be independent。

 - 有状态session（stateful Kie Session）
drools默认的session类型就是有状态的。有状态session与无状态session的一个显著区别就是，我们不用等到全部输入参数到达才进行规则匹配，可以在一部分参数到达以后就进行计算，另外一部分参数到达以后继续进行运算，两次运算看成实在一个session中完成的，相互之间具有关联关系。
有状态session使用完成以后必须进行释放。
有状态session的运算需要先使用insert函数插入fact，然后调用fireAllRules方法进行规则运算。

nginx负载均衡
---------
参考链接：http://blog.csdn.net/tjcyjd/article/details/50695922
nginx负载均衡有5种配置方式：

 1. 轮询（默认）
 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。
 
 2. weight
指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。
例如：
upstream bakend {
server 192.168.0.14 weight=10;
server 192.168.0.15 weight=90;
}
 3. ip_hash
每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。
例如：
upstream bakend {
ip_hash;
server 192.168.0.14:88;
server 192.168.0.15:80;
}
 4. url_hash（第三方）
 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。
 例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法
upstream backend {
server squid1:3128;
server squid2:3128;
hash $request_uri;
hash_method crc32;
}
 5. fair（第三方）
按后端服务器的响应时间来分配请求，响应时间短的优先分配。
upstream backend {
server server1;
server server2;
fair;
}

Java锁的类型
--------
参考
 1. 公平锁/非公平锁
 公平锁是指多个线程按照申请锁的顺序来获取锁。
 非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。
 对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。
对于Synchronized而言，也是一种非公平锁，由于其并不像ReentrantLock是通过AQS（AbstractQueuedSynchronized）的来实现线程调度，所以并没有任何办法使其变成公平锁。
 2. 可重入锁
 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。说的有点抽象，下面会有一个代码的示例。
 对于Java ReentrantLock而言, 他的名字就可以看出是一个可重入锁，其名字是Re entrant Lock重新进入锁。
对于Synchronized而言,也是一个可重入锁。可重入锁的一个好处是可一定程度避免死锁。
```java
synchronized void setA() throws Exception{
    Thread.sleep(1000);
    setB();
}

synchronized void setB() throws Exception{
    Thread.sleep(1000);
}
```
上面的代码就是一个可重入锁的一个特点，如果不是可重入锁的话，setB可能不会被当前线程执行，可能造成死锁。
 1. 独享锁/共享锁
 独享锁是指该锁一次只能被一个线程所持有。
共享锁是指该锁可被多个线程所持有。
对于Java ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。
读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。
独享锁与共享锁也是通过AQS（AbstractQueuedSynchronized）来实现的，通过实现不同的方法，来实现独享或者共享。
对于Synchronized而言，当然是独享锁。
 2. 互斥锁/读写锁
 上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。
互斥锁在Java中的具体实现就是ReentrantLock
读写锁在Java中的具体实现就是ReadWriteLock
 3. 乐观锁/悲观锁
 乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。
悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。
乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。
从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。
悲观锁在Java中的使用，就是利用各种锁。
乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。
 4. 分段锁
 分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。
我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7与JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是一个ReentrantLock（Segment继承了ReentrantLock)。
当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。
但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。
分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。
 5. 自旋锁
 在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。
 

Java中的AQS
---------
https://www.cnblogs.com/waterystone/p/4920797.html
谈到并发，不得不谈ReentrantLock；而谈到ReentrantLock，不得不谈AbstractQueuedSynchronized（AQS）！
它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。如下图所示：
![此处输入图片的描述][15]


  

java实现冒泡排序和快速排序
---------------
```java
package com.audi.mySort;

public class Sort
{
	public static void bubbleSort(int a[])
	{
		if (a == null|| a.length==0 ||a.length==1)
		{
			return;
		}
		int temp=0;
		for (int i = 0; i < a.length-1; i++)
		{
			for (int j = 0; j < a.length-i-1; j++)
			{
				if (a[j]>a[j+1])
				{
					temp=a[j];
					a[j]=a[j+1];
					a[j+1]=temp;
				}
			}
		}
	}

	//看不懂的时候看一下这个http://developer.51cto.com/art/201403/430986.htm
	public static void quickSort(int a[],int low,int high)
	{
		if (a==null|| a.length==0 ||a.length==1)
		{
			return;
		}
		int i,j,pivoit,temp;
		if (low<high)
		{
			pivoit = a[low];
			i=low;
			j=high;
			while(i<j)
			{
				while(i<j&&a[j]>=pivoit)
					j--;
				while(i<j&&a[i]<=pivoit)
					i++;
				if (i<j)
				{
					temp=a[i];
					a[i]=a[j];
					a[j]=temp;
				}
			}
			a[low]=a[i];
			a[i]=pivoit;
			quickSort(a, low, i-1);
			quickSort(a, i+1, high);
		}
	}
	
	public static void main(String[] args)
	{
		int a[]= {1,34,0,21,342,2,90};
		System.out.println("冒泡排序：");
		Sort.bubbleSort(a);
		for(int i:a)
		{
			System.out.println(i);
		}
		int b[]= {1,34,0,21,342,2,90};
		System.out.println("快速排序：");
		Sort.quickSort(b, 0, b.length-1);
		for(int i:b)
		{
			System.out.println(i);
		}
	}
}

```
为什么快速排序都要先从右边开始？
参考链接http://blog.csdn.net/w282529350/article/details/50982650
考虑数组6 1 2 7 9
6在左，9在右  我们将6作为基数。
假设从左边开始（与正确程序正好相反）
于是i 就会移动到现在的 数字 7 那个位置停下来，而  j 原来在 数字 9 那个位置 ，因为
while(arr[j]>=temp&&i<j)
于是，j 也会停留在数字7 那个位置，于是问题来了。当你最后交换基数6与7时，不对呀！！。
问题在于当我们先从在边开始时，那么 i 所停留的那个位置肯定是大于基数6的，而在上述例子中，为了满足 i<j 于是 j也停留在7的位置
但最后交换回去的时候，7就到了左边，不行，因为我们原本 交换后数字6在边应该是全部小于6，右边全部大于6.但现在不行了。
于是，我们必须从右边开始，也就是从基数的对面开始。

java中基本类型变量存在哪里
---------------
参考链接https://www.cnblogs.com/panxuejun/p/5970739.html
基本数据类型是放在栈中还是放在堆中，这取决于基本类型声明的位置。

 

 一：在方法中声明的变量，即该变量是局部变量，每当程序调用方法时，系统都会为该方法建立一个方法栈，其所在方法中声明的变量就放在方法栈中，当方法结束系统会释放方法栈，其对应在该方法中声明的变量随着栈的销毁而结束，这就局部变量只能在方法中有效的原因

      在方法中声明的变量可以是基本类型的变量，也可以是引用类型的变量。

         （1）当声明是基本类型的变量的时，其变量名及值（变量名及值是两个概念）是放在方法栈中

         （2）当声明的是引用变量时，所声明的变量（该变量实际上是在方法中存储的是内存地址值）是放在方法的栈中，该变量所指向的对象是放在堆类存中的。

   二：在类中声明的变量是成员变量，也叫全局变量，放在堆中的（因为全局变量不会随着某个方法执行结束而销毁）。

       同样在类中声明的变量即可是基本类型的变量 也可是引用类型的变量

       （1）当声明的是基本类型的变量其变量名及其值放在堆内存中的

       （2）引用类型时，其声明的变量仍然会存储一个内存地址值，该内存地址值指向所引用的对象。引用变量名和对应的对象仍然存储在相应的堆中
在JVM中，主内存究竟在哪里
--------------
虚拟机栈是为每个线程分配的，堆是公用的。
多线程的时候，并发引起的问题就是线程从主内存读取数据自己做一个copy，以后操作的都是这个copy，操作完以后才会写会主内存。
那么这个主内存是否可以认为是堆里保存的数据，而线程copy以后的数据是保存在线程自己的虚拟机栈里呢？

个人理解2017年12月7日10:57:20
所谓主内存  只是对于线程来说的
至于主内存可以是堆、方法区
主内存是堆：类中声明的属性，多线程时，主内存就是堆
主内存是方法区：类的string属性，多线程时，主内存就是方法区
![主内存和工作内存][16]


  

CMS和G1
------
参考链接http://blog.csdn.net/linhu007/article/details/48897597
http://www.linuxidc.com/Linux/2015-01/112092.htm
并行：并行, 多个线程各做各的事情(互相间无共享状态)
并发：并发, 多个线程协同做同一件事情(有状态)
 1. CMS收集器

 CMS(Concurrent Mark Sweep)并发收集器是一种以获取最短回收停顿时间为目标的收集器。基于“标记-清除”算法实现，它的运作过程如下（具体分为6个步骤）：
 
初始标记（CMS initial mark）

并发标记（CMS concurrent mark）

并发预清理（CMS-concurrent-preclean）

重新标记（CMS remark）

并发清除（CMS concurrent sweep）

并发重置（CMS-concurrent-reset）
示例代码：
```java
package hello;

import java.util.ArrayList;

import java.util.List;

/**
 * 
 * 简单的JAVA虚拟机内存回收,cms收集器的使用
 * 
 * 参数：-Xms30m -Xmx60m-Xmn10m -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails
 * 
 * @author 范芳铭
 */

public class EasyCMS
{

	public byte[] placeHolder = new byte[64 * 1024]; // 占位符

	public static void main(String[] args) throws Exception
	{
		outOfMemoryByExpansionSize();
	}

	private static void outOfMemoryByExpansionSize() throws Exception
	{
		List<EasyCMS> list = new ArrayList<EasyCMS>();
		while (true)
		{
			EasyCMS serial = new EasyCMS();
			list.add(serial);
			System.out.println("=========="+System.currentTimeMillis());
			Thread.sleep(10);// 停顿10毫秒
		}
	}
}

```
其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。其他动作都是并发的。

 1、 G1收集器
参考文档：http://blog.csdn.net/renfufei/article/details/41897113
G1 (Garbage-First)是一款**面向服务器**的垃圾收集器,主要针对配备**多颗处理器**及**大容量内存**的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征. 在Oracle JDK 7 update 4 及以上版本中得到完全支持, 专为以下应用程序设计:
 - 可以像CMS收集器一样,GC操作与应用的线程一起并发执行
 - 紧凑的空闲内存区间且没有很长的GC停顿时间.
 - 需要可预测的GC暂停耗时.
 - 不想牺牲太多吞吐量性能.
 - 启动后不需要请求更大的Java堆.
G1的长期目标是取代CMS(Concurrent Mark-Sweep Collector, 并发标记-清除). 因为特性的不同使G1成为比CMS更好的解决方案. 一个区别是,G1是一款压缩型的收集器.G1通过有效的压缩完全避免了对细微空闲内存空间的分配,不用依赖于regions，这不仅大大简化了收集器，而且还消除了潜在的内存碎片问题。除压缩以外，G1的垃圾收集停顿也比CMS容易估计，也允许用户自定义所希望的停顿参数(pause targets)

 更详细的可以参见https://github.com/WQZ321123/learn/blob/master/G1/G1.md，转自https://github.com/cncounter/translation/blob/master/tiemao_2014/G1/G1.md


jdk1.8舍去了永久区
------------
参考链接https://www.cnblogs.com/paddix/p/5309550.html
和https://www.cnblogs.com/hadoop-dev/p/7169252.html
一般，我们说常量池存在方法区，方法区又存在永久区（Perm Space）（方法区是虚拟机规范，永久区是虚拟机的规范的具体实现，要区分开，不能说方法区就是永久区）。但是这是有条件的，条件就是：

 - JDK版本必须要在1.6及更低版本。
 
 - 针对HotSpot虚拟机而言，因为其他虚拟机没有永久区的概念。

从JDK1.7开始，就已经提出了要将perm去掉的想法，只是没有**完全**实现。之所以说没有完全实现，是因为perm在JDK1.7并没有被去除，但是JDK1.7及之后版本的JVM已经将运行时常量池从方法区中移了出来，在Java 堆（Heap）中开辟了一块区域存放运行时常量池（对于这段话的java代码验证可以参考https://www.cnblogs.com/paddix/p/5309550.html）。
JDK1.8已经完全抛弃了perm区，取而代之的是元数据区，且存在虚拟机之外的本地内存上，如下图所示：
![此处输入图片的描述][17]


String.intern方法
---------------
String.intern()是一个Native方法，它的作用是：

 - JDK1.6：如果运行时常量池中已经包含一个等于此String对象内容的字符串，则返回常量池中该字符串的引用；如果没有，则在常量池中创建与此String内容相同的字符串，并返回常量池中创建的字符串的引用。
 
 - JDK1.7:当常量池中没有该字符串时，JDK7的intern（）方法的实现不再是在常量池中创建与此String内容相同的字符串，而改为在常量池中记录java Heap中首次出现的该字符串的引用，并返回该引用。
 

java的finalize方法
---------------
参考链接：http://blog.csdn.net/pi9nc/article/details/12374049
finalize()方法属于Object类的一个protected方法，源码如下：
```java
   /**
     * Called by the garbage collector on an object when garbage collection
     * determines that there are no more references to the object.
     * A subclass overrides the {@code finalize} method to dispose of
     * system resources or to perform other cleanup.
     * <p>
     * The general contract of {@code finalize} is that it is invoked
     * if and when the Java&trade; virtual
     * machine has determined that there is no longer any
     * means by which this object can be accessed by any thread that has
     * not yet died, except as a result of an action taken by the
     * finalization of some other object or class which is ready to be
     * finalized. The {@code finalize} method may take any action, including
     * making this object available again to other threads; the usual purpose
     * of {@code finalize}, however, is to perform cleanup actions before
     * the object is irrevocably discarded. For example, the finalize method
     * for an object that represents an input/output connection might perform
     * explicit I/O transactions to break the connection before the object is
     * permanently discarded.
     * <p>
     * The {@code finalize} method of class {@code Object} performs no
     * special action; it simply returns normally. Subclasses of
     * {@code Object} may override this definition.
     * <p>
     * The Java programming language does not guarantee which thread will
     * invoke the {@code finalize} method for any given object. It is
     * guaranteed, however, that the thread that invokes finalize will not
     * be holding any user-visible synchronization locks when finalize is
     * invoked. If an uncaught exception is thrown by the finalize method,
     * the exception is ignored and finalization of that object terminates.
     * <p>
     * After the {@code finalize} method has been invoked for an object, no
     * further action is taken until the Java virtual machine has again
     * determined that there is no longer any means by which this object can
     * be accessed by any thread that has not yet died, including possible
     * actions by other objects or classes which are ready to be finalized,
     * at which point the object may be discarded.
     * <p>
     * The {@code finalize} method is never invoked more than once by a Java
     * virtual machine for any given object.
     * <p>
     * Any exception thrown by the {@code finalize} method causes
     * the finalization of this object to be halted, but is otherwise
     * ignored.
     *
     * @throws Throwable the {@code Exception} raised by this method
     * @see java.lang.ref.WeakReference
     * @see java.lang.ref.PhantomReference
     * @jls 12.6 Finalization of Class Instances
     */
    protected void finalize() throws Throwable { }
```
大致完成的功能是，在一个对象被虚拟机确认为不可达状态以后，**可能**会执行这个对象的finalize方法来进行一些资源的回收甚至对象的复活工作。
方法声明为protected类型的，是防止在该类之外定义的代码访问finalize()方法。

self-introduction
-----------------
Hello, my name is Wang Quanzhou.
Previously, I have done several projects about java web development, like loan mortgage and commission settlement system development. The main skill we use are java, spring, oracle, mybatis. Except these, I love sports, such as ridding and fitness. 
Give me a chance, I will give you the surprise.
Thank you.

Java中的Error能不能被Catch
--------------------
Error是一个**类**，它的父类（**注意**，是个类，不是接口）是Throwable，它的兄弟**类**是Exception，如下图所示：
![此处输入图片的描述][18]


  那么error到底可不可以被catch呢？
  参考http://blog.csdn.net/sunshinestation/article/details/4169763，实验结果表明是可以被catch的。
```java
package com.audi;

public class ExceptionDemo
{
	/**
	 * @param args
	 */
	public static void main(String[] args)
	{
		try
		{
			throw new MyError("My Error");
		} catch (MyError e)
		{
			System.out.println("Catch Error");
			System.out.println(e.getMessage());
		}
	}
}

class MyError extends Error
{

	private static final long serialVersionUID = 1L;

	public MyError()
	{
		super();
	}

	public MyError(String message, Throwable cause)
	{
		super(message, cause);
	}

	public MyError(String message)
	{
		super(message);
	}

	public MyError(Throwable cause)
	{
		super(cause);
	}

}
```
运行结果：
![此处输入图片的描述][19]


HBase中WAL(Write-Ahead-Log)
--------------------------
参考链接:http://blog.csdn.net/u010916254/article/details/48025445，http://blog.csdn.net/zdc524/article/details/49994589

通俗的讲：WAL就是在数据写入数据库的时候，先将数据的日志写入磁盘文件，再将数据写入实际的数据库表。（例如HBase、postgresql等支持WAL）


MYSQL的BIN log
--------------

hadoop简介
--------
Hadoop核心项目:

　　　　HDFS:Hadoop Distributed File System分布式文件系统,用来管理文件的.在hdfs上存储的数据是分散很多服务器之上的,但是用户感觉不到,文件真的分布在很多台机器上,就像一台机器上似的.

　　　　MapReduce:分布式并行计算框架.实现的是分布式计算,大数据分布在很多台服务器上,需要它去并行地去执行

　　　　Map:在每个分散的机器上进行计算的那部分.Reduce:主要做最后的一个汇总

 - HDFS架构:
 hdfs和MapReduce都是主从结构.管理与被管理这种关系,分为管理者和被管理者.被管理者通常做具体的事物的,管理者通常是组织,协调,管理工作的.

　　　　　　节点:网络环境中的每一台服务器.

　　　　主节点:只有一个**NameNode**,负责各个节点数据的组织管理,

　　　　从节点:有很多个**DataNode**,负责存储数据,数据节点

　　　　　NameNode对外,DataNode对内,NameNode接收用户的操作请求,NameNode负责协调管理,不是真正的存放数据,会把数据分散到各个节点上去存储

　　海量数据是单节点处理不了的,所以我们的数据需要存放在多台服务器上,作为管理的NamNode知道数据具体存放在DataNode的哪些节点上面
　　
　　

 - NameNode如何知道数据存放在DataNode节点的位置的呢?
 NameNode对外暴露的就是目录的文件系统

　　　　用户要进行hdfs操作的时候,首先和NameNode打交道,NameNode上边有一个文件系统的目录结构,用户通过看文件系统的目录结构,就知道我们的数据是存放在那个路径下面,文件叫什么名字,文件的路径,文件有多大,我们的数据具体存放在那些节点上,客户是不需要关心的

　　　　NamNode负责:接收用户操作请求,是用户操作的入口.维护文件系统的目录结构,称作命名空间.

　　　　DataNode负责:存储文件数据，dataNode的数据一般都会做冗余，一般冗余为3，即除了自己以外，在同集群还有一个副本DataNode，在另外的集群还有一个副本DataNode。

 - MapReduce架构:
 主节点执行一个管理者的角色,从节点执行一个被管理者的角色.管理和被管理完成数据的一个计算(任何对数据的处理都叫做计算,查询,过滤,数据的检索..利用cpu和内存进行数据处理).

　　　　主节点只有一个:JobTracker,

　　　　把我们用户的操作请求,拿过来,分发给TaskTracker,接收用户提交的计算任务,把计算任务分配到TaskTracker去执行,监控TaskTracker的执行情况

　　　　从节点有很多个:TaskTracker,

　　　　是我们自己安装部署的,通常TaskTracker和DataNode,执行用户的操作,运行时根据TaskTracker上DataNode的数据只执行一部分,执行程序时,去找DataNode本地的数据,然后加载DataNode上边的数据,去运行

　　　　MapReduce进行计算时,处理的数据就是用户提交的这些数据

　　　　TaskTracker通过反射将我们的程序读进内存中,然后在jvm中运行,程序在含有数据的DataNode的节点上运行

　　　　TaskTracker负责用户提交的计算任务

　　　　节点的数量越多,整体的计算时间越短,JobTracker管理执行任务的TaskTracker

　　　　NameNode和DataNode负责完成数据存储

　　　　JobTracker和TaskTracker完成数据的计算

　　　　NameNode和JobTracker不一定非要在同一台机器上,在生产中,通常是分开的,因为用户的请求,NameNode也接收,JobTracker也接收,为了防止NameNode操作慢,所以NameNode　　最好是一台机器,充分利用cpu和内存,JobTracker也是一台机器,都是独立的

　　　　**DataNode和TaskTracker通常是同一台机器**,是因为TaskTracker在运行的时候,可以执行本地的数据,如果不在一起,就要经过网络传播(网络一不稳定,二耗时) DataNode只管理本地,　　不管理远程

　　　　JobTracker和TaskTracker不从HDFS上读数据一样可以去做事情

　　　　用户存储数据首先和NameNode打交道,用户的数据直接和DataNode打交道,绕过了NameNode,就是说用户在进行存储的时候,去问NameNode我要去哪里读写数据,一旦用户知道了,　　就没有NameNode的事了,直接去DataNode那去处理了.假设用户处理数据一定经过NameNode,那么两三个用户上来之后,NameNode内存几乎全爆了,因为是海量数据,内存肯定是装不下　　的.只是向NameNode申请block块和blockId

　　　　架构的设计是让数据传输的时候不经过NameNode,所以架构没有瓶颈

storm简介
-------
Storm是一个分布式的、高容错的实时计算系统。
Storm对于实时计算的的意义相当于Hadoop对于批处理的意义。Hadoop为我们提供了Map和Reduce原语，使我们对数据进行批处理变的非常的简单和优美。同样，Storm也对数据的实时计算提供了简单Spout和Bolt原语。
Storm适用的场景：
1、流数据处理：Storm可以用来用来处理源源不断的消息，并将处理之后的结果保存到持久化介质中。
2、分布式RPC：由于Storm的处理组件都是分布式的，而且处理延迟都极低，所以可以Storm可以做为一个通用的分布式RPC框架来使用。
在这个教程里面我们将学习如何创建Topologies, 并且把topologies部署到storm的集群里面去。Java将是我们主要的示范语言， 个别例子会使用python以演示storm的多语言特性。

 - 一个Storm集群的基本组件
 storm的集群表面上看和hadoop的集群非常像。但是在Hadoop上面你运行的是MapReduce的Job, 而在Storm上面你运行的是Topology。它们是非常不一样的 — 一个关键的区别是： 一个MapReduce Job最终会结束， 而一个Topology运永远运行（除非你显式的杀掉他）。
在Storm的集群里面有两种节点： 控制节点(master node)和工作节点(worker node)。控制节点上面运行一个后台程序：Nimbus， 它的作用类似Hadoop里面的JobTracker。Nimbus负责在集群里面分布代码，分配工作给机器， 并且监控状态。
每一个工作节点上面运行一个叫做Supervisor的节点（类似 TaskTracker）。Supervisor会监听分配给它那台机器的工作，根据需要 启动/关闭工作进程。每一个工作进程执行一个Topology（类似 Job）的一个子集；一个运行的Topology由运行在很多机器上的很多工作进程 Worker（类似 Child）组成。

![此处输入图片的描述][20]


  ![此处输入图片的描述][21]


分布式事务一致性
--------
参考链接：https://www.cnblogs.com/dinglang/p/5679542.html，
http://blog.csdn.net/zheng0518/article/details/51194942

分布式系统中进行事务一致性控制，其实是比较难以实现的。根据CAP（consistency，available，partition tolerance）理论，三者同时满足是不可能的。

 - 强一致性：当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值。这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。根据CAP 理论，这种实现需要牺牲可用性。
 - 弱一致性：系统并不保证续进程或者线程的访问都会返回最新的更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。
 - 最终一致性：弱一致性的特定形式。系统保证在没有后续更新的前提下，系统最终返回上一次更新操作的值。在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。DNS
   是一个典型的最终一致性系统。

在互联网领域的绝大多数的场景，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。

要实现分布式系统的事务一致性，有一下一些方法：

 1. 串行化
 业务整合方案主要采用将接口整合到本地执行的方法。拿问题场景来说，则可以将服务 A、B、C 整合为一个服务 D 给业务，这个服务 D 再通过转换为本地事务的方式，比如服务 D 包含本地服务和服务 E，而服务 E 是本地服务 A ~ C 的整合。
优点：解决（规避）了分布式事务。
缺点：显而易见，把本来规划拆分好的业务，又耦合到了一起，业务职责不清晰，不利于维护。
由于这个方法存在明显缺点，通常不建议使用。
 2. 本地消息表
这种实现方式的思路，其实是源于ebay，后来通过支付宝等公司的布道，在业内广泛使用。其基本的设计思想是将远程分布式事务拆分成一系列的本地事务。如果不考虑性能及设计优雅，借助关系型数据库中的表即可实现。

举个经典的**跨行转账**的例子来描述。

第一步伪代码如下，扣款1W，通过本地事务保证了凭证消息插入到消息表中。
![此处输入图片的描述][22]


  第二步，通知对方银行账户上加1W了。那问题来了，如何通知到对方呢？

通常采用两种方式：

采用时效性高的MQ，由对方订阅消息并监听，有消息时自动触发事件
采用定时轮询扫描的方式，去检查消息表的数据。
两种方式其实各有利弊，仅仅依靠MQ，可能会出现通知失败的问题。而过于频繁的定时轮询，效率也不是最佳的（90%是无用功）。所以，我们一般会把两种方式结合起来使用。

解决了通知的问题，又有新的问题了。万一这消息有重复被消费，往用户帐号上多加了钱，那岂不是后果很严重？

仔细思考，其实我们可以消息消费方，也通过一个“消费状态表”来记录消费状态。在执行“加款”操作之前，检测下该消息（提供标识）是否已经消费过，消费完成后，通过本地事务控制来更新这个“消费状态表”。这样子就避免重复消费的问题。

总结：上诉的方式是一种非常经典的实现，基本避免了分布式事务，实现了“最终一致性”。但是，关系型数据库的吞吐量和性能方面存在瓶颈（因为本地消息表和消费消息表是数据库表，oracle或者mysql的表），频繁的读写消息会给数据库造成压力。所以，在真正的高并发场景下，该方案也会有瓶颈和限制的。

 1. MQ（非事务消息）
 所谓**非事务**，就是说，消息的生产者和消费者是不具备事务关系，即消费者执行失败时，生产者是不会产生回滚的。
 通常情况下，在使用非事务消息支持的MQ产品时，我们很难将业务操作与对MQ的操作放在一个本地事务域中管理。通俗点描述，还是以上述提到的“跨行转账”为例，我们很难保证在扣款完成之后对MQ投递消息的操作就一定能成功。这样一致性似乎很难保证。

先从消息生产者这端来分析，请看伪代码：

![此处输入图片的描述][23]


  根据上述代码及注释，我们来分析下可能的情况：

1、操作数据库成功，向MQ中投递消息也成功，皆大欢喜
2、操作数据库失败，不会向MQ中投递消息了
3、操作数据库成功，但是向MQ中投递消息时失败，向外抛出了异常，刚刚执行的更新数据库的操作将被回滚
从上面分析的几种情况来看，貌似问题都不大的。那么我们来分析下消费者端面临的问题：

消息出列后，消费者对应的业务操作要执行成功。如果业务执行失败，消息不能失效或者丢失。需要保证消息与业务操作一致
尽量避免消息重复消费。如果重复消费，也不能因此影响业务结果
如何保证消息与业务操作一致，不丢失？

主流的MQ产品都具有持久化消息的功能。如果消费者宕机或者消费失败，都可以执行重试机制的（有些MQ可以自定义重试次数）。

如何避免消息被重复消费造成的问题？

保证消费者调用业务的服务接口的幂等性
通过消费日志或者类似状态表来记录消费状态，便于判断（建议在业务上自行实现，而不依赖MQ产品提供该特性）
 

总结：这种方式比较常见，性能和吞吐量是优于使用关系型数据库消息表的方案。如果MQ自身和业务都具有高可用性，理论上是可以满足大部分的业务场景的。不过在没有充分测试的情况下，不建议在交易业务中直接使用。

 1. MQ（事务消息）
 举个例子，Bob向Smith转账，那我们到底是先发送消息，还是先执行扣款操作？

好像都可能会出问题。如果先发消息，扣款操作失败，那么Smith的账户里面会多出一笔钱。反过来，如果先执行扣款操作，后发送消息，那有可能扣款成功了但是消息没发出去，Smith收不到钱。除了上面介绍的通过异常捕获和回滚的方式外，还有没有其他的思路呢？

下面以阿里巴巴的RocketMQ中间件为例，分析下其设计和实现思路。

RocketMQ第一阶段发送Prepared消息时，会拿到消息的地址，第二阶段执行本地事物，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。细心的读者可能又发现问题了，如果确认消息发送失败了怎么办？RocketMQ会定期扫描消息集群中的事物消息，这时候发现了Prepared消息，它会向消息发送者确认，Bob的钱到底是减了还是没减呢？如果减了是回滚还是继续发送确认消息呢？RocketMQ会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。如下图：、

![此处输入图片的描述][24]


  总结：据笔者的了解，各大知名的电商平台和互联网公司，几乎都是采用类似的设计思路来实现“最终一致性”的。这种方式适合的业务场景广泛，而且比较可靠。不过这种方式技术实现的难度比较大。目前主流的开源MQ（ActiveMQ、RabbitMQ、Kafka）均未实现对事务消息的支持，所以需二次开发或者新造轮子。比较遗憾的是，RocketMQ事务消息部分的代码也并未开源，需要自己去实现。
  
  
http://blog.jobbole.com/95632/

 - 两阶段提交协议
所谓两阶段，就是**准备阶段**和**执行阶段**。该协议一般有协调者和参与者两个角色。
准备阶段：
事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备，只欠东风”的状态。
提交阶段：
如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)
当协调者节点从所有参与者节点获得的相应消息都为”同意”时:

![此处输入图片的描述][25]


如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：

![此处输入图片的描述][26]

二阶段提交协议有如下一些缺点：
>1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

>2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）

>3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。

>4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。

 - 三阶段提交协议

三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。
与两阶段提交不同的是，三阶段提交有两个改动点:
>1、引入超时机制。同时在协调者和参与者中都引入超时机制。

>2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。
也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。

在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者abort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。）

 - 2PC与3PC的区别
 
 相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制**也会导致数据一致性问题**，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。


分布式锁
----
参考链接http://blog.csdn.net/zxp_cpinfo/article/details/53692922

目前几乎很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。分布式的CAP理论告诉我们“任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。”所以，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。

在很多场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。有的时候，我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，Java中其实提供了很多并发处理相关的API，但是这些API在分布式场景中就无能为力了。也就是说单纯的Java Api并不能提供分布式锁的能力。所以针对分布式锁的实现目前有多种方案。
针对分布式锁的实现，目前比较常用的有以下几种方案：

 - 基于数据库实现分布式锁
 - 基于缓存（redis，memcached，tair）实现分布式锁
 - 基于Zookeeper实现分布式锁
 在分析这几种实现方案之前我们先来想一下，我们需要的分布式锁应该是怎么样的？（这里以方法锁为例，资源锁同理）
 是这样的：
 可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。
这把锁要是一把可重入锁（避免死锁）
这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条）
有高可用的获取锁和释放锁功能
获取锁和释放锁的性能要好


**方案一**：基于数据库实现分布式锁
基于数据库表

要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。

当我们要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。
创建这样一张数据库表：
```sql
CREATE TABLE `methodLock` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
  `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名',
  `desc` varchar(1024) NOT NULL DEFAULT '备注信息',
  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的方法';
```
当我们想要锁住某个方法时，执行以下SQL：
```sql
insert into methodLock(method_name,desc) values (‘method_name’,‘desc’)
```
因为我们对method_name做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。

当方法执行完毕之后，想要释放锁的话，需要执行以下Sql:
```sql
delete from methodLock where method_name ='method_name'
```
>上面这种简单的实现有以下几个问题：
1、这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。
2、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。
3、这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。
4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。

**方案二**：基于数据库排他锁
除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。

我们还用刚刚创建的那张数据库表。可以通过数据库的排他锁来实现分布式锁。 基于MySql的InnoDB引擎，可以使用以下方法来实现加锁操作：
```java
public boolean lock(){
    connection.setAutoCommit(false)
    while(true){
        try{
            result = select * from methodLock where method_name=xxx for update;
            if(result！=null){
                return true;
            }
        }catch(Exception e){

        }
        sleep(1000);
    }
    return false;
}
```
在查询语句后面增加**for update**，数据库会在查询过程中给数据库表增加排他锁（这里再多提一句，InnoDB引擎在加锁的时候，**只有通过索引进行检索的时候才会使用行级锁**，否则会使用表级锁。这里**我们希望使用行级锁，就要给method_name添加索引**，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。）。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。

我们可以认为获得排它锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，再通过以下方法解锁：
```java
public void unlock(){
    connection.commit();
}
```
**方案三**：基于Zookeeper实现分布式锁
基于zookeeper临时有序节点可以实现的分布式锁。

大致思想即为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。
获取锁:
```java
public void lock(){
    path = 在父节点下创建临时顺序节点
    while(true){
        children = 获取父节点的所有节点
        if(path是children中的最小的){
            代表获取了节点
            return;
        }else{
            添加监控前一个节点是否存在的watcher
            wait();
        }
    }
}

watcher中的内容{
    notifyAll();
}
```
释放锁：
```java
public void release(){
    删除上述创建的节点
}
```
ZooKeeper版本的分布式锁问题相对比较来说少。

 - 锁的占用时间限制：
redis就有占用时间限制，而ZooKeeper则没有，最主要的原因是redis目前没有办法知道已经获取锁的客户端的状态，是已经挂了呢还是正在执行耗时较长的业务逻辑。而ZooKeeper通过临时节点就能清晰知道，如果临时节点存在说明还在执行业务逻辑，如果临时节点不存在说明已经执行完毕释放锁或者是挂了。由此看来redis如果能像ZooKeeper一样添加一些与客户端绑定的临时键，也是一大好事。
 - 是否单点故障：
redis本身有很多中玩法，如客户端一致性hash，服务器端sentinel方案或者cluster方案，很难做到一种分布式锁方式能应对所有这些方案。而ZooKeeper只有一种玩法，多台机器的节点数据是一致的，没有redis的那么多的麻烦因素要考虑。

总体上来说ZooKeeper实现分布式锁更加的简单，可靠性更高。

ZooKeeper实现分布式锁架构图：
![此处输入图片的描述][27]

其中，左边的整个区域表示一个Zookeeper集群，locker是Zookeeper的一个持久节点，node_1、node_2、node_3是locker这个持久节点下面的临时顺序节点。client_1、client_2、client_n表示多个客户端，Service表示需要互斥访问的共享资源。

客户端A要获取分布式锁的时候首先到locker下创建一个临时顺序节点（node_n），然后立即获取locker下的所有（一级）子节点。
此时因为会有多个客户端同一时间争取锁，因此locker下的子节点数量就会大于1。对于顺序节点，特点是节点名称后面自动有一个数字编号，
先创建的节点数字编号小于后创建的，因此可以将子节点按照节点名称后缀的数字顺序从小到大排序，这样排在第一位的就是最先创建的顺序节点，
此时它就代表了最先争取到锁的客户端！此时判断最小的这个节点是否为客户端A之前创建出来的node_n，如果是则表示客户端A获取到了锁，如果不是则表示锁已经被其它客户端获取，因此客户端A要等待它释放锁，也就是等待获取到锁的那个客户端B把自己创建的那个节点删除。
此时就通过监听比node_n次小的那个顺序节点的删除事件来知道客户端B是否已经释放了锁，如果是，此时客户端A再次获取locker下的所有子节点，
再次与自己创建的node_n节点对比，直到自己创建的node_n是locker的所有子节点中顺序号最小的，此时表示客户端A获取到了锁！

**方案四**：基于redis缓存实现分布式锁
首先是获取锁：
```java
public void lock(){
    for(){
        ret = setnx lock_ley (current_time + lock_timeout)
        if(ret){
            //获取到了锁
            break;
        }
        //没有获取到锁
        sleep(100);
    }
}
```
然后是释放锁：
```java
public void release(){
    del lock_ley
}
```
setnx来创建一个key，如果key不存在则创建成功返回1，如果key已经存在则返回0。依照上述来判定是否获取到了锁

获取到锁的执行业务逻辑，完毕后删除lock_key，来实现释放锁

其他未获取到锁的则进行**不断重试**，直到自己获取到了锁.

**方案五**：基于redis缓存实现分布式锁的改进版本
方案四在正常情况下是OK的，但是一旦获取到锁的客户端挂了，没有执行上述释放锁的操作，则其他客户端就无法获取到锁了，所以在这种情况下有2种方式来解决：

1、为lock_key设置一个过期时间
2、对lock_key的value进行判断是否过期
以第一种为例，在set键值的时候带上过期时间，即使挂了，也会在过期时间之后，其他客户端能够重新竞争获取锁
```java
public void lock(){
    while(true){
        ret = set lock_key identify_value nx ex lock_timeout
        if(ret){
            //获取到了锁
            return;
        }
        sleep(100);
    }
}

public void release(){
    value = get lock_key
    if(identify_value == value){
        del lock_key
    }
}
```
该方案的缺点是：过期时长不好设置，无法保证任务一定能在过期时间内完成。


java移位运算符
---------
java中有三种移位运算符

/<<      :     左移运算符，num << 1,相当于num乘以2

/>>      :     右移运算符，num >> 1,相当于num除以2

/>>>    :     无符号右移，忽略符号位，空位都以0补齐


jsp的内置对象
--------
参考链接：http://www.cnblogs.com/leirenyuan/p/6016063.html

JSP中一共预先定义了9个这样的对象，分别为：request、response、session、application、out、pagecontext、config、page、exception

1、request对象
request 对象是 javax.servlet.httpServletRequest类型的对象。 该对象代表了客户端的请求信息，主要用于接受通过HTTP协议传送到服务器的数据。（包括头信息、系统信息、请求方式以及请求参数等）。request对象的作用域为一次请求。

2、response对象
response 代表的是对客户端的响应，主要是将JSP容器处理过的对象传回到客户端。response对象也具有作用域，它只在JSP页面内有效。

3、session对象
session 对象是由服务器自动创建的与用户请求相关的对象。服务器为每个用户都生成一个session对象，用于保存该用户的信息，跟踪用户的操作状态。session对象内部使用Map类来保存数据，因此保存数据的格式为 “Key/value”。 session对象的value可以使复杂的对象类型，而不仅仅局限于字符串类型。

4、application对象
 application 对象可将信息保存在服务器中，直到服务器关闭，否则application对象中保存的信息会在整个应用中都有效。与session对象相比，application对象生命周期更长，类似于系统的“全局变量”。

5、out 对象
out 对象用于在Web浏览器内输出信息，并且管理应用服务器上的输出缓冲区。在使用 out 对象输出数据时，可以对数据缓冲区进行操作，及时清除缓冲区中的残余数据，为其他的输出让出缓冲空间。待数据输出完毕后，要及时关闭输出流。

6、pageContext 对象
pageContext 对象的作用是取得任何范围的参数，通过它可以获取 JSP页面的out、request、reponse、session、application 等对象。pageContext对象的创建和初始化都是由容器来完成的，在JSP页面中可以直接使用 pageContext对象。

7、config 对象
config 对象的主要作用是取得服务器的配置信息。通过 pageConext对象的 getServletConfig() 方法可以获取一个config对象。当一个Servlet 初始化时，容器把某些信息通过 config对象传递给这个 Servlet。 开发者可以在web.xml 文件中为应用程序环境中的Servlet程序和JSP页面提供初始化参数。

8、page 对象
page 对象代表JSP本身，只有在JSP页面内才是合法的。 page隐含对象本质上包含当前 Servlet接口引用的变量，类似于Java编程中的 this 指针。

9、exception 对象
exception 对象的作用是显示异常信息，只有在包含 isErrorPage="true" 的页面中才可以被使用，在一般的JSP页面中使用该对象将无法编译JSP文件。excepation对象和Java的所有对象一样，都具有系统提供的继承结构。exception 对象几乎定义了所有异常情况。在Java程序中，可以使用try/catch关键字来处理异常情况； 如果在JSP页面中出现没有捕获到的异常，就会生成 exception 对象，并把 exception 对象传送到在page指令中设定的错误页面中，然后在错误页面中处理相应的 exception 对象。

jsp中的charset和pageEncoding的区别
--------

 - charset设置的是当前jsp响应页面的字符编码方式，鼠标放在charset上，可以看到Attribute :
   contentType；实际调用的是response.setContentType("jsp页面上的contentType的值");
 - pageEncoding则是设置当前页面的保存方式，在eclipse中，如果采用默认的GBK编码，而pageEncoding方式采用“ISO-8859-1”的话，如果jsp中中文字符，则会出现错误，当前jsp页面无法以“IS0-8859-1”的编码方式进行保存。
 

Http协议与TCP协议简单理解
----------------
参考链接：https://www.cnblogs.com/dingjiaoyang/p/5326544.html
TCP协议对应于传输层，而HTTP协议对应于应用层，从本质上来说，二者没有可比性。Http协议是建立在TCP协议基础之上的，当浏览器需要从服务器获取网页数据的时候，会发出一次Http请求。Http会通过TCP建立起一个到服务器的连接通道，当本次请求需要的数据完毕后，Http会立即将TCP连接断开，这个过程是很短的。所以Http连接是一种短连接，是一种**无状态**的连接。所谓的无状态，是指浏览器每次向服务器发起请求的时候，不是通过一个连接，而是每次都建立一个新的连接。如果是一个连接的话，服务器进程中就能保持住这个连接并且在内存中记住一些信息状态。而每次请求结束后，连接就关闭，相关的内容就释放了，所以记不住任何状态，成为无状态连接。

 

cookie 和session 的区别详解
---------------------
1. 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。这个Session是保存在服务端的，有一个唯一标识。在服务端保存Session的方法很多，内存、数据库、文件都有。集群的时候也要考虑Session的转移，在大型的网站，一般会有专门的Session服务器集群，用来保存用户会话，这个时候 Session 信息都是放在内存的，使用一些缓存服务比如Memcached之类的来放 Session。
2. 思考一下服务端如何识别特定的客户？这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。
3. Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。所以，总结一下：Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中；Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。

简单来说：

 - 相同点
 都可以存储一些信息
 都可以设置过期时间
 都是基于kye-value的结构来存储数据
 
 - 不同点
session存在服务器端，cookie存在浏览器端
session更加安全，可以存储一些敏感信息。cookie安全性相对较低，一般存放一些不是很重要的信息
session理论上大小没有限制，但是有过期时间可以设置。单个cookie的大小是有限制的，单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。


前端垮域问题
------
http://www.jianshu.com/p/c3dc6081cf8b

 - 为什么需要跨域
 受浏览器同源策略的限制，本域的js不能操作其他域的页面对象（比如DOM）。但在安全限制的同时也给注入iframe或是ajax应用上带来了不少麻烦。所以我们要通过一些方法使本域的js能够操作其他域的页面对象或者使其他域的js能操作本域的页面对象（iframe之间）。
这里需要明确的一点是： 所谓的域跟js的存放服务器没有关系，比如baidu.com的页面加载了google.com的js，那么此js的所在域是baidu.com而不是google.com。也就是说此时该js能操作baidu.com的页面对象，而不能操作google.com的页面对象。

1.同源策略：

它是由Netscape提出的一个著名的安全策略。同源是指，域名，协议，端口相同。浏览器执行javascript脚本时，会检查这个脚本属于那个页面，如果不是同源页面，就不会被执行。

2.为什么script标签引入的文件不受同源策略的限制？

因为script标签引入的文件内容是不能够被客户端的js获取到的，不会影响到被引用文件的安全，所以没必要使script标签引入的文件遵循浏览器的同源策略。而通过ajax加载的文件内容是能够被客户端js获取到的，所以ajax必须遵循同源策略，否则被引入文件的内容会泄漏或者存在其他风险。

3.静态HTTP服务器：Nginx是一个HTTP服务器，可以将服务器上的静态文件（如HTML、图片）通过HTTP协议展现给客户端

4.反向代理服务器：客户端本来可以直接通过HTTP协议访问某网站应用服务器，网站管理员可以在中间加上一个Nginx，客户端请求Nginx，Nginx请求应用服务器，然后将结果返回给客户端，此时Nginx就是反向代理服务器。

5.负载均衡:当网站访问量非常大，一台服务器已经不够用了。于是将同一个应用部署在多台服务器上，将大量用户的请求分配给多台机器处理。Nginx可以通过反向代理来实现负载均衡。

 - 跨域常用的解决方式
通过jsonp跨域
通过修改document.domain来跨子域
使用window.name来进行跨域
使用HTML5中新引进的window.postMessage方法来跨域传送数据
JavaScript中的null、undefined
-------------------------------
Undefined类型只有一个值，即undefined。当声明的变量还未被初始化时，变量的默认值为undefined。
Null类型也只有一个值，即null。null用来表示尚未存在的对象，常用来表示函数企图返回一个不存在的对象。
例1：js 代码：
    var oValue;  
    alert(oValue == undefined); //output "true"  
     
    这段代码显示为true,代表oVlaue的值即为undefined，因为我们没有初始化它。
例2：js 代码：
    alert(null == document.getElementById('notExistElement'));  
     
    当页面上不存在id为"notExistElement"的DOM节点时，这段代码显示为"true"，因为我们尝试获取一个不存在的对象。
例3：js 代码：
    alert(typeof undefined); //output "undefined"  
    alert(typeof null); //output "object"
    
```javascript
Number(null)
// 0
5 + null
// 5


Number(undefined)
// NaN
5 + undefined
// NaN
```

redis如何建立集群
-----------
参考链接http://blog.csdn.net/fengshizty/article/details/51368004，
https://www.cnblogs.com/hjwublog/p/5681700.html
从redis3.0开始，官方已经开始支持集群模式。
一个主从结构的redis集群模式，如下图所示：

![此处输入图片的描述][28]


上图中的master节点之间，以及slave节点之间，以及master-slave节点之间，都进行了连接。这样做有一个好处，首先主从节点之间数据复制十分方便。然后，当新增集群master节点，或者减少master节点时，会涉及到数据迁移，主节点之间的数据迁移，这时主节点之间相互建立连接就方便了。

增删节点的情况：
假设原来有三个主节点：
节点A覆盖0－5460;
节点B覆盖5461－10922;
节点C覆盖10923－16383.

现在增加一个主节点D：
节点A覆盖1365-5460
节点B覆盖6827-10922
节点C覆盖12288-16383
节点D覆盖0-1364,5461-6826,10923-12287
**注意**节点D覆盖的hash范围，这样做的好处是可以更少的移动元素。


 - 集群分区好处

无论是memcached的一致性哈希算法，还是redis的集群分区，最主要的目的都是在移除、添加一个节点时对已经存在的缓存数据的定位影响尽可能的降到最小。redis将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点， 比如说：

如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。

与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。

因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线，从而保证集群的可用性。下面我们就来学习下集群中节点的增加和删除。

查询出student表中选课数大于3的人学号及课程总分
------------
```sql
mysql> select * from student;
+-----+--------+-------+
| num | course | score |
+-----+--------+-------+
|   1 | yuwen  | 60    |
|   1 | yingyu | 60    |
|   1 | shuxue | 60    |
|   1 | wuli   | 60    |
|   1 | huaxue | 60    |
|   2 | huaxue | 60    |
|   3 | huaxue | 60    |
|   4 | huaxue | 60    |
|   4 | wuli   | 60    |
|   4 | shuxue | 60    |
|   4 | yingyu | 60    |
+-----+--------+-------+
11 rows in set (0.00 sec)

mysql> select num,sum(score) from student group by num having count(num) >3;
+-----+------------+
| num | sum(score) |
+-----+------------+
|   1 |        300 |
|   4 |        240 |
+-----+------------+
2 rows in set (0.00 sec)

mysql> 
```

linux运维知识：
----------
linux常用命令，参考链接http://blog.csdn.net/xiaoguaihai/article/details/8705992/，写的很全面，注释也很多

Linux下几个常见的文件查找命令：

which       查看**可执行文件**的位置 
whereis    寻找特定文件，查看文件的位置 
locate       配合数据库查看文件位置 
find          **实际搜寻硬盘查询文件名称** 


线上查询及帮助命令（1个）
man 
目录操作命令（6个）
ls tree pwd mkdir rmdir cd


文件操作命令（7个）
touch cp mv rm ln find rename


文件查看及处理命令（21个）
cat more less head tac head tail cut paste 
sort uniq wc iconv dos2unix file diff tree chattr 
lsattr rev vimdiff


文件打包压缩命令（3个）
gzip tar unzip


信息显示命令（12个）
uname hostname dmesg uptime file stat du df top free w date


搜索文件命令（4个）
find which whereis locate


用户管理命令（10个）
useradd userdel passwd chage usermod id su sudo visudo 
groupadd


基本网络操作命令（10个）
telnet ssh scp wget ping route ifconfig ifup ifdown netstat


深入网络操作命令（6个）
route mail mutt nslookup dig wget


有关磁盘空间的命令（6个）
mount umount df du fsck dd


关机和查看系统信息的命令（7个）
shutdown reboot ps top kill date


安装和登录命令（3个）
shutdown halt reboot


系统管理相关命令（9个）
top free vmstat mpstat iostat sar kill chkconfig last


系统安全相关命令（13个）
passwd su sudo umask chgrp chmod chown chattr lsattr ps 
whoami


查看系统用户登陆信息命令（6个）
w who users last lastlog fingers


查看硬件相关命令（6个）
ethtool mii-tool dmidecode dmesg lspci


其它（14个）
chkconfig echo yum watch alias unalias date clear history eject 
time nohup nc xargs


监视物理组件的高级 Linux命令

内存:top free vmstat mpstat iostat sar 
CPU:top vmstat mpstat iostat sar 
I/O:vmstat mpstat iostat sar 
进程:ipcs ipcrm 
负载:uptime


以上命令属于武功里的《九阴真经》，如果掌握好了，会非常牛。


关机/重启/注销命令


关机: 
shutdown -h now ——>立刻关机(生产常用) 
shhutdown -h +1 ——>1 分钟以后关机
init 0 
halt ——>立即停止系统，需要人工关闭电源
halt -p 
poweroff ——>立即停止系统，并且关闭电源
重启: 
reboot(生产常用) 
shutdown -r now(生产常用) 
shhutdown -r +1 ——>1 分钟以后重起
init 6 
注销
logout 
exit(生产常用) 
ctl+d ——>快捷键(生产常用)


进程管理：（16个）
bg：后台运行 fg：挂起程序 jobs：显示后台程序 kill,killall,pkill：杀掉进程
crontab：设置定时 ps：查看进程 pstree：显示进程状态树
top：显示进程 nice：改变优先权 nohup：用户退出系统之后继续工作
pgrep：查找匹配条件的进程 strace：跟踪一个进程的系统调用
ltrace：跟踪进程调用库函数的情 vmstat：报告虚拟内存统计信息


危险的系统命令：
mv rm dd fdisk parted


linux 四剑客（4 个）
grep egrep sed awk

 - tar命令
 tar命令可以调用gzip命令（-z参数）或者是bzip命令(-j参数)来进行文件的压缩（tar命令本身只执行打包，并不会压缩）。
 
 以使用gzip为例：压缩src文件夹
 压缩：tar -zcv -f src.tar.gz src
 解压缩：tar -zxv -f src.tar.gz
 
 压缩率bzip2 > gzip > zip
 zip的通用性较好，而现在windows下软件winrar,7zip等对tar.gz的支持也非常好。推荐用tar.gz，bzip2要耗费更多的cpu
 
 

Integer包装类
----------
```java
        Integer ii = 100;
		int jj = 100;
		if (ii == jj)
		{
			System.out.println("简单类型，范围没有超过-128至127，所以相等");
		}
		Integer iii = 200;
		int jjj = 200;
		if (iii == jjj)
		{
			System.out.println("简单类型，范围虽然超过了-128至127，但是使用了自动拆箱，所以相等");
		}
		Integer iiii = 100;
		Integer jjjj = 100;
		if (iiii == jjjj)
		{
			System.out.println("自动拆箱成简单类型，范围在-128至127，所以相等");
		}
		
		Integer iiiii = 200;
		Integer jjjjj = 200;
		if (iiiii == jjjjj)
		{
			System.out.println("范围超过了-128至127，相当于new了两个对象，所以不相等");
		}
```

程序的运行结果如下：

![此处输入图片的描述][29]


  当我们使用Integer类去定义一个变量时，会用到java提供的自动装箱的技术，即会使用Integer中的这个函数：
  
```java
   /**
     * Returns an {@code Integer} instance representing the specified
     * {@code int} value.  If a new {@code Integer} instance is not
     * required, this method should generally be used in preference to
     * the constructor {@link #Integer(int)}, as this method is likely
     * to yield significantly better space and time performance by
     * caching frequently requested values.
     *
     * This method will always cache values in the range -128 to 127,
     * inclusive, and may cache other values outside of this range.
     *
     * @param  i an {@code int} value.
     * @return an {@code Integer} instance representing {@code i}.
     * @since  1.5
     */
    public static Integer valueOf(int i) {
        if (i >= IntegerCache.low && i <= IntegerCache.high)
            return IntegerCache.cache[i + (-IntegerCache.low)];
        return new Integer(i);
    }
```

正如这个函数的注释部分所说，将-128至127范围内的数据认为是频繁使用的数据，使用数组进行缓存。

当然，下面这种纯粹的简单类型，是不会涉及装箱技术的：

```java
        int kk =200;
		int ll = 200;
		if (kk==ll)
		{
			System.out.println("相等");
		}
```

HashMap插入null的具体操作
------------------
HashMap是允许存在key为null的情况的，有且仅有一个key是null，当key为null的时候，put操作会怎么处理呢？
首先看一下put操作的源码：
```java
/**
     * Associates the specified value with the specified key in this map.
     * If the map previously contained a mapping for the key, the old
     * value is replaced.
     *
     * @param key key with which the specified value is to be associated
     * @param value value to be associated with the specified key
     * @return the previous value associated with <tt>key</tt>, or
     *         <tt>null</tt> if there was no mapping for <tt>key</tt>.
     *         (A <tt>null</tt> return can also indicate that the map
     *         previously associated <tt>null</tt> with <tt>key</tt>.)
     */
    public V put(K key, V value) {
        return putVal(hash(key), key, value, false, true);
    }
```

它内部是调用了putVal方法，但是其实putVal方法内部是没有对null的key的这种情况进行特殊的处理，是在hash方法的内部处理的。
下面是hash方法的源码：
```java
 /**
     * Computes key.hashCode() and spreads (XORs) higher bits of hash
     * to lower.  Because the table uses power-of-two masking, sets of
     * hashes that vary only in bits above the current mask will
     * always collide. (Among known examples are sets of Float keys
     * holding consecutive whole numbers in small tables.)  So we
     * apply a transform that spreads the impact of higher bits
     * downward. There is a tradeoff between speed, utility, and
     * quality of bit-spreading. Because many common sets of hashes
     * are already reasonably distributed (so don't benefit from
     * spreading), and because we use trees to handle large sets of
     * collisions in bins, we just XOR some shifted bits in the
     * cheapest possible way to reduce systematic lossage, as well as
     * to incorporate impact of the highest bits that would otherwise
     * never be used in index calculations because of table bounds.
     */
    static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }
```

从源码可以看出，key为null的时候，它就被存在0位置。
**注意**，Hashtable中是不允许key为null的情况。下面是Hashtable的put方法的源码：
```java
/**
     * Maps the specified <code>key</code> to the specified
     * <code>value</code> in this hashtable. Neither the key nor the
     * value can be <code>null</code>. <p>
     *
     * The value can be retrieved by calling the <code>get</code> method
     * with a key that is equal to the original key.
     *
     * @param      key     the hashtable key
     * @param      value   the value
     * @return     the previous value of the specified key in this hashtable,
     *             or <code>null</code> if it did not have one
     * @exception  NullPointerException  if the key or value is
     *               <code>null</code>
     * @see     Object#equals(Object)
     * @see     #get(Object)
     */
    public synchronized V put(K key, V value) {
        // Make sure the value is not null
        if (value == null) {
            throw new NullPointerException();
        }

        // Makes sure the key is not already in the hashtable.
        Entry<?,?> tab[] = table;
        int hash = key.hashCode();
        int index = (hash & 0x7FFFFFFF) % tab.length;
        @SuppressWarnings("unchecked")
        Entry<K,V> entry = (Entry<K,V>)tab[index];
        for(; entry != null ; entry = entry.next) {
            if ((entry.hash == hash) && entry.key.equals(key)) {
                V old = entry.value;
                entry.value = value;
                return old;
            }
        }

        addEntry(hash, key, value, index);
        return null;
    }
```

注意源码中的：Neither the key nor the value can be null。

**注意**，ConcurrentHashMap也**不**允许key和value为null的情况。
下面是put方法的源码：
```java
/**
     * Maps the specified key to the specified value in this table.
     * Neither the key nor the value can be null.
     *
     * <p>The value can be retrieved by calling the {@code get} method
     * with a key that is equal to the original key.
     *
     * @param key key with which the specified value is to be associated
     * @param value value to be associated with the specified key
     * @return the previous value associated with {@code key}, or
     *         {@code null} if there was no mapping for {@code key}
     * @throws NullPointerException if the specified key or value is null
     */
    public V put(K key, V value) {
        return putVal(key, value, false);
    }

    /** Implementation for put and putIfAbsent */
    final V putVal(K key, V value, boolean onlyIfAbsent) {
        if (key == null || value == null) throw new NullPointerException();
        int hash = spread(key.hashCode());
        int binCount = 0;
        for (Node<K,V>[] tab = table;;) {
            Node<K,V> f; int n, i, fh;
            if (tab == null || (n = tab.length) == 0)
                tab = initTable();
            else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
                if (casTabAt(tab, i, null,
                             new Node<K,V>(hash, key, value, null)))
                    break;                   // no lock when adding to empty bin
            }
            else if ((fh = f.hash) == MOVED)
                tab = helpTransfer(tab, f);
            else {
                V oldVal = null;
                synchronized (f) {
                    if (tabAt(tab, i) == f) {
                        if (fh >= 0) {
                            binCount = 1;
                            for (Node<K,V> e = f;; ++binCount) {
                                K ek;
                                if (e.hash == hash &&
                                    ((ek = e.key) == key ||
                                     (ek != null && key.equals(ek)))) {
                                    oldVal = e.val;
                                    if (!onlyIfAbsent)
                                        e.val = value;
                                    break;
                                }
                                Node<K,V> pred = e;
                                if ((e = e.next) == null) {
                                    pred.next = new Node<K,V>(hash, key,
                                                              value, null);
                                    break;
                                }
                            }
                        }
                        else if (f instanceof TreeBin) {
                            Node<K,V> p;
                            binCount = 2;
                            if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                           value)) != null) {
                                oldVal = p.val;
                                if (!onlyIfAbsent)
                                    p.val = value;
                            }
                        }
                    }
                }
                if (binCount != 0) {
                    if (binCount >= TREEIFY_THRESHOLD)
                        treeifyBin(tab, i);
                    if (oldVal != null)
                        return oldVal;
                    break;
                }
            }
        }
        addCount(1L, binCount);
        return null;
    }
```

stringbuffer和stringbuilder的扩容函数的具体实现
------------------------------------
二者都是继承自AbstractStringBuilder抽象类，二者在扩容的时候，都会调用AbstractStringBuilder的ensureCapacityInternal方法，源码如下：

```java
   /**
     * For positive values of {@code minimumCapacity}, this method
     * behaves like {@code ensureCapacity}, however it is never
     * synchronized.
     * If {@code minimumCapacity} is non positive due to numeric
     * overflow, this method throws {@code OutOfMemoryError}.
     */
    private void ensureCapacityInternal(int minimumCapacity) {
        // overflow-conscious code
        if (minimumCapacity - value.length > 0) {
            value = Arrays.copyOf(value,
                    newCapacity(minimumCapacity));
        }
    }
```
注意，其中的newCapacity方法源码如下，扩容的思路大概思路是：首先扩容到原大小的2倍大+2的大小。然后判断扩容后的数组能否存下，否则就按照新的字符串长度进行扩容。
```java
/**
     * Returns a capacity at least as large as the given minimum capacity.
     * Returns the current capacity increased by the same amount + 2 if
     * that suffices.
     * Will not return a capacity greater than {@code MAX_ARRAY_SIZE}
     * unless the given minimum capacity is greater than that.
     *
     * @param  minCapacity the desired minimum capacity
     * @throws OutOfMemoryError if minCapacity is less than zero or
     *         greater than Integer.MAX_VALUE
     */
    private int newCapacity(int minCapacity) {
        // overflow-conscious code
        int newCapacity = (value.length << 1) + 2;
        if (newCapacity - minCapacity < 0) {
            newCapacity = minCapacity;
        }
        return (newCapacity <= 0 || MAX_ARRAY_SIZE - newCapacity < 0)
            ? hugeCapacity(minCapacity)
            : newCapacity;
    }
```

注意上面源码中的hugeCapacity方法，当newCapacity超过了MAX_ARRAY_SIZE（Integer.MAX_VALUE - 8）的时候，将会调用该方法，下面是该方法源码。但是也要注意newCapacity**不能**超过Integer.MAX_VALUE，否则会抛出OutOfMemoryError异常。

```java
    private int hugeCapacity(int minCapacity) {
        if (Integer.MAX_VALUE - minCapacity < 0) { // overflow
            throw new OutOfMemoryError();
        }
        return (minCapacity > MAX_ARRAY_SIZE)
            ? minCapacity : MAX_ARRAY_SIZE;
    }
```

统计出某个log文件中ip出现次数最多的ip
----------------------
cat  access_log_2011_06_26.log |awk '{print $1}'|uniq -c|sort -n ｜head 5

其中，$1表示IP地址会出现在log文件的第1列.




ps命令、netstat命令
--------------

```java
ps -ef|grep java
```
查看所有的java进程，示例输出如下：

```linux
audi@audi-PC:~/WorkSpace_git/learn$ ps -ef|grep java
audi      2590  2573 71 12:05 ?        00:04:20 /usr/lib/jvm/java/jdk1.8.0_151/jre/bin/java -Dosgi.requiredJavaVersion=1.8 -Dosgi.instance.area.default=@user.home/eclipse-workspace -XX:+UseG1GC -XX:+UseStringDeduplication -Dosgi.requiredJavaVersion=1.8 -Xms256m -Xmx1024m -jar /home/audi/runtime/eclipse//plugins/org.eclipse.equinox.launcher_1.4.0.v20161219-1356.jar -os linux -ws gtk -arch x86_64 -showsplash /home/audi/runtime/eclipse//plugins/org.eclipse.epp.package.common_4.7.1.20171005-1200/splash.bmp -launcher /home/audi/runtime/eclipse/eclipse -name Eclipse --launcher.library /home/audi/runtime/eclipse//plugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.550.v20170928-1359/eclipse_1629.so -startup /home/audi/runtime/eclipse//plugins/org.eclipse.equinox.launcher_1.4.0.v20161219-1356.jar --launcher.appendVmargs -exitdata 58800d -product org.eclipse.epp.package.jee.product -vm /usr/lib/jvm/java/jdk1.8.0_151/jre/bin/java -vmargs -Dosgi.requiredJavaVersion=1.8 -Dosgi.instance.area.default=@user.home/eclipse-workspace -XX:+UseG1GC -XX:+UseStringDeduplication -Dosgi.requiredJavaVersion=1.8 -Xms256m -Xmx1024m -jar /home/audi/runtime/eclipse//plugins/org.eclipse.equinox.launcher_1.4.0.v20161219-1356.jar
audi      2839  2590 99 12:11 ?        00:00:04 /usr/lib/jvm/java/jdk1.8.0_151/bin/java -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java/jdk1.8.0_151/jre/lib/resources.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/rt.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/jsse.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/jce.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/charsets.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/jfr.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/jfxrt.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/sunec.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/localedata.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java/jdk1.8.0_151/jre/lib/ext/jaccess.jar:/home/audi/WorkSpace_git/helloWorld/bin:/home/audi/WorkSpace_git/helloWorld/mysql-connector-java-6.0.6.jar:/home/audi/WorkSpace_git/helloWorld/cglib-3.2.5.jar com.audi.SayHello
audi      2860  2404  0 12:11 pts/0    00:00:00 grep java
audi@audi-PC:~/WorkSpace_git/learn$ 
```

输出结果的第二列就是PID。得到了PID就可以使用kill -9 命令进行强制停止线程。

netstat命令可以用于查看程序的端口链接情况，或者占用情况。
例如，Linux下查看tomcat占用端口，可以使用如下命令：
```linux
1、先查看tomcat的进程号
ps -ef | grep tomcat*
后面带*号，是为了查看多个tomcat，例如tomcat6，tomcat7。

2、根据进程号查看端口号

netstat -anop | grep 15161

可以看到8865、8866、8867端口号被占用
```

找到当前系统的最大的文件
------------
```linux
find / -type f -size +1G 找到根目录下大于1G的文件
find . -type f -size +800M 找到当前目录下大于800MB的文件

查找当前目录（搜索深度为1）下最大的10个文件
du --max-depth=1 / -h |sort -h -r |head -10
```

Linux du命令和df命令区别
-----------------
  

 - du，disk usage
是通过搜索文件来计算每个文件的大小然后累加，du能看到的文件只是一些当前存在的，没有被删除的。他计算的大小就是当前他认为存在的所有文件大小的累加和。
       
 - df，disk free
通过文件系统来快速获取空间大小的信息，当我们删除一个文件的时候，这个文件不是马上就在文件系统当中消失了，而是暂时消失了，当所有程序都不用时，才会根据OS的规则释放掉已经删除的文件， df记录的是通过文件系统获取到的文件的大小，他比du强的地方就是能够看到已经删除的文件，而且计算大小的时候，把这一部分的空间也加上了，更精确了。当文件系统也确定删除了该文件后，这时候du与df就一致了。

使用示例如下，注意加入-h参数可以使得结果更加便于阅读（转换成了KB或者MB）。
```linux
root@audi-PC:/usr/local# du  （查看的是当前目录下的文件空间占用情况）
23152	./redis/bin
23220	./redis
4	./etc
4	./sbin
4	./games
4	./src
284	./lib/python2.7/dist-packages/pygments/formatters
284	./lib/python2.7/dist-packages/pygments/styles
32	./lib/python2.7/dist-packages/pygments/filters
5832	./lib/python2.7/dist-packages/pygments/lexers
6760	./lib/python2.7/dist-packages/pygments
56	./lib/python2.7/dist-packages/Pygments-2.2.0.dist-info
6820	./lib/python2.7/dist-packages
4	./lib/python2.7/site-packages
6828	./lib/python2.7
4	./lib/python3.5/dist-packages
8	./lib/python3.5
6840	./lib
4	./include
8	./bin
4	./share/sgml/declaration
4	./share/sgml/dtd
4	./share/sgml/misc
4	./share/sgml/entities
4	./share/sgml/stylesheet
24	./share/sgml
4	./share/xml/declaration
4	./share/xml/misc
4	./share/xml/schema
4	./share/xml/entities
20	./share/xml
16	./share/icons/hicolor/128x128/apps
20	./share/icons/hicolor/128x128
24	./share/icons/hicolor/256x256/apps
28	./share/icons/hicolor/256x256
32	./share/icons/hicolor/64x64/apps
36	./share/icons/hicolor/64x64
40	./share/icons/hicolor/16x16/apps
44	./share/icons/hicolor/16x16
8	./share/icons/hicolor/22x22/apps
12	./share/icons/hicolor/22x22
44	./share/icons/hicolor/48x48/apps
48	./share/icons/hicolor/48x48
48	./share/icons/hicolor/512x512/apps
52	./share/icons/hicolor/512x512
28	./share/icons/hicolor/32x32/apps
32	./share/icons/hicolor/32x32
28	./share/icons/hicolor/24x24/apps
32	./share/icons/hicolor/24x24
308	./share/icons/hicolor
312	./share/icons
4	./share/fonts
16	./share/applications
4	./share/emacs/site-lisp
8	./share/emacs
4	./share/man
4	./share/ca-certificates
396	./share
30488	.
root@audi-PC:/usr/local# du -h
23M	./redis/bin
23M	./redis
4.0K	./etc
4.0K	./sbin
4.0K	./games
4.0K	./src
284K	./lib/python2.7/dist-packages/pygments/formatters
284K	./lib/python2.7/dist-packages/pygments/styles
32K	./lib/python2.7/dist-packages/pygments/filters
5.7M	./lib/python2.7/dist-packages/pygments/lexers
6.7M	./lib/python2.7/dist-packages/pygments
56K	./lib/python2.7/dist-packages/Pygments-2.2.0.dist-info
6.7M	./lib/python2.7/dist-packages
4.0K	./lib/python2.7/site-packages
6.7M	./lib/python2.7
4.0K	./lib/python3.5/dist-packages
8.0K	./lib/python3.5
6.7M	./lib
4.0K	./include
8.0K	./bin
4.0K	./share/sgml/declaration
4.0K	./share/sgml/dtd
4.0K	./share/sgml/misc
4.0K	./share/sgml/entities
4.0K	./share/sgml/stylesheet
24K	./share/sgml
4.0K	./share/xml/declaration
4.0K	./share/xml/misc
4.0K	./share/xml/schema
4.0K	./share/xml/entities
20K	./share/xml
16K	./share/icons/hicolor/128x128/apps
20K	./share/icons/hicolor/128x128
24K	./share/icons/hicolor/256x256/apps
28K	./share/icons/hicolor/256x256
32K	./share/icons/hicolor/64x64/apps
36K	./share/icons/hicolor/64x64
40K	./share/icons/hicolor/16x16/apps
44K	./share/icons/hicolor/16x16
8.0K	./share/icons/hicolor/22x22/apps
12K	./share/icons/hicolor/22x22
44K	./share/icons/hicolor/48x48/apps
48K	./share/icons/hicolor/48x48
48K	./share/icons/hicolor/512x512/apps
52K	./share/icons/hicolor/512x512
28K	./share/icons/hicolor/32x32/apps
32K	./share/icons/hicolor/32x32
28K	./share/icons/hicolor/24x24/apps
32K	./share/icons/hicolor/24x24
308K	./share/icons/hicolor
312K	./share/icons
4.0K	./share/fonts
16K	./share/applications
4.0K	./share/emacs/site-lisp
8.0K	./share/emacs
4.0K	./share/man
4.0K	./share/ca-certificates
396K	./share
30M	.
root@audi-PC:/usr/local# df  （查看的是整个磁盘的占用情况，还有剩余空间情况）
文件系统           1K-块      已用     可用 已用% 挂载点
udev             4018948         0  4018948    0% /dev
tmpfs             808520      1436   807084    1% /run
/dev/sda7      129679476  34844620 88204408   29% /
tmpfs            4042596    154992  3887604    4% /dev/shm
tmpfs               5120         4     5116    1% /run/lock
tmpfs            4042596         0  4042596    0% /sys/fs/cgroup
tmpfs             808516        24   808492    1% /run/user/1000
/dev/sda5      209720508 146873736 62846772   71% /media/audi/0002DA810009FB6F
root@audi-PC:/usr/local# df -h
文件系统        容量  已用  可用 已用% 挂载点
udev            3.9G     0  3.9G    0% /dev
tmpfs           790M  1.5M  789M    1% /run
/dev/sda7       124G   34G   85G   29% /
tmpfs           3.9G  152M  3.8G    4% /dev/shm
tmpfs           5.0M  4.0K  5.0M    1% /run/lock
tmpfs           3.9G     0  3.9G    0% /sys/fs/cgroup
tmpfs           790M   24K  790M    1% /run/user/1000
/dev/sda5       201G  141G   60G   71% /media/audi/0002DA810009FB6F
root@audi-PC:/usr/local# 
```

当fixedThreadPool的等待队列满了以后会怎样
----------------------------
先简单介绍一下线程池Executors类的几个实现方法，简单来说有5个，分别是newFixedThreadPool、newCachedThreadPool、newSingleThreadExecutor、newScheduledThreadPool、newWorkStealingPool（JDK1.8新增）
简要介绍一下集中线程池。

 - newFixedThreadPool

定长线程池，源码如下：
```java
/**
     * Creates a thread pool that reuses a fixed number of threads
     * operating off a shared unbounded queue.  At any point, at most
     * {@code nThreads} threads will be active processing tasks.
     * If additional tasks are submitted when all threads are active,
     * they will wait in the queue until a thread is available.
     * If any thread terminates due to a failure during execution
     * prior to shutdown, a new one will take its place if needed to
     * execute subsequent tasks.  The threads in the pool will exist
     * until it is explicitly {@link ExecutorService#shutdown shutdown}.
     *
     * @param nThreads the number of threads in the pool
     * @return the newly created thread pool
     * @throws IllegalArgumentException if {@code nThreads <= 0}
     */
    public static ExecutorService newFixedThreadPool(int nThreads) {
        return new ThreadPoolExecutor(nThreads, nThreads,
                                      0L, TimeUnit.MILLISECONDS,
                                      new LinkedBlockingQueue<Runnable>());
    }
```
从源码可以看出，它内部调用了ThreadPoolExecutor类的构造函数来生成ExecutorService类型的实例对象。**注意**，ThreadPoolExecutor是AbstractExecutorService抽象类的子类，AbstractExecutorService是ExecutorService接口的实现类。

ThreadPoolExecutor构造函数中的第3个参数**0L**表示，线程会一直存活，不会被回收。

ThreadPoolExecutor的源码如下：
```java
/**
     * Creates a new {@code ThreadPoolExecutor} with the given initial
     * parameters and default thread factory and rejected execution handler.
     * It may be more convenient to use one of the {@link Executors} factory
     * methods instead of this general purpose constructor.
     *
     * @param corePoolSize the number of threads to keep in the pool, even
     *        if they are idle, unless {@code allowCoreThreadTimeOut} is set
     * @param maximumPoolSize the maximum number of threads to allow in the
     *        pool
     * @param keepAliveTime when the number of threads is greater than
     *        the core, this is the maximum time that excess idle threads
     *        will wait for new tasks before terminating.
     * @param unit the time unit for the {@code keepAliveTime} argument
     * @param workQueue the queue to use for holding tasks before they are
     *        executed.  This queue will hold only the {@code Runnable}
     *        tasks submitted by the {@code execute} method.
     * @throws IllegalArgumentException if one of the following holds:<br>
     *         {@code corePoolSize < 0}<br>
     *         {@code keepAliveTime < 0}<br>
     *         {@code maximumPoolSize <= 0}<br>
     *         {@code maximumPoolSize < corePoolSize}
     * @throws NullPointerException if {@code workQueue} is null
     */
    public ThreadPoolExecutor(int corePoolSize,
                              int maximumPoolSize,
                              long keepAliveTime,
                              TimeUnit unit,
                              BlockingQueue<Runnable> workQueue) {
        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
             Executors.defaultThreadFactory(), defaultHandler);
    }
```

 - newCachedThreadPool
 根据实时任务的调用数量来动态的增加或者减少线程的数量，其源码如下：
```java
 /**
     * Creates a thread pool that creates new threads as needed, but
     * will reuse previously constructed threads when they are
     * available.  These pools will typically improve the performance
     * of programs that execute many short-lived asynchronous tasks.
     * Calls to {@code execute} will reuse previously constructed
     * threads if available. If no existing thread is available, a new
     * thread will be created and added to the pool. Threads that have
     * not been used for sixty seconds are terminated and removed from
     * the cache. Thus, a pool that remains idle for long enough will
     * not consume any resources. Note that pools with similar
     * properties but different details (for example, timeout parameters)
     * may be created using {@link ThreadPoolExecutor} constructors.
     *
     * @return the newly created thread pool
     */
    public static ExecutorService newCachedThreadPool() {
        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                      60L, TimeUnit.SECONDS,
                                      new SynchronousQueue<Runnable>());
    }
```
从源码中可以看出，它内部其实也是调用了ThreadPoolExecutor构造函数来进行线程池的构造。**注意**，线程池内的线程空闲超过60s，那么就会进行回收，线程池内默认没有任何线程。

 - newSingleThreadExecutor
创建一个只有单个线程的线程池，其源码如下：
```java
/**
     * Creates an Executor that uses a single worker thread operating
     * off an unbounded queue. (Note however that if this single
     * thread terminates due to a failure during execution prior to
     * shutdown, a new one will take its place if needed to execute
     * subsequent tasks.)  Tasks are guaranteed to execute
     * sequentially, and no more than one task will be active at any
     * given time. Unlike the otherwise equivalent
     * {@code newFixedThreadPool(1)} the returned executor is
     * guaranteed not to be reconfigurable to use additional threads.
     *
     * @return the newly created single-threaded Executor
     */
    public static ExecutorService newSingleThreadExecutor() {
        return new FinalizableDelegatedExecutorService
            (new ThreadPoolExecutor(1, 1,
                                    0L, TimeUnit.MILLISECONDS,
                                    new LinkedBlockingQueue<Runnable>()));
    }
```

从源码中可以看出，它的过期时间也是0L，也就是说不会被回收。

 - newScheduledThreadPool
创建一个可延缓指定时间执行的线程池，或者可以周期性执行的线程池，其源码如下：
```java
/**
     * Creates a thread pool that can schedule commands to run after a
     * given delay, or to execute periodically.
     * @param corePoolSize the number of threads to keep in the pool,
     * even if they are idle
     * @return a newly created scheduled thread pool
     * @throws IllegalArgumentException if {@code corePoolSize < 0}
     */
    public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) {
        return new ScheduledThreadPoolExecutor(corePoolSize);
    }
```
它内部调用的就不是ThreadPoolExecutor构造函数了，而是ScheduledThreadPoolExecutor构造函数，ScheduledThreadPoolExecutor其实是ThreadPoolExecutor的子类。
ScheduledThreadPoolExecutor的源码如下：
```java
/**
     * Creates a new {@code ScheduledThreadPoolExecutor} with the
     * given core pool size.
     *
     * @param corePoolSize the number of threads to keep in the pool, even
     *        if they are idle, unless {@code allowCoreThreadTimeOut} is set
     * @throws IllegalArgumentException if {@code corePoolSize < 0}
     */
    public ScheduledThreadPoolExecutor(int corePoolSize) {
        super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,
              new DelayedWorkQueue());
    }
```
从源码可以看出，它内部其实还是调用的ThreadPoolExecutor的构造函数，只不过它的队列类型是DelayedWorkQueue。

调度可以通过ScheduledExecutorService类的4个schedule方法实现,具体如下所示:
```java
   /**
     * Creates and executes a one-shot action that becomes enabled
     * after the given delay.
     *
     * @param command the task to execute
     * @param delay the time from now to delay execution
     * @param unit the time unit of the delay parameter
     * @return a ScheduledFuture representing pending completion of
     *         the task and whose {@code get()} method will return
     *         {@code null} upon completion
     * @throws RejectedExecutionException if the task cannot be
     *         scheduled for execution
     * @throws NullPointerException if command is null
     */
    public ScheduledFuture<?> schedule(Runnable command,
                                       long delay, TimeUnit unit);

    /**
     * Creates and executes a ScheduledFuture that becomes enabled after the
     * given delay.
     *
     * @param callable the function to execute
     * @param delay the time from now to delay execution
     * @param unit the time unit of the delay parameter
     * @param <V> the type of the callable's result
     * @return a ScheduledFuture that can be used to extract result or cancel
     * @throws RejectedExecutionException if the task cannot be
     *         scheduled for execution
     * @throws NullPointerException if callable is null
     */
    public <V> ScheduledFuture<V> schedule(Callable<V> callable,
                                           long delay, TimeUnit unit);

    /**
     * Creates and executes a periodic action that becomes enabled first
     * after the given initial delay, and subsequently with the given
     * period; that is executions will commence after
     * {@code initialDelay} then {@code initialDelay+period}, then
     * {@code initialDelay + 2 * period}, and so on.
     * If any execution of the task
     * encounters an exception, subsequent executions are suppressed.
     * Otherwise, the task will only terminate via cancellation or
     * termination of the executor.  If any execution of this task
     * takes longer than its period, then subsequent executions
     * may start late, but will not concurrently execute.
     *
     * @param command the task to execute
     * @param initialDelay the time to delay first execution
     * @param period the period between successive executions
     * @param unit the time unit of the initialDelay and period parameters
     * @return a ScheduledFuture representing pending completion of
     *         the task, and whose {@code get()} method will throw an
     *         exception upon cancellation
     * @throws RejectedExecutionException if the task cannot be
     *         scheduled for execution
     * @throws NullPointerException if command is null
     * @throws IllegalArgumentException if period less than or equal to zero
     */
    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command,
                                                  long initialDelay,
                                                  long period,
                                                  TimeUnit unit);

    /**
     * Creates and executes a periodic action that becomes enabled first
     * after the given initial delay, and subsequently with the
     * given delay between the termination of one execution and the
     * commencement of the next.  If any execution of the task
     * encounters an exception, subsequent executions are suppressed.
     * Otherwise, the task will only terminate via cancellation or
     * termination of the executor.
     *
     * @param command the task to execute
     * @param initialDelay the time to delay first execution
     * @param delay the delay between the termination of one
     * execution and the commencement of the next
     * @param unit the time unit of the initialDelay and delay parameters
     * @return a ScheduledFuture representing pending completion of
     *         the task, and whose {@code get()} method will throw an
     *         exception upon cancellation
     * @throws RejectedExecutionException if the task cannot be
     *         scheduled for execution
     * @throws NullPointerException if command is null
     * @throws IllegalArgumentException if delay less than or equal to zero
     */
    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command,
                                                     long initialDelay,
                                                     long delay,
                                                     TimeUnit unit);

}
```


 - newWorkStealingPool（JDK1.8新增）

利用分治的思想，将一个任务划分给多个线程执行，有点类似于mapReduce的思想。其源码如下：
```java
/**
     * Creates a thread pool that maintains enough threads to support
     * the given parallelism level, and may use multiple queues to
     * reduce contention. The parallelism level corresponds to the
     * maximum number of threads actively engaged in, or available to
     * engage in, task processing. The actual number of threads may
     * grow and shrink dynamically. A work-stealing pool makes no
     * guarantees about the order in which submitted tasks are
     * executed.
     *
     * @param parallelism the targeted parallelism level
     * @return the newly created thread pool
     * @throws IllegalArgumentException if {@code parallelism <= 0}
     * @since 1.8
     */
    public static ExecutorService newWorkStealingPool(int parallelism) {
        return new ForkJoinPool
            (parallelism,
             ForkJoinPool.defaultForkJoinWorkerThreadFactory,
             null, true);
    }

    /**
     * Creates a work-stealing thread pool using all
     * {@link Runtime#availableProcessors available processors}
     * as its target parallelism level.
     * @return the newly created thread pool
     * @see #newWorkStealingPool(int)
     * @since 1.8
     */
    public static ExecutorService newWorkStealingPool() {
        return new ForkJoinPool
            (Runtime.getRuntime().availableProcessors(),
             ForkJoinPool.defaultForkJoinWorkerThreadFactory,
             null, true);
    }
```

其中的ForkJoinPool构造函数源码如下：
```java
/**
     * Creates a {@code ForkJoinPool} with the given parameters.
     *
     * @param parallelism the parallelism level. For default value,
     * use {@link java.lang.Runtime#availableProcessors}.
     * @param factory the factory for creating new threads. For default value,
     * use {@link #defaultForkJoinWorkerThreadFactory}.
     * @param handler the handler for internal worker threads that
     * terminate due to unrecoverable errors encountered while executing
     * tasks. For default value, use {@code null}.
     * @param asyncMode if true,
     * establishes local first-in-first-out scheduling mode for forked
     * tasks that are never joined. This mode may be more appropriate
     * than default locally stack-based mode in applications in which
     * worker threads only process event-style asynchronous tasks.
     * For default value, use {@code false}.
     * @throws IllegalArgumentException if parallelism less than or
     *         equal to zero, or greater than implementation limit
     * @throws NullPointerException if the factory is null
     * @throws SecurityException if a security manager exists and
     *         the caller is not permitted to modify threads
     *         because it does not hold {@link
     *         java.lang.RuntimePermission}{@code ("modifyThread")}
     */
    public ForkJoinPool(int parallelism,
                        ForkJoinWorkerThreadFactory factory,
                        UncaughtExceptionHandler handler,
                        boolean asyncMode) {
        this(checkParallelism(parallelism),
             checkFactory(factory),
             handler,
             asyncMode ? FIFO_QUEUE : LIFO_QUEUE,
             "ForkJoinPool-" + nextPoolId() + "-worker-");
        checkPermission();
    }
```
默认会使用可用的CPU数量来定义并行级别。

 - 当fixedThreadPool的等待队列满了以后会怎样
一般来说，不会满，因为这个阻塞队列可以最大到Integer.MAX_VALUE，也就是2147483647 [0x7fffffff]，不过这么大的队列，很容易造成堆栈溢出。

```java
/**
     * Creates a {@code LinkedBlockingQueue} with a capacity of
     * {@link Integer#MAX_VALUE}.
     */
    public LinkedBlockingQueue() {
        this(Integer.MAX_VALUE);
    }
```

我们可以自己手动新建一个队列，指定队列的最大值（当队列满的时候，put操作会被阻塞住，新的元素无法插入，直到队列有空余，然后将这个队列传入ThreadPoolExecutor构造函数进行线程池的创建。
示例代码如下：
```java
		// 设置队列最大容量为10
		LinkedBlockingQueue<Runnable> lbq = new LinkedBlockingQueue(10);
		ExecutorService cachedThreadPool = new ThreadPoolExecutor(5, 5, 0L, TimeUnit.SECONDS, lbq);
```

接口的幂等性的概念
---------
幂等性是指重复使用同样的参数调用同一方法时总能获得同样的结果。比如对同一资源的GET请求访问结果都是一样的。

http://www.cnblogs.com/binyue/p/4015884.html


  
  
  

mysql的binlog
------------

java8的lambda表达式
---------------
lambda表达式的主要作用就是代替匿名内部类的繁琐语法
lambda表达式只能为函数式接口创建实例;
匿名内部类可以为任何接口创建实例,只要匿名内部类实现所有的抽象方法即可;
匿名内部类可以为抽象类身子普通类创建实例,但是lambda只能为函数式接口创建实例;
匿名内部类实现的抽象方法的方法体允许调用接口中定义的默认方法,但是lambda表达式不允许.

代码实例:
首先是函数式接口:
```java
public interface LambdaTest
{
	int add(int a,int b);
}
```

下面lambda的实现,以及main方法中的测试代码:
```java
	LambdaTest lambdaTest = (a,b)->a+b;
	System.out.println(lambdaTest.add(2, 3));
	LambdaTest lambdaTest1 = (a,b)->a-b;
	System.out.println(lambdaTest1.add(2, 3));
```
程序运行输出结果:
```java
5
-1
```

Java8的optional类
---------------
http://www.importnew.com/6675.html


这是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。
下面简要介绍该类的几个方法:
orElse
如果有值则将其返回，否则返回指定的其它值。源码如下:
```java
    /**
     * Return the value if present, otherwise return {@code other}.
     *
     * @param other the value to be returned if there is no value present, may
     * be null
     * @return the value, if present, otherwise {@code other}
     */
    public T orElse(T other) {
        return value != null ? value : other;
    }
```

map
map方法文档说明如下：
如果有值，则对其执行调用mapping函数得到返回值。如果返回值不为null，则创建包含mapping返回值的Optional作为map方法返回值，否则返回空Optional。
map方法用来对Optional实例的值执行一系列操作。通过一组实现了Function接口的lambda表达式传入操作。源码如下:
```java
/**
     * If a value is present, apply the provided mapping function to it,  
     * and if the result is non-null, return an {@code Optional} describing the
     * result.  Otherwise return an empty {@code Optional}.
     *
     * @apiNote This method supports post-processing on optional values, without
     * the need to explicitly check for a return status.  For example, the
     * following code traverses a stream of file names, selects one that has
     * not yet been processed, and then opens that file, returning an
     * {@code Optional<FileInputStream>}:
     *
     * <pre>{@code
     *     Optional<FileInputStream> fis =
     *         names.stream().filter(name -> !isProcessedYet(name))
     *                       .findFirst()
     *                       .map(name -> new FileInputStream(name));
     * }</pre>
     *
     * Here, {@code findFirst} returns an {@code Optional<String>}, and then
     * {@code map} returns an {@code Optional<FileInputStream>} for the desired
     * file if one exists.
     *
     * @param <U> The type of the result of the mapping function
     * @param mapper a mapping function to apply to the value, if present
     * @return an {@code Optional} describing the result of applying a mapping
     * function to the value of this {@code Optional}, if a value is present,
     * otherwise an empty {@code Optional}
     * @throws NullPointerException if the mapping function is null
     */
    public<U> Optional<U> map(Function<? super T, ? extends U> mapper) {
        Objects.requireNonNull(mapper);
        if (!isPresent())
            return empty();
        else {
            return Optional.ofNullable(mapper.apply(value));
        }
    }
```

filter

filter个方法通过传入限定条件对Optional实例的值进行过滤。文档描述如下：

如果有值并且满足断言条件返回包含该值的Optional，否则返回空Optional。

读到这里，可能你已经知道如何为filter方法传入一段代码。是的，这里可以传入一个lambda表达式。对于filter函数我们应该传入实现了Predicate接口的lambda表达式。
源码如下:
```java
 /**
     * If a value is present, and the value matches the given predicate,
     * return an {@code Optional} describing the value, otherwise return an
     * empty {@code Optional}.
     *
     * @param predicate a predicate to apply to the value, if present
     * @return an {@code Optional} describing the value of this {@code Optional}
     * if a value is present and the value matches the given predicate,
     * otherwise an empty {@code Optional}
     * @throws NullPointerException if the predicate is null
     */
    public Optional<T> filter(Predicate<? super T> predicate) {
        Objects.requireNonNull(predicate);
        if (!isPresent())
            return this;
        else
            return predicate.test(value) ? this : empty();
    }
```

mock测试springMVC的controller
--------------------------
https://www.cnblogs.com/xiaohunshi/p/5706943.html

redis哨兵模式
---------
它的**主要功能**有一下几点:
1、不时地监控redis是否按照预期良好地运行;
2、如果发现某个redis节点运行出现状况，能够通知另外一个进程(例如它的客户端);
3、能够进行自动切换。当一个master节点不可用时，能够选举出master的多个slave(如果有超过一个slave的话)中的一个来作为新的master,其它的slave节点会将它所追随的master的地址改为被提升为master的slave的新地址。
4、哨兵为客户端提供服务发现，客户端链接哨兵，哨兵提供当前master的地址然后提供服务，如果出现切换，也就是master挂了，哨兵会提供客户端一个新地址。

需要指出的是,哨兵（sentinel）本身也是支持**集群**的.


ServiceLoader作用及原理
------------------
https://blog.csdn.net/is_zhoufeng/article/details/50722440

在java中根据一个子类获取其父类或接口信息非常方便，但是根据一个接口获取该接口的所有实现类却没那么容易。 这个时候就可以使用ServiceLoader来实现上述需求。java本身提供了一种方式来获取一个接口的子类，那就是使用java.util.ServiceLoader#load(java.lang.Class)

方法，但是直接使用该方法也是不能获取到给定接口所有的子类的。

需要接口的子类以配置的方式主动注册到一个接口上，才能使用ServiceLoader进行加载到子类，并且**子类需要有一个无参构造方法**，用于被ServiceLoader进行实例化。

ServiceLoader的原理其实很简单，就是根据给定的参数（接口）就能定位到该接口与实现类的映射配置文件的路径了，然后读取该配置文件，就能获取到该接口的子类

使用实例：

 1. 编写Service

```java
package com.mogujie.uni.sl;
/**
 1. Created by laibao
 */
public interface Animal {
        void eat();
}   
```
 2. 编写实现类（注意：实现类不一定要与接口在同一个工程中，可以存在于其他的jar包中）
 ```java
 package com.mogujie.uni.sl;
/**
 * Created by laibao
 */
public class Pig implements Animal {
    @Override
    public void eat() {
        System.out.println("Pig eating...");
    }
}
 ```
 
 ```java
 package com.mogujie.uni.sl;
/**
 * Created by laibao
 */
public class Dog implements Animal {
    @Override
    public void eat() {
        System.out.println("Dog eating...");
    }
}
 ```
 
在实现类所在的工程的classpath下面的建立META-INF/services目录，该目录是固定的，一定要按照规定的名称去创建，该目录用于配置接口与实现类的映射关系 
```java
com.mogujie.uni.sl.Pig
com.mogujie.uni.sl.Dog
```

接下来就能使用ServiceLoader的方法获取com.mogujie.uni.sl.Animal接口的所有子类了。测试类如下：
```java
package com.mogujie.uni;
import com.mogujie.uni.sl.Animal;
import java.util.Iterator;
import java.util.ServiceLoader;
/**
 * Created by laibao
 */
public class TestServiceLoader {
    public static void main(String[] args) {
        ServiceLoader<Animal> serviceLoader = ServiceLoader.load(Animal.class);
        Iterator<Animal> animalIterator = serviceLoader.iterator();
        while(animalIterator.hasNext()){
            Animal animal = animalIterator.next();
            animal.eat();
        }
    }
}
```

测试输出：
```java
Pig eating...
Dog eating...
```

下面自己实现一个CustomServiceLoader与系统的ServiceLoader具有同样的功能
```java
package com.mogujie.uni;

import org.apache.commons.io.IOUtils;
import java.net.URL;
import java.util.Enumeration;
import java.util.LinkedList;
import java.util.List;

/**
 * Created by laibao
 */
public class CustomServiceLoader {

    public static final String MAPPING_CONFIG_PREFIX = "META-INF/services";

    public static <S> List<S> loade(Class<S> service) throws Exception{
        String mappingConfigFile = MAPPING_CONFIG_PREFIX + "/" + service.getName() ;
        //由于一个接口的实现类可能存在多个jar包中的META-INF目录下，所以下面使用getResources返回一个URL数组
        Enumeration<URL> configFileUrls =  CustomServiceLoader.class.getClassLoader().getResources(mappingConfigFile);
        if(configFileUrls == null){
            return null ;
        }
        List<S> services = new LinkedList<S>();
        while(configFileUrls.hasMoreElements()){
            URL configFileUrl = configFileUrls.nextElement();
            String configContent = IOUtils.toString(configFileUrl.openStream());
            String[] serviceNames = configContent.split("\n");
            for(String serviceName : serviceNames){
                Class serviceClass = CustomServiceLoader.class.getClassLoader().loadClass(serviceName);
                Object serviceInstance = serviceClass.newInstance();
                services.add((S)serviceInstance);
            }
        }
        return services ;
    }

}
```

测试类如下：
```java
package com.mogujie.uni;
import com.mogujie.uni.sl.Animal;
import java.util.List;
/**
 * Created by laibao
 */
public class CustomServiceLoaderTest {
    public static void main(String[] args) throws Exception {
        List<Animal> animals = CustomServiceLoader.loade(Animal.class);
        for (Animal animal : animals){
            animal.eat();
        }
    }
}
```

java Condition
--------------

Java中的锁
-------
http://ifeve.com/locks/

可重入锁

Java中的synchronized同步块是可重入的。这意味着如果一个java线程进入了代码中的synchronized同步块，并因此获得了该同步块使用的同步对象对应的管程上的锁，那么这个线程可以进入由同一个管程对象所同步的另一个java代码块。下面是一个例子：
```java
public class Reentrant{
    public synchronized outer(){
        inner();
    }
 
    public synchronized inner(){
        //do something
    }
}

```

注意outer()和inner()都被声明为synchronized，这在Java中和synchronized(this)块等效。如果一个线程调用了outer()，在outer()里调用inner()就没有什么问题，因为这两个方法（代码块）都由同一个管程对象（”this”)所同步。如果一个线程已经拥有了一个管程对象上的锁，那么它就有权访问被这个管程对象同步的所有代码块。这就是可重入。线程可以进入任何一个它已经拥有的锁所同步着的代码块。


java中的happen before原则
---------------------
http://ifeve.com/java-%E4%BD%BF%E7%94%A8-happen-before-%E8%A7%84%E5%88%99%E5%AE%9E%E7%8E%B0%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%E7%9A%84%E5%90%8C%E6%AD%A5%E6%93%8D%E4%BD%9C/

happen before（HB）原则，简单来说，java中一个动作的执行要先于另外一个动作，否则就会出错。

HB 有哪些规则？

 1. 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作；
 2. 锁定规则：在监视器锁上的解锁操作必须在同一个监视器上的加锁操作之前执行。
 3. volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作；
 4. 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C；
 5. 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作；
 6. 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生；
 7. 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行；
 8. 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始
 
 假设A HB C，那么A对共享变量的修改，对于C来说都是可见的（可见性）。可见性的实现是通过cache protocol（缓存一致性）和memory barrier（内存栅栏）实现的。
 
 例如下面的交替打印奇偶数的例子:
 ```java
/**
 * @desc 程序使用两个线程,实现奇偶数字的打印
 *
 */
public class HappenBefore {

	static int num = 0; // 没有使用volatile关键字修饰符
	static volatile boolean flag = false;

	public static void main(String[] args) {

		Thread t1 = new Thread(() -> {
			Thread.currentThread().setName("1号线程:");
			for (; 100 > num;) {
				if (!flag && (num == 0 || ++num % 2 == 0)) {
					System.out.println(Thread.currentThread().getName() + num);
					flag = true;
				}
			}
		});

		Thread t2 = new Thread(() -> {
			Thread.currentThread().setName("2号线程:");
			for (; 100 > num;) {
				if (flag && (++num % 2 != 0)) {
					System.out.println(Thread.currentThread().getName() + num);
					flag = false;
				}
			}
		});

		t1.start();
		t2.start();
	}
}
 ```
 
 虽然num 变量没有使用volatile,但是程序也可以正常输出0-100(注意**边界值**0和100也要输出来)的奇偶数.同样的,如果num变量使用volatile关键字修饰,但是flag变量没有使用volatile修饰,程序照样可以正常输出结果.程序控制台输出如下所示:
 ```java
 Picked up _JAVA_OPTIONS:   -Dawt.useSystemAAFontSettings=gasp
1号线程:0
2号线程:1
1号线程:2
2号线程:3
1号线程:4
2号线程:5
1号线程:6
2号线程:7
1号线程:8
2号线程:9
1号线程:10
2号线程:11
1号线程:12
2号线程:13
1号线程:14
2号线程:15
1号线程:16
2号线程:17
1号线程:18
2号线程:19
1号线程:20
2号线程:21
1号线程:22
2号线程:23
1号线程:24
2号线程:25
1号线程:26
2号线程:27
1号线程:28
2号线程:29
1号线程:30
2号线程:31
1号线程:32
2号线程:33
1号线程:34
2号线程:35
1号线程:36
2号线程:37
1号线程:38
2号线程:39
1号线程:40
2号线程:41
1号线程:42
2号线程:43
1号线程:44
2号线程:45
1号线程:46
2号线程:47
1号线程:48
2号线程:49
1号线程:50
2号线程:51
1号线程:52
2号线程:53
1号线程:54
2号线程:55
1号线程:56
2号线程:57
1号线程:58
2号线程:59
1号线程:60
2号线程:61
1号线程:62
2号线程:63
1号线程:64
2号线程:65
1号线程:66
2号线程:67
1号线程:68
2号线程:69
1号线程:70
2号线程:71
1号线程:72
2号线程:73
1号线程:74
2号线程:75
1号线程:76
2号线程:77
1号线程:78
2号线程:79
1号线程:80
2号线程:81
1号线程:82
2号线程:83
1号线程:84
2号线程:85
1号线程:86
2号线程:87
1号线程:88
2号线程:89
1号线程:90
2号线程:91
1号线程:92
2号线程:93
1号线程:94
2号线程:95
1号线程:96
2号线程:97
1号线程:98
2号线程:99
1号线程:100
 ```
 
 但是,如果num和flag都没有使用使用volatile关键字修饰的话,那输出会怎样?会卡住,即,输出到某个数字的时候,程序会一直处于等待状态.因为两个线程的共享变量的值,不知道什么时候会从线程的working-memory刷新到内存中.

注意,使用下面的程序也可以达到同样的目的. 
 ```java
/**
 * @desc 程序使用两个线程,实现奇偶数字的打印
 *
 */
public class HappenBefore {

	static volatile int num = 0; // 没有使用volatile关键字修饰符
	static boolean flag = false;

	public static void main(String[] args) {

		Thread t1 = new Thread(() -> {
			Thread.currentThread().setName("1号线程:");
			for (; 1001 > num;) {
				if ((num % 2 == 0) && num < 1001) {
					System.out.println(Thread.currentThread().getName() + num);
					num++;
				}
			}
		});

		Thread t2 = new Thread(() -> {
			Thread.currentThread().setName("2号线程:");
			for (; 1000 > num;) {
				if ((num % 2 != 0)) {
					System.out.println(Thread.currentThread().getName() + num);
					num++;
				}
			}
		});

		t1.start();
		t2.start();
	}
}
 ```
 
 注意t1的边界条件,如果写成1000的话,那么1000有**可能**会被打印出来,也可能不会.
 
 

volatile如何保证可见性
---------------
http://ifeve.com/from-singleton-happens-before/

 1. 可见性问题的由来
 
 大家都知道CPU的处理速度非常快，快到内存都无法跟上CPU的速度而且差距非常大，而这个地方不加以处理通常会成为CPU效率的瓶颈，为了消除速度差带来的影响，CPU通常自带了缓存：一级、二级甚至三级缓存（我们可以在电脑描述信息上面看到）。JVM也是出于同样的道理给每个线程分配了工作内存（Woking Memory，注意：不是主内存）。我们要知道线程对变量的修改都会反映到工作内存中，然后JVM找一个合适的时刻将工作内存上的更改同步到主内存中。正是由于线程更改变量到工作内存同步到主内存中存在一个时间差，所以这里会造成数据一致性问题，这就是可见性问题的由来。
 
 
 2. volatile采取的措施
 
 volatile采取的措施其实很好理解：只要被volatile修饰的变量被更改就立即同步到主内存，同时其它线程的工作内存中变量的值失效，使用时必须从主内存中读取。 换句话说，线程的工作内存“不缓存”被volatile修饰的变量。
 
信号量Semaphore
------------

https://www.cnblogs.com/dolphin0520/p/3920397.html

Semaphore可以控同时访问的线程个数，通过 acquire() 获取一个许可，如果没有就等待，而 release() 释放一个许可。
它的两个构造函数如下(注意参数permits可以为负数,此时表示执行任何的acquire操作之前,都必须先执行release操作):
```java
/**
     * Creates a {@code Semaphore} with the given number of
     * permits and nonfair fairness setting.
     *
     * @param permits the initial number of permits available.
     *        This value may be negative, in which case releases
     *        must occur before any acquires will be granted.
     */
    public Semaphore(int permits) {
        sync = new NonfairSync(permits);
    }

    /**
     * Creates a {@code Semaphore} with the given number of
     * permits and the given fairness setting.
     *
     * @param permits the initial number of permits available.
     *        This value may be negative, in which case releases
     *        must occur before any acquires will be granted.
     * @param fair {@code true} if this semaphore will guarantee
     *        first-in first-out granting of permits under contention,
     *        else {@code false}
     */
    public Semaphore(int permits, boolean fair) {
        sync = fair ? new FairSync(permits) : new NonfairSync(permits);
    }
```

下面说一下Semaphore类中比较重要的几个方法，首先是acquire()、release()方法：
```java
public void acquire() throws InterruptedException {  }     //获取一个许可
public void acquire(int permits) throws InterruptedException { }    //获取permits个许可
public void release() { }          //释放一个许可
public void release(int permits) { }    //释放permits个许可
```
需要注意的是,acquire方法是阻塞的,如果想要非阻塞的,那么可以使用下面的方法:
```java
public boolean tryAcquire() { };    //尝试获取一个许可，若获取成功，则立即返回true，若获取失败，则立即返回false
public boolean tryAcquire(long timeout, TimeUnit unit) throws InterruptedException { };  //尝试获取一个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false
public boolean tryAcquire(int permits) { }; //尝试获取permits个许可，若获取成功，则立即返回true，若获取失败，则立即返回false
public boolean tryAcquire(int permits, long timeout, TimeUnit unit) throws InterruptedException { }; //尝试获取permits个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false
```
此外,可以使用如下方法获取当前信号量中可获取的许可数量(jdk建议一般是在debug或者test时使用该方法):
```java
/**
     * Returns the current number of permits available in this semaphore.
     *
     * <p>This method is typically used for debugging and testing purposes.
     *
     * @return the number of permits available in this semaphore
     */
    public int availablePermits() {
        return sync.getPermits();
    }
```

下面照抄博客上的一个使用信号量进行同步控制的例子,假若一个工厂有5台机器，但是有8个工人，一台机器同时只能被一个工人使用，只有使用完了，其他工人才能继续使用。那么我们就可以通过Semaphore来实现：:
```java
public class TestSemaphore {
	public static void main(String[] args) {
		int N = 8; // 工人数
		Semaphore semaphore = new Semaphore(5); // 机器数目
		for (int i = 0; i < N; i++)
			new Worker(i, semaphore).start();
	}

	static class Worker extends Thread {
		private int num;
		private Semaphore semaphore;

		public Worker(int num, Semaphore semaphore) {
			this.num = num;
			this.semaphore = semaphore;
		}

		@Override
		public void run() {
			try {
				semaphore.acquire();
				System.out.println("工人" + this.num + "占用一个机器在生产...");
				Thread.sleep(2000);
				System.out.println("工人" + this.num + "释放出机器");
				semaphore.release();
			} catch (InterruptedException e) {
				e.printStackTrace();
			}
		}
	}
}
```
程序运行输出:
```java
工人0占用一个机器在生产...
工人1占用一个机器在生产...
工人2占用一个机器在生产...
工人3占用一个机器在生产...
工人5占用一个机器在生产...
工人0释放出机器
工人3释放出机器
工人2释放出机器
工人4占用一个机器在生产...
工人1释放出机器
工人5释放出机器
工人7占用一个机器在生产...
工人6占用一个机器在生产...
工人4释放出机器
工人7释放出机器
工人6释放出机器
```

CountDownLatch
--------------
https://www.cnblogs.com/dolphin0520/p/3920397.html#undefined

CountDownLatch一般用于实现线程之间的同步等待,即,一个线程是否继续执行,需要依靠其他几个线程是否执行完成.
该类的构造器如下:
```java
/**
     * Constructs a {@code CountDownLatch} initialized with the given count.
     *
     * @param count the number of times {@link #countDown} must be invoked
     *        before threads can pass through {@link #await}
     * @throws IllegalArgumentException if {@code count} is negative
     */
    public CountDownLatch(int count) {
        if (count < 0) throw new IllegalArgumentException("count < 0");
        this.sync = new Sync(count);
    }
```
然后下面这3个方法是CountDownLatch类中最重要的方法：
```java
public void await() throws InterruptedException { };   //调用await()方法的线程会被挂起，它会等待直到count值为0才继续执行
public boolean await(long timeout, TimeUnit unit) throws InterruptedException { };  //和await()类似，只不过等待一定的时间后count值还没变为0的话就会继续执行
public void countDown() { };  //将count值减1
```
下面照抄博客CountDownLatch的例子:
```java
public class TestCountDownLatch {
	public static void main(String[] args) {
		final CountDownLatch latch = new CountDownLatch(2);

		new Thread() {
			public void run() {
				try {
					System.out.println("子线程" + Thread.currentThread().getName() + "正在执行");
					Thread.sleep(3000);
					System.out.println("子线程" + Thread.currentThread().getName() + "执行完毕");
					latch.countDown();
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			};
		}.start();

		new Thread() {
			public void run() {
				try {
					System.out.println("子线程" + Thread.currentThread().getName() + "正在执行");
					Thread.sleep(3000);
					System.out.println("子线程" + Thread.currentThread().getName() + "执行完毕");
					latch.countDown();
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			};
		}.start();

		try {
			System.out.println("等待2个子线程执行完毕...");
			latch.await();  //这里执行等待操作
			System.out.println("2个子线程已经执行完毕");
			System.out.println("继续执行主线程");
		} catch (InterruptedException e) {
			e.printStackTrace();
		}
	}
}
```
控制台输出如下:
```java
子线程Thread-0正在执行
子线程Thread-1正在执行
等待2个子线程执行完毕...
子线程Thread-0执行完毕
子线程Thread-1执行完毕
2个子线程已经执行完毕
继续执行主线程
```

CyclicBarrier
-------------
https://www.cnblogs.com/dolphin0520/p/3920397.html#undefined

通过它可以实现让一组线程等待至某个状态之后再全部同时执行,需要注意的是CyclicBarrier是可以重用的.而CountDownLatch是不可以重用的,Semaphore也可以重用.
它的两个构造函数如下(注意,barrierAction是由最后一个达到parties数量的线程去执行的):
```java
/**
     * Creates a new {@code CyclicBarrier} that will trip when the
     * given number of parties (threads) are waiting upon it, and which
     * will execute the given barrier action when the barrier is tripped,
     * performed by the last thread entering the barrier.
     *
     * @param parties the number of threads that must invoke {@link #await}
     *        before the barrier is tripped
     * @param barrierAction the command to execute when the barrier is
     *        tripped, or {@code null} if there is no action
     * @throws IllegalArgumentException if {@code parties} is less than 1
     */
    public CyclicBarrier(int parties, Runnable barrierAction) {
        if (parties <= 0) throw new IllegalArgumentException();
        this.parties = parties;
        this.count = parties;
        this.barrierCommand = barrierAction;
    }

    /**
     * Creates a new {@code CyclicBarrier} that will trip when the
     * given number of parties (threads) are waiting upon it, and
     * does not perform a predefined action when the barrier is tripped.
     *
     * @param parties the number of threads that must invoke {@link #await}
     *        before the barrier is tripped
     * @throws IllegalArgumentException if {@code parties} is less than 1
     */
    public CyclicBarrier(int parties) {
        this(parties, null);
    }
```
然后CyclicBarrier中最重要的方法就是await方法，它有2个重载版本：
```java
public int await() throws InterruptedException, BrokenBarrierException { };
public int await(long timeout, TimeUnit unit)throws InterruptedException,BrokenBarrierException,TimeoutException { };
```

 1. 第一个版本比较常用，用来挂起当前线程，直至所有线程都到达barrier状态再同时执行后续任务；
 
 1. 第二个版本是让这些线程等待至一定的时间，如果还有线程没有到达barrier状态就直接让到达barrier的线程执行后续任务。
 
 一下是一些用例:
 
 假若有若干个线程都要进行写数据操作，并且只有所有线程都完成写数据操作之后，这些线程才能继续做后面的事情，此时就可以利用CyclicBarrier了：
 ```java
public class TestCyclicBarrier {
	public static void main(String[] args) {
		int N = 4;
		CyclicBarrier barrier = new CyclicBarrier(N); // 注意这种方式创建的CyclicBarrier对象,在达到指定的数量后,所有的线程都会继续执行各自后续的任务
		for (int i = 0; i < N; i++)
			new Writer(barrier).start();
	}

	static class Writer extends Thread {
		private CyclicBarrier cyclicBarrier;

		public Writer(CyclicBarrier cyclicBarrier) {
			this.cyclicBarrier = cyclicBarrier;
		}

		@Override
		public void run() {
			System.out.println("线程" + Thread.currentThread().getName() + "正在写入数据...");
			try {
				Thread.sleep(5000); // 以睡眠来模拟写入数据操作
				System.out.println("线程" + Thread.currentThread().getName() + "写入数据完毕，等待其他线程写入完毕");
				cyclicBarrier.await();
			} catch (InterruptedException e) {
				e.printStackTrace();
			} catch (BrokenBarrierException e) {
				e.printStackTrace();
			}
			System.out.println("所有线程写入完毕，继续处理其他任务...");
		}
	}
}
 ```
 
 控制台输出:
 ```java
线程Thread-0正在写入数据...
线程Thread-2正在写入数据...
线程Thread-1正在写入数据...
线程Thread-3正在写入数据...
线程Thread-0写入数据完毕，等待其他线程写入完毕
线程Thread-2写入数据完毕，等待其他线程写入完毕
线程Thread-1写入数据完毕，等待其他线程写入完毕
线程Thread-3写入数据完毕，等待其他线程写入完毕
所有线程写入完毕，继续处理其他任务...
所有线程写入完毕，继续处理其他任务...
所有线程写入完毕，继续处理其他任务...
所有线程写入完毕，继续处理其他任务...
 ```

如果说想在所有线程写入操作完之后，进行额外的其他操作可以为CyclicBarrier提供Runnable参数：
```java
public class TestCyclicBarrier2 {
	public static void main(String[] args) {
		int N = 4;
		
		//这种方式创建的CyclicBarrier对象,当达到指定的数量后,最后一个达到指定数量的线程获取执行指定的action操作
		CyclicBarrier barrier = new CyclicBarrier(N, new Runnable() {
			@Override
			public void run() {
				System.out.println("当前线程" + Thread.currentThread().getName());
			}
		});

		for (int i = 0; i < N; i++)
			new Writer(barrier).start();
	}

	static class Writer extends Thread {
		private CyclicBarrier cyclicBarrier;

		public Writer(CyclicBarrier cyclicBarrier) {
			this.cyclicBarrier = cyclicBarrier;
		}

		@Override
		public void run() {
			System.out.println("线程" + Thread.currentThread().getName() + "正在写入数据...");
			try {
				Thread.sleep(5000); // 以睡眠来模拟写入数据操作
				System.out.println("线程" + Thread.currentThread().getName() + "写入数据完毕，等待其他线程写入完毕");
				cyclicBarrier.await();
			} catch (InterruptedException e) {
				e.printStackTrace();
			} catch (BrokenBarrierException e) {
				e.printStackTrace();
			}
			System.out.println("所有线程写入完毕，继续处理其他任务...");
		}
	}
}
```
控制台输出如下:
```java
线程Thread-0正在写入数据...
线程Thread-1正在写入数据...
线程Thread-3正在写入数据...
线程Thread-2正在写入数据...
线程Thread-0写入数据完毕，等待其他线程写入完毕
线程Thread-1写入数据完毕，等待其他线程写入完毕
线程Thread-2写入数据完毕，等待其他线程写入完毕
线程Thread-3写入数据完毕，等待其他线程写入完毕
当前线程Thread-2
所有线程写入完毕，继续处理其他任务...
所有线程写入完毕，继续处理其他任务...
所有线程写入完毕，继续处理其他任务...
所有线程写入完毕，继续处理其他任务...
```
 下面看一下为await指定等待时间的效果：
 
 ```java
 public class TestCyclicBarrier3 {
	public static void main(String[] args) {
		int N = 4;
		CyclicBarrier barrier = new CyclicBarrier(N);

		for (int i = 0; i < N; i++) {
			if (i < N - 1)
				new Writer(barrier).start();
			else {
				try {
					Thread.sleep(5000);
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
				new Writer(barrier).start();
			}
		}
	}

	static class Writer extends Thread {
		private CyclicBarrier cyclicBarrier;

		public Writer(CyclicBarrier cyclicBarrier) {
			this.cyclicBarrier = cyclicBarrier;
		}

		@Override
		public void run() {
			System.out.println("线程" + Thread.currentThread().getName() + "正在写入数据...");
			try {
				Thread.sleep(5000); // 以睡眠来模拟写入数据操作
				System.out.println("线程" + Thread.currentThread().getName() + "写入数据完毕，等待其他线程写入完毕");
				try {
					cyclicBarrier.await(2000, TimeUnit.MILLISECONDS);
				} catch (TimeoutException e) {
					System.out.println(Thread.currentThread().getName() + "等待超时,抛出了TimeoutException异常");
					e.printStackTrace();
				}
			} catch (InterruptedException e) {
				e.printStackTrace();
			} catch (BrokenBarrierException e) {
				e.printStackTrace();
			}
			System.out.println(Thread.currentThread().getName() + "所有线程写入完毕，继续处理其他任务...");
		}
	}
}
 ```
 控制台输出如下:
 ```java
Picked up _JAVA_OPTIONS:   -Dawt.useSystemAAFontSettings=gasp
线程Thread-2正在写入数据...
线程Thread-0正在写入数据...
线程Thread-1正在写入数据...
线程Thread-0写入数据完毕，等待其他线程写入完毕
线程Thread-2写入数据完毕，等待其他线程写入完毕
线程Thread-1写入数据完毕，等待其他线程写入完毕
线程Thread-3正在写入数据...
Thread-1等待超时,抛出了TimeoutException异常
java.util.concurrent.BrokenBarrierException
Thread-0所有线程写入完毕，继续处理其他任务...
	at java.util.concurrent.CyclicBarrier.dowait(CyclicBarrier.java:250)
	at java.util.concurrent.CyclicBarrier.await(CyclicBarrier.java:435)
	at com.audi.TestCyclicBarrier3$Writer.run(TestCyclicBarrier3.java:41)
java.util.concurrent.TimeoutException
	at java.util.concurrent.CyclicBarrier.dowait(CyclicBarrier.java:257)
	at java.util.concurrent.CyclicBarrier.await(CyclicBarrier.java:435)
	at com.audi.TestCyclicBarrier3$Writer.run(TestCyclicBarrier3.java:41)
java.util.concurrent.BrokenBarrierExceptionThread-1所有线程写入完毕，继续处理其他任务...
	at java.util.concurrent.CyclicBarrier.dowait(CyclicBarrier.java:250)

	at java.util.concurrent.CyclicBarrier.await(CyclicBarrier.java:435)
	at com.audi.TestCyclicBarrier3$Writer.run(TestCyclicBarrier3.java:41)
Thread-2所有线程写入完毕，继续处理其他任务...
线程Thread-3写入数据完毕，等待其他线程写入完毕
java.util.concurrent.BrokenBarrierException
	at java.util.concurrent.CyclicBarrier.dowait(CyclicBarrier.java:207)
	at java.util.concurrent.CyclicBarrier.await(CyclicBarrier.java:435)
	at com.audi.TestCyclicBarrier3$Writer.run(TestCyclicBarrier3.java:41)
Thread-3所有线程写入完毕，继续处理其他任务...
 ```
 其他线程在等待超时的情况下,会选择一个线程(注意,并不是所有等待的线程都会抛出这个异常)抛出TimeoutException异常,然后继续执行后续的逻辑.
 
 
 下面,看一下CyclicBarrier的重用例子:
 ```java
 public class TestCyclicBarrier4 {
	public static void main(String[] args) {
		int N = 4;
		CyclicBarrier barrier = new CyclicBarrier(N);

		for (int i = 0; i < N; i++) {
			new Writer(barrier).start();
		}

		try {
			Thread.sleep(25000);
		} catch (InterruptedException e) {
			e.printStackTrace();
		}

		System.out.println("CyclicBarrier重用");

		for (int i = 0; i < N; i++) {
			new Writer(barrier).start();
		}
	}

	static class Writer extends Thread {
		private CyclicBarrier cyclicBarrier;

		public Writer(CyclicBarrier cyclicBarrier) {
			this.cyclicBarrier = cyclicBarrier;
		}

		@Override
		public void run() {
			System.out.println("线程" + Thread.currentThread().getName() + "正在写入数据...");
			try {
				Thread.sleep(5000); // 以睡眠来模拟写入数据操作
				System.out.println("线程" + Thread.currentThread().getName() + "写入数据完毕，等待其他线程写入完毕");
				cyclicBarrier.await();
			} catch (InterruptedException e) {
				e.printStackTrace();
			} catch (BrokenBarrierException e) {
				e.printStackTrace();
			}
			System.out.println(Thread.currentThread().getName() + "所有线程写入完毕，继续处理其他任务...");
		}
	}
}
 ```
 控制台输出如下:
 ```java
线程Thread-0正在写入数据...
线程Thread-1正在写入数据...
线程Thread-2正在写入数据...
线程Thread-3正在写入数据...
线程Thread-0写入数据完毕，等待其他线程写入完毕
线程Thread-1写入数据完毕，等待其他线程写入完毕
线程Thread-2写入数据完毕，等待其他线程写入完毕
线程Thread-3写入数据完毕，等待其他线程写入完毕
Thread-3所有线程写入完毕，继续处理其他任务...
Thread-1所有线程写入完毕，继续处理其他任务...
Thread-2所有线程写入完毕，继续处理其他任务...
Thread-0所有线程写入完毕，继续处理其他任务...
**CyclicBarrier重用**
线程Thread-5正在写入数据...
线程Thread-4正在写入数据...
线程Thread-7正在写入数据...
线程Thread-6正在写入数据...
线程Thread-5写入数据完毕，等待其他线程写入完毕
线程Thread-4写入数据完毕，等待其他线程写入完毕
线程Thread-7写入数据完毕，等待其他线程写入完毕
线程Thread-6写入数据完毕，等待其他线程写入完毕
Thread-6所有线程写入完毕，继续处理其他任务...
Thread-5所有线程写入完毕，继续处理其他任务...
Thread-7所有线程写入完毕，继续处理其他任务...
Thread-4所有线程写入完毕，继续处理其他任务...
 ```
 

Java中的同步容器类
-----------
https://www.cnblogs.com/dolphin0520/p/3933404.html



 

调试利器-SSH隧道
----------
https://segmentfault.com/a/1190000011846777

java 限流方法(单机\分布式)
-----------------

参考链接:https://www.cnblogs.com/clds/p/5850070.html
http://jinnianshilongnian.iteye.com/blog/2305117
https://crossoverjie.top/2018/04/28/sbc/sbc7-Distributed-Limit/

首先针对单机场景:
可以使用的限流算法大概有计数器限流,漏桶算法,令牌桶算法等.

 - 计数器限流算法
 
 简单来说就是:在固定的时间间隔内,只允许固定数量的访问请求.使用一张参考链接里的示意图:
 ![此处输入图片的描述][30]


  该算法虽然实现简单,但是有一个临界问题,具体如下图所示:
  ![此处输入图片的描述][31]


  在临界值附近,系统还是有可能接收到比自己处理能力范围内多的多的请求.为了解决这个问题 ,我们使用滑动窗口方法,这个类似于计算机网络里的滑动窗口的概念.示意图如下:
  ![此处输入图片的描述][32]


  思路其实就是将计数器的时间窗口细化,这样相对可以减小请求对系统的冲击,窗口划分的越细,请求的控制越严格,根据实际区块来选择时间窗口的长短.
  
 - 漏桶算法
 ![此处输入图片的描述][33]


  [1]: https://github.com/WQZ321123/learn/blob/master/image/collection/collection_map.png?raw=true
  [2]: https://github.com/WQZ321123/learn/blob/master/image/OOM/LeakSuspectsReport.png?raw=true
  [3]: https://github.com/WQZ321123/learn/blob/master/image/OOM/5.png?raw=true
  [4]: https://github.com/WQZ321123/learn/blob/master/image/OOM/6.png?raw=true
  [5]: https://github.com/WQZ321123/learn/blob/master/image/OOM/dominator_tree.png?raw=true
  [6]: https://github.com/WQZ321123/learn/blob/master/image/OOM/dominator_tree2.png?raw=true
  [7]: https://github.com/WQZ321123/learn/blob/master/image/mysql/MyISAM%E4%B8%AD%E7%9A%84B+Tree%E7%B4%A2%E5%BC%95_%E4%B8%BB%E7%B4%A2%E5%BC%95.png?raw=true
  [8]: https://github.com/WQZ321123/learn/blob/master/image/mysql/MyISAM%E4%B8%AD%E7%9A%84B+Tree%E7%B4%A2%E5%BC%95_%E5%89%AF%E7%B4%A2%E5%BC%95.png?raw=true
  [9]: https://github.com/WQZ321123/learn/blob/master/image/mysql/InnoDB%E4%B8%AD%E7%9A%84B+Tree%E7%B4%A2%E5%BC%95_%E4%B8%BB%E7%B4%A2%E5%BC%95.png?raw=true
  [10]: https://github.com/WQZ321123/learn/blob/master/image/mysql/InnoDB%E4%B8%AD%E7%9A%84B+Tree%E7%B4%A2%E5%BC%95_%E5%89%AF%E7%B4%A2%E5%BC%95.png?raw=true
  [11]: https://github.com/WQZ321123/learn/blob/master/image/mysql/bitmap_table.png?raw=true
  [12]: https://github.com/WQZ321123/learn/blob/master/image/mysql/bitmap_sex.png?raw=true
  [13]: https://github.com/WQZ321123/learn/blob/master/image/mysql/bitmap_marriage.png?raw=true
  [14]: https://github.com/WQZ321123/learn/blob/master/image/mysql/bitmap_match.png?raw=true
  [15]: http://images2015.cnblogs.com/blog/721070/201705/721070-20170504110246211-10684485.png
  [16]: https://github.com/WQZ321123/learn/blob/master/image/%E4%B8%BB%E5%86%85%E5%AD%98%E5%92%8C%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AD%98.jpg?raw=true
  [17]: https://github.com/WQZ321123/learn/blob/master/image/perm/remove_perm.png?raw=true
  [18]: https://github.com/WQZ321123/learn/blob/master/image/Throwable.png?raw=true
  [19]: https://github.com/WQZ321123/learn/blob/master/image/Throwable_result.png?raw=true
  [20]: https://github.com/WQZ321123/learn/blob/master/image/bigData/storm_arch.jpg?raw=true
  [21]: https://github.com/WQZ321123/learn/blob/master/image/bigData/stormVShadoop.jpg?raw=true
  [22]: https://github.com/WQZ321123/learn/blob/master/image/distributed/localMessageTable.png?raw=true
  [23]: https://github.com/WQZ321123/learn/blob/master/image/distributed/mq_append.png?raw=true
  [24]: https://github.com/WQZ321123/learn/blob/master/image/distributed/mq_transcation.png?raw=true
  [25]: https://github.com/WQZ321123/learn/blob/master/image/distributed/success.png?raw=true
  [26]: https://github.com/WQZ321123/learn/blob/master/image/distributed/fail.png?raw=true
  [27]: https://github.com/WQZ321123/learn/blob/master/image/distributed/zookeeper_lock.png?raw=true
  [28]: https://github.com/WQZ321123/learn/blob/master/image/redis/redis_cluster.png?raw=true
  [29]: https://github.com/WQZ321123/learn/blob/master/image/other/interger%E5%8C%85%E8%A3%85%E7%B1%BB.png?raw=true
  [30]: https://github.com/WQZ321123/learn/blob/master/image/rateLimit/countLimit.jpg?raw=true
  [31]: https://github.com/WQZ321123/learn/blob/master/image/rateLimit/countLimitProblem.jpg?raw=true
  [32]: https://github.com/WQZ321123/learn/blob/master/image/rateLimit/slideWindow.jpg?raw=true
  [33]: https://github.com/WQZ321123/learn/blob/master/image/rateLimit/leadBucket.png?raw=true
  
  思想也很简单,就是先将系统的请求统一的收集起来,后端在处理的时候再统一按照恒定的速率获取请求,进行相应的处理.代码实现的话,可以使用有界队列来存放请求,也可以使用ActiveMQ消息队列来实现.
  
 - 令牌桶算法
 
令牌桶限流算法与漏桶限流算法很类似,只是令牌桶算法在决定是否处理请求的时候,需要依靠能否获取到令牌来决定.令牌(token)以固定的速率增加,每个请求的处理也会从桶中移除固定数量的token.
Guava框架提供了令牌桶算法实现，可直接拿来使用。Guava RateLimiter提供了令牌桶算法实现：平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)实现。
```java

SmoothBursty

RateLimiter limiter = RateLimiter.create(5);
System.out.println(limiter.acquire());
System.out.println(limiter.acquire());
System.out.println(limiter.acquire());
System.out.println(limiter.acquire());
System.out.println(limiter.acquire());
System.out.println(limiter.acquire());
```

控制台输出如下:
```java
0.0
0.194005
0.194746
0.199362
0.200081
```
RateLimiter limiter = RateLimiter.create(5);表示桶初始容量为5,且一秒新增5个令牌,即200ms新增一个令牌.
**SmoothBursty允许一定程度的突发**，会有人担心如果允许这种突发，假设突然间来了很大的流量，那么系统很可能扛不住这种突发。因此需要一种平滑速率的限流工具，从而系统冷启动后慢慢的趋于平均固定速率（即刚开始速率小一些，然后慢慢趋于我们设置的固定速率）。Guava也提供了SmoothWarmingUp来实现这种需求，其可以认为是漏桶算法，但是在某些特殊场景又不太一样。
SmoothWarmingUp创建方式：RateLimiter.create(doublepermitsPerSecond, long warmupPeriod, TimeUnit unit)
permitsPerSecond表示每秒新增的令牌数，warmupPeriod表示在从冷启动速率过渡到平均速率的时间间隔。
示例代码如下:
```java
RateLimiter limiter1 = RateLimiter.create(5, 1000, TimeUnit.MILLISECONDS);
		for (int i = 0; i < 5; i++) {
			System.out.println(limiter1.acquire());
		}
		try {
			Thread.sleep(1000L);
		} catch (InterruptedException e) {
			e.printStackTrace();
		}
		for (int i = 0; i < 5; i++) {
			System.out.println(limiter1.acquire());
		}
```
控制台输出如下:
```java
0.0
0.519917
0.359702
0.219499
0.2002
0.0
0.360285
0.220173
0.199885
0.19952
```

 - 分布式限流

前面都是应用级单机限流,如果要做到分布式限流,最关键的是要将限流服务做成原子化，而解决方案可以使使用redis+lua或者nginx+lua技术进行实现，通过这两种技术可以实现的高并发和高性能。

 - redis+lua分布式限流
 
 首先我们来使用redis+lua实现时间窗内某个接口的请求数限流，实现了该功能后可以改造为限流总并发/请求数和限制总资源数。Lua本身就是一种编程语言，也可以使用它实现复杂的令牌桶或漏桶算法。
 lua脚本如下:
 ```lua
 local key = KEYS[1] --限流KEY（一秒一个）
local limit = tonumber(ARGV[1]) --限流大小
local current = tonumber(redis.call('get', key) or "0")
if current + 1 > limit then --如果超出限流大小
    return 0
else --请求数+1，并设置2秒过期
    redis.call("INCRBY", key,"1")
    redis.call("expire", key,"2")
    return 1
end
 ```
 如下是Java中判断是否需要限流的代码：
 ```java
 public static boolean acquire() throws Exception {
String luaScript = Files.toString(new File("limit.lua"), Charset.defaultCharset());
Jedis jedis = new Jedis("192.168.147.52", 6379);
String key = "ip:" + System.currentTimeMillis()/ 1000; //此处将当前时间戳取秒数
Stringlimit = "3"; //限流大小
return (Long)jedis.eval(luaScript,Lists.newArrayList(key), Lists.newArrayList(limit)) == 1;
}
 ```
 其中,Lists.newArrayList(key), Lists.newArrayList(limit)会作为参数传入lua脚本.
 
 因为Redis的限制（Lua中有写操作不能使用带随机性质的读操作，如TIME）不能在Redis Lua中使用TIME获取时间戳，因此只好从应用获取然后传入，在某些极端情况下（机器时钟不准的情况下），限流会存在一些小问题。
 
 
 - Nginx+Lua实现限流：

 ```lua
 local locks = require "resty.lock"
local function acquire()
    local lock =locks:new("locks")
    local elapsed, err =lock:lock("limit_key") --互斥锁
    local limit_counter =ngx.shared.limit_counter --计数器
    local key = "ip:" ..os.time()
    local limit = 5 --限流大小
    local current =limit_counter:get(key)

    if current ~= nil and current + 1> limit then --如果超出限流大小
        lock:unlock()
        return 0
    end
    if current == nil then
        limit_counter:set(key, 1, 1) --第一次需要设置过期时间，设置key的值为1，过期时间为1秒
    else
        limit_counter:incr(key, 1) --第二次开始加1即可
    end
    lock:unlock()
    return 1
end
ngx.print(acquire())
 ```
 实现中我们需要使用lua-resty-lock互斥锁模块来解决原子性问题(在实际工程中使用时请考虑获取锁的超时问题)，并使用ngx.shared.DICT共享字典来实现计数器。如果需要限流则返回0，否则返回1。使用时需要先定义两个共享字典（分别用来存放锁和计数器数据）：

Java代码

```java
http {  
    ……  
    lua_shared_dict locks 10m;  
    lua_shared_dict limit_counter 10m;  
}  
```
有人会纠结如果应用并发量非常大那么redis或者nginx是不是能抗得住；不过这个问题要从多方面考虑：你的流量是不是真的有这么大，是不是可以通过一致性哈希将分布式限流进行分片，是不是可以当并发量太大降级为应用级限流；对策非常多，可以根据实际情况调节；像在京东使用Redis+Lua来限流抢购流量，一般流量是没有问题的。
 
对于分布式限流目前遇到的场景是业务上的限流，而不是流量入口的限流；流量入口限流应该在接入层完成，而接入层笔者(博客作者)一般使用Nginx。

 

Callable、Future和FutureTask
--------------------------
https://www.cnblogs.com/dolphin0520/p/3949310.html

 - Callable与Runnable
 
 Runnable接口run()方法返回值为空,所以在执行完任务之后无法返回任何结果。
 
 Callable位于java.util.concurrent包下，它也是一个接口，在它里面也只声明了一个方法，只不过这个方法叫做call()：
 ```java
 @FunctionalInterface
 public interface Callable<V> {
    /**
     * Computes a result, or throws an exception if unable to do so.
     *
     * @return computed result
     * @throws Exception if unable to compute a result
     */
    V call() throws Exception;
}
 ```
 可以看到，这是一个泛型接口，call()函数返回的类型就是传递进来的V类型。
 那么怎么使用Callable呢？一般情况下是配合ExecutorService来使用的，在ExecutorService接口中声明了若干个submit方法的重载版本：
 ```java
 <T> Future<T> submit(Callable<T> task);
<T> Future<T> submit(Runnable task, T result);
Future<?> submit(Runnable task);
 ```
 第一个submit方法里面的参数类型就是Callable。
 一般情况下我们使用第一个submit方法和第三个submit方法，第二个submit方法很少使用。


java 几种任务调度方法(框架)原理及对比
======================

Java Timer和TimerTask
--------------------
https://www.cnblogs.com/dolphin0520/p/3938991.html

其实就Timer来讲就是一个调度器,而TimerTask呢只是一个实现了run方法的一个类,而具体的TimerTask需要由你自己来实现,例如这样:
```java
Timer timer = new Timer();
timer.schedule(new TimerTask() {
        public void run() {
            System.out.println("abc");
        }
}, 20000 , 1000);
```
TimerTask的任务会在20s以后开始执行,每1秒循环执行一次.

先看一下Timer类的几个调度方法:
```java
/**
     * Schedules the specified task for execution at the specified time.  If
     * the time is in the past, the task is scheduled for immediate execution.
     *
     * @param task task to be scheduled.
     * @param time time at which task is to be executed.
     * @throws IllegalArgumentException if <tt>time.getTime()</tt> is negative.
     * @throws IllegalStateException if task was already scheduled or
     *         cancelled, timer was cancelled, or timer thread terminated.
     * @throws NullPointerException if {@code task} or {@code time} is null
     */
    public void schedule(TimerTask task, Date time) {
        sched(task, time.getTime(), 0);
    }

    /**
     * Schedules the specified task for repeated <i>fixed-delay execution</i>,
     * beginning after the specified delay.  Subsequent executions take place
     * at approximately regular intervals separated by the specified period.
     *
     * <p>In fixed-delay execution, each execution is scheduled relative to
     * the actual execution time of the previous execution.  If an execution
     * is delayed for any reason (such as garbage collection or other
     * background activity), subsequent executions will be delayed as well.
     * In the long run, the frequency of execution will generally be slightly
     * lower than the reciprocal of the specified period (assuming the system
     * clock underlying <tt>Object.wait(long)</tt> is accurate).
     *
     * <p>Fixed-delay execution is appropriate for recurring activities
     * that require "smoothness."  In other words, it is appropriate for
     * activities where it is more important to keep the frequency accurate
     * in the short run than in the long run.  This includes most animation
     * tasks, such as blinking a cursor at regular intervals.  It also includes
     * tasks wherein regular activity is performed in response to human
     * input, such as automatically repeating a character as long as a key
     * is held down.
     *
     * @param task   task to be scheduled.
     * @param delay  delay in milliseconds before task is to be executed.
     * @param period time in milliseconds between successive task executions.
     * @throws IllegalArgumentException if {@code delay < 0}, or
     *         {@code delay + System.currentTimeMillis() < 0}, or
     *         {@code period <= 0}
     * @throws IllegalStateException if task was already scheduled or
     *         cancelled, timer was cancelled, or timer thread terminated.
     * @throws NullPointerException if {@code task} is null
     */
    public void schedule(TimerTask task, long delay, long period) {
        if (delay < 0)
            throw new IllegalArgumentException("Negative delay.");
        if (period <= 0)
            throw new IllegalArgumentException("Non-positive period.");
        sched(task, System.currentTimeMillis()+delay, -period);
    }

    /**
     * Schedules the specified task for repeated <i>fixed-delay execution</i>,
     * beginning at the specified time. Subsequent executions take place at
     * approximately regular intervals, separated by the specified period.
     *
     * <p>In fixed-delay execution, each execution is scheduled relative to
     * the actual execution time of the previous execution.  If an execution
     * is delayed for any reason (such as garbage collection or other
     * background activity), subsequent executions will be delayed as well.
     * In the long run, the frequency of execution will generally be slightly
     * lower than the reciprocal of the specified period (assuming the system
     * clock underlying <tt>Object.wait(long)</tt> is accurate).  As a
     * consequence of the above, if the scheduled first time is in the past,
     * it is scheduled for immediate execution.
     *
     * <p>Fixed-delay execution is appropriate for recurring activities
     * that require "smoothness."  In other words, it is appropriate for
     * activities where it is more important to keep the frequency accurate
     * in the short run than in the long run.  This includes most animation
     * tasks, such as blinking a cursor at regular intervals.  It also includes
     * tasks wherein regular activity is performed in response to human
     * input, such as automatically repeating a character as long as a key
     * is held down.
     *
     * @param task   task to be scheduled.
     * @param firstTime First time at which task is to be executed.
     * @param period time in milliseconds between successive task executions.
     * @throws IllegalArgumentException if {@code firstTime.getTime() < 0}, or
     *         {@code period <= 0}
     * @throws IllegalStateException if task was already scheduled or
     *         cancelled, timer was cancelled, or timer thread terminated.
     * @throws NullPointerException if {@code task} or {@code firstTime} is null
     */
    public void schedule(TimerTask task, Date firstTime, long period) {
        if (period <= 0)
            throw new IllegalArgumentException("Non-positive period.");
        sched(task, firstTime.getTime(), -period);
    }

    /**
     * Schedules the specified task for repeated <i>fixed-rate execution</i>,
     * beginning after the specified delay.  Subsequent executions take place
     * at approximately regular intervals, separated by the specified period.
     *
     * <p>In fixed-rate execution, each execution is scheduled relative to the
     * scheduled execution time of the initial execution.  If an execution is
     * delayed for any reason (such as garbage collection or other background
     * activity), two or more executions will occur in rapid succession to
     * "catch up."  In the long run, the frequency of execution will be
     * exactly the reciprocal of the specified period (assuming the system
     * clock underlying <tt>Object.wait(long)</tt> is accurate).
     *
     * <p>Fixed-rate execution is appropriate for recurring activities that
     * are sensitive to <i>absolute</i> time, such as ringing a chime every
     * hour on the hour, or running scheduled maintenance every day at a
     * particular time.  It is also appropriate for recurring activities
     * where the total time to perform a fixed number of executions is
     * important, such as a countdown timer that ticks once every second for
     * ten seconds.  Finally, fixed-rate execution is appropriate for
     * scheduling multiple repeating timer tasks that must remain synchronized
     * with respect to one another.
     *
     * @param task   task to be scheduled.
     * @param delay  delay in milliseconds before task is to be executed.
     * @param period time in milliseconds between successive task executions.
     * @throws IllegalArgumentException if {@code delay < 0}, or
     *         {@code delay + System.currentTimeMillis() < 0}, or
     *         {@code period <= 0}
     * @throws IllegalStateException if task was already scheduled or
     *         cancelled, timer was cancelled, or timer thread terminated.
     * @throws NullPointerException if {@code task} is null
     */
    public void scheduleAtFixedRate(TimerTask task, long delay, long period) {
        if (delay < 0)
            throw new IllegalArgumentException("Negative delay.");
        if (period <= 0)
            throw new IllegalArgumentException("Non-positive period.");
        sched(task, System.currentTimeMillis()+delay, period);
    }
```

需要注意的是:
```java
public void schedule(TimerTask task, long delay, long period)

public void schedule(TimerTask task, Date firstTime, long period)

public void scheduleAtFixedRate(TimerTask task, long delay, long period)
```
两个schedule方法,如果上一次的调度出现了延迟(比如GC或者其他后台任务),那么后续的调度也会相应的等待.
但是scheduleAtFixedRate方法不会,它会严格按照指定的period周期进行调度,不会因为上一次调度出现等待,而重新计算下一次调度的时间.
其实,查看schedule和scheduleAtFixedRate二者的实现,他们底层都会调用sched方法,只不过在参数传递的时候,schedule传递的是**-period**,scheduleAtFixedRate传递的是period.
下面看一下sched方法的实现源码:
```java
 /**
     * Schedule the specified timer task for execution at the specified
     * time with the specified period, in milliseconds.  If period is
     * positive, the task is scheduled for repeated execution; if period is
     * zero, the task is scheduled for one-time execution. Time is specified
     * in Date.getTime() format.  This method checks timer state, task state,
     * and initial execution time, but not period.
     *
     * @throws IllegalArgumentException if <tt>time</tt> is negative.
     * @throws IllegalStateException if task was already scheduled or
     *         cancelled, timer was cancelled, or timer thread terminated.
     * @throws NullPointerException if {@code task} is null
     */
    private void sched(TimerTask task, long time, long period) {
        if (time < 0)
            throw new IllegalArgumentException("Illegal execution time.");

        // Constrain value of period sufficiently to prevent numeric
        // overflow while still being effectively infinitely large.
        if (Math.abs(period) > (Long.MAX_VALUE >> 1))
            period >>= 1;

        synchronized(queue) {
            if (!thread.newTasksMayBeScheduled)
                throw new IllegalStateException("Timer already cancelled.");

            synchronized(task.lock) {
                if (task.state != TimerTask.VIRGIN)
                    throw new IllegalStateException(
                        "Task already scheduled or cancelled");
                task.nextExecutionTime = time;
                task.period = period;
                task.state = TimerTask.SCHEDULED;
            }

            queue.add(task);
            if (queue.getMin() == task)
                queue.notify();
        }
    }
```
上述方法主要将的就是将task加入到队列queue中.下面看一下queue的定义:
```java
/**
 * This class represents a timer task queue: a priority queue of TimerTasks,
 * ordered on nextExecutionTime.  Each Timer object has one of these, which it
 * shares with its TimerThread.  Internally this class uses a heap, which
 * offers log(n) performance for the add, removeMin and rescheduleMin
 * operations, and constant time performance for the getMin operation.
 */
class TaskQueue {
    /**
     * Priority queue represented as a balanced binary heap: the two children
     * of queue[n] are queue[2*n] and queue[2*n+1].  The priority queue is
     * ordered on the nextExecutionTime field: The TimerTask with the lowest
     * nextExecutionTime is in queue[1] (assuming the queue is nonempty).  For
     * each node n in the heap, and each descendant of n, d,
     * n.nextExecutionTime <= d.nextExecutionTime.
     */
    private TimerTask[] queue = new TimerTask[128];

    /**
     * The number of tasks in the priority queue.  (The tasks are stored in
     * queue[1] up to queue[size]).
     */
    private int size = 0;
```

可见，TaskQueue的结构很简单，为一个数组，加一个size，有点像ArrayList，是不是长度就128呢，当然不是，ArrayList可以扩容，它可以，只是会造成内存拷贝而已，所以一个Timer来讲，只要内部的task个数不超过128是不会造成扩容的；内部提供了add(TimerTask)、size()、getMin()、get(int)、removeMin()、quickRemove(int)、rescheduleMin(long newTime)、isEmpty()、clear()、fixUp()、fixDown()、heapify()；

对于fixUp和fixDown方法来讲，前者是当新增一个task的时候，首先将元素放在队列的尾部，然后向前找是否有比自己还要晚执行的任务，如果有，就将两个任务的顺序进行交换一下。而fixDown正好相反，执行完第一个任务后，需要加上一个时间片得到下一次执行时间，从而需要将其顺序与后面的任务进行对比下。

下面是fixDown的源码:
```java
private void fixDown(int k) {
       int j;
       while ((j = k << 1) <= size && j > 0) {
           if (j < size &&
               queue[j].nextExecutionTime > queue[j+1].nextExecutionTime)
               j++; // j indexes smallest kid
           if (queue[k].nextExecutionTime <= queue[j].nextExecutionTime)
               break;
           TimerTask tmp = queue[j];  queue[j] = queue[k]; queue[k] = tmp;
           k = j;
       }
   }
```
这种方式并非排序，而是找到一个合适的位置来交换，因为并不是通过队列逐个找的，而是每次移动一个二进制位，例如传入1的时候，接下来就是2、4、8、16这些位置，找到合适的位置放下即可，顺序未必是完全有序的，它只需要看到距离调度部分的越近的是有序性越强的时候就可以了，这样即可以保证一定的顺序性，达到较好的性能。

下面看一下cancel方法:
```java
 /**
     * Terminates this timer, discarding any currently scheduled tasks.
     * Does not interfere with a currently executing task (if it exists).
     * Once a timer has been terminated, its execution thread terminates
     * gracefully, and no more tasks may be scheduled on it.
     *
     * <p>Note that calling this method from within the run method of a
     * timer task that was invoked by this timer absolutely guarantees that
     * the ongoing task execution is the last task execution that will ever
     * be performed by this timer.
     *
     * <p>This method may be called repeatedly; the second and subsequent
     * calls have no effect.
     */
    public void cancel() {
        synchronized(queue) {
            thread.newTasksMayBeScheduled = false;
            queue.clear();
            queue.notify();  // In case queue was already empty.
        }
    }
```
从注释中可以看出,这个结束当前调度的task任务,且是优雅终止,即不会终止当前正在执行的任务.

当你对很多Task做了cancel操作后，此时通过调用purge方法实现对这些cancel掉的类空间的回收，上面已经提到，此时会造成顺序混乱，所以需要调用队里的heapify方法来完成顺序的重排(heapify，其实就是将队列的后半截，全部做一次fixeDown的操作，这个操作主要是为了回补quickRemove方法，当大量的quickRmove后，顺序被打乱后，此时将一半的区域做一次非常简单的排序即可。)，源码如下：
```java
/**
     * Removes all cancelled tasks from this timer's task queue.  <i>Calling
     * this method has no effect on the behavior of the timer</i>, but
     * eliminates the references to the cancelled tasks from the queue.
     * If there are no external references to these tasks, they become
     * eligible for garbage collection.
     *
     * <p>Most programs will have no need to call this method.
     * It is designed for use by the rare application that cancels a large
     * number of tasks.  Calling this method trades time for space: the
     * runtime of the method may be proportional to n + c log n, where n
     * is the number of tasks in the queue and c is the number of cancelled
     * tasks.
     *
     * <p>Note that it is permissible to call this method from within a
     * a task scheduled on this timer.
     *
     * @return the number of tasks removed from the queue.
     * @since 1.5
     */
     public int purge() {
         int result = 0;

         synchronized(queue) {
             for (int i = queue.size(); i > 0; i--) {
                 if (queue.get(i).state == TimerTask.CANCELLED) {
                     queue.quickRemove(i);
                     result++;
                 }
             }

             if (result != 0)
                 queue.heapify();
         }

         return result;
     }
```

下面看一下TimerThread类的mainLoop方法:
```java
/**
     * The main timer loop.  (See class comment.)
     */
    private void mainLoop() {
        while (true) {
            try {
                TimerTask task;
                boolean taskFired;
                synchronized(queue) {
                    // Wait for queue to become non-empty
                    while (queue.isEmpty() && newTasksMayBeScheduled)
                        queue.wait();
                    if (queue.isEmpty())
                        break; // Queue is empty and will forever remain; die

                    // Queue nonempty; look at first evt and do the right thing
                    long currentTime, executionTime;
                    task = queue.getMin();
                    synchronized(task.lock) {
                        if (task.state == TimerTask.CANCELLED) {
                            queue.removeMin();
                            continue;  // No action required, poll queue again
                        }
                        currentTime = System.currentTimeMillis();
                        executionTime = task.nextExecutionTime;
                        if (taskFired = (executionTime<=currentTime)) {
                            if (task.period == 0) { // Non-repeating, remove
                                queue.removeMin();
                                task.state = TimerTask.EXECUTED;
                            } else { // Repeating task, reschedule
                                queue.rescheduleMin(
                                  task.period<0 ? currentTime   - task.period
                                                : executionTime + task.period);
                            }
                        }
                    }
                    if (!taskFired) // Task hasn't yet fired; wait
                        queue.wait(executionTime - currentTime);
                }
                if (taskFired)  // Task fired; run it, holding no locks
                    task.run();
            } catch(InterruptedException e) {
            }
        }
    }
```

需要注意一下代码中的task.period这个值,这个值为0的时候表示任务只执行一次.当为正数的时候,就相当于是scheduleAtFixedRate方法在调度;当为负数的时候,就相当于是schedule方法.

另外:
```java
if (taskFired) // Task fired; run it, holding no locks
    task.run();
```
表明线程的执行,是通过单线程方式来执行的.为了达到多线程调度的要求,我们可以使用多个Timer进行调度,但是一般为了达到多线程调度的目的,我们会使用Executors.newScheduledThreadPool来进行任务调度.

二：TimerTask的cancel方法是取消单个任务的执行，即将其状态置为CANCELLED，这样在调用Timer的purge方法时，会将任务队列中状态为CANCELLED的任务清除，并对最小堆进行重排序。
三：任务队列是用最小堆实现的，具体是：用一个数组实现最小堆，下标从1开始。关于这种实现方式可以参考Mark Allen Weiss的【数据结构与算法分析 java语言描述】的最小堆一节。









