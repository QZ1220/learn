# 2022学习笔记


---
* [携程最终一致和强一致性缓存实践](#携程最终一致和强一致性缓存实践)
   * [最终一致性分布式缓存场景](#最终一致性分布式缓存场景)
      * [场景描述](#场景描述)
      * [整体方案](#整体方案)
      * [数据准确性设计](#数据准确性设计)
         * [并发控制](#并发控制)
         * [基于updateTime的更新顺序控制](#基于updatetime的更新顺序控制)
      * [数据完整性设计](#数据完整性设计)
      * [系统可用性保证](#系统可用性保证)
* [IM系统](#im系统)
* [Spring IOC容器是什么样的数据结构](#spring-ioc容器是什么样的数据结构)
* [Spring如何防止bean重复加载](#spring如何防止bean重复加载)
   * [什么是XSS攻击](#什么是xss攻击)
   * [什么是CSRF攻击](#什么是csrf攻击)
   * [什么是SSRF攻击](#什么是ssrf攻击)
* [对于http协议，服务端如何知道客户端的请求数据已经发送完成](#对于http协议服务端如何知道客户端的请求数据已经发送完成)
* [mysql联合索引的数据结构是什么](#mysql联合索引的数据结构是什么)
* [mysql join操作底层原理](#mysql-join操作底层原理)
* [java 新建对象的方式](#java-新建对象的方式)
* [为什么说hashmap线程不安全](#为什么说hashmap线程不安全)
* [如何理解池化资源](#如何理解池化资源)
* [kafka的消息是pull还是push模式](#kafka的消息是pull还是push模式)
* [如何让时间最短的线程停止后，安全的通知其他线程停止](#如何让时间最短的线程停止后安全的通知其他线程停止)
* [双写](#双写)
   * [迁移过程需要满足的目标](#迁移过程需要满足的目标)
   * [同步双写](#同步双写)
      * [最容易出现的问题](#最容易出现的问题)


# 携程最终一致和强一致性缓存实践

- https://mp.weixin.qq.com/s/E-chAZyHtaZOdA19mW59-Q

文章主要分为两个方面（最终一致性分布式缓存场景、强一致性分布式缓存场景）来讲解携程是如何使用缓存（redis）来降低mysql等db层面的查询压力的。

## 最终一致性分布式缓存场景

### 场景描述

经过几年演进，携程金融形成了自顶向下的多层次系统架构，如业务层、平台层、基础服务层等，其中用户信息、产品信息、订单信息等基础数据由基础平台等底层系统产生，服务于所有的金融系统，对这部分基础数据我们引入了统一的缓存服务（系统名utag），缓存数据有三大特点：**全量、准实时、永久有效**，在数据实时性要求不高的场景下，业务系统可直接调用统一的缓存查询接口。

我们的典型使用场景有：风控流程、APP入口信息提示等，而对数据一致性要求高的场景依然需要走实时的业务接口查询。引入缓存前后系统架构对比如下：

![1](1.png)

由于引入了缓存，势必会带来以下一些好处：查询的效率显著提高，查询接口层面可以做到统一，缓解底层基础服务的查询压力。

但是，也因为引入了缓存会增加系统的整体复杂度以及数据一致性问题。接下来主要从如下三个方面来讨论。

- 数据准确性：DB中单条数据的更新一定要准确同步到缓存服务。
- 数据完整性：将对应DB表的全量数据进行缓存且永久有效，从而可以替代对应的DB查询。
- 系统可用性：我们多个产品线的多个核心服务都已经接入，utag的高可用性显的尤为关键。

接下来先说明统一缓存服务的整体方案，再逐一介绍此三个关键特性的设计实现方案。

### 整体方案

简单来说就是，缓存集群服务多地部署，并且缓存更新的触发源存在多个，实现互补。缓存的更新通过mq通知到各集群。如下图所示：

![2](2.png)

### 数据准确性设计

从上图我们，可以知道触发源存在多个。不同的触发源，对缓存更新过程是一样的，整个更新步骤可抽象为4步：

- step1：触发更新，查询DB中的新数据，并发送统一的MQ
- step2：接收MQ，查询缓存中的老数据
- step3：新老数据对比，判断是否需要更新
- step4：若需要，则更新缓存

由于我们业务的大部分核心系统和所有的DB都在A地机房，所以触发源（如binlog的消费、业务MQ的接收、扫表任务的执行）都在A侧，触发更新后，第一步查询DB数据也只能在A侧查询（避免跨网络专线的数据库连接，影响性能）。查询到新数据后，发送更新缓存的MQ，两地机房的utag服务进行消费，之后进行统一的缓存更新流程。总体的缓存更新方案如下图所示：

![3](3.png)

由于有多个触发源，不同的触发源之间可能会对同一条数据的缓存更新请求出现并发，此外可能出现同一条数据在极短时间内（如1秒内）更新多次，无法区分数据更新顺序，因此需要做两方面的操作来确保数据更新的准确性。

#### 并发控制

若一条DB数据出现了多次更新，且刚好被不同的触发源触发，更新缓存时候若未加控制，可能出现数据更新错乱，如下图所示：

![4](4.png)

需要将图中的第2、3、4步进行加锁，是的缓存操作串行化。由于utag本身就依赖了redis，此处我们的分布式锁就基于redis实现。

#### 基于updateTime的更新顺序控制

携程的mysql规范要求db层面的表必须包含updateTime字段，并且设置了ON UPDATE CURRENT_TIMESTAMP。但是并没有控制updateTime的精度，大多都是秒级别。因此很可能存在一条数据存在并发更新的可能，从而导致缓存数据更新出现一致性问题。

针对，这个问题，因为我们讨论的是「最终一致性」，因此这里引入了延迟消息的概念。即，对于同一秒更新的数据，在更新缓存成功以后，再发送一个延迟1秒的缓存更新触发消息。再次更新缓存。

举个例子：假设同一秒内同一条数据出现了两次更新，value=1和value=2，期望最终缓存中的数据是value=2。若这两次更新后的数据被先后触发，分两种情况：

- case1：若value=1先更新，value=2后更新，（两者都可更新到缓存中，因为虽然是同一秒，但是值不相等）则缓存中最终数据为value=2。
- case2：若value=2先更新，value=1后更新，则第一轮更新后缓存数据为value=1，不是期望数据，之后对比发现是同一秒数据后会通过消息触发二次更新，重新查询DB数据为value=2，可以更新到缓存中。如下图所示：

![5](5.png)

- 为什么需要延迟消息？

其实不用延迟消息也是可以的，毕竟DB数据的更新时间是不变的，但是考虑到出现同一秒更新的可能是高频更新场景，若直接发消息，然后立即消费并触发二次更新，可能依然查到同一秒内更新的其他数据，为减少此种情况下的多次循环更新，延迟几秒再刷新可作为一种优化策略。

- 不支持db的删除操作

因为删除操作和update操作无法进行数据对比，无法确定操作的先后顺序，进而可能导致更新错乱。而在数据异常宝贵的时代，一般的业务系统中也没有物理删除的逻辑。





### 数据完整性设计

### 系统可用性保证



# IM系统
- https://xie.infoq.cn/article/19e95a78e2f5389588debfb1c
- http://www.52im.net/thread-2848-1-1.html
- https://mp.weixin.qq.com/s/fMF_FjcdLiXc_JVmf4fl0w


# Spring IOC容器是什么样的数据结构

Spring IOC容器并不是一种特定的数据结构，而是一种管理对象的容器。它的核心思想是通过反射和依赖注入来实现对象的创建、组装和管理。

在Spring的IOC容器中，通常使用Map、List等数据结构来存储对象。具体来说，Spring提供了不同类型的IOC容器，如BeanFactory、ApplicationContext等，它们在底层使用了不同的数据结构来管理对象。

常见的IOC容器实现类如下：

- DefaultListableBeanFactory：它是Spring IOC容器的默认实现类，底层使用了HashMap来存储对象和BeanDefinition的注册信息。
- AnnotationConfigApplicationContext：它是基于注解的IOC容器实现类，底层使用了ConcurrentHashMap来存储bean的定义和实例化信息。
- ClassPathXmlApplicationContext：它是基于XML配置文件的IOC容器实现类，底层使用了HashMap和ConcurrentHashMap来存储bean的定义和实例化信息。
无论是基于注解还是基于XML配置的IOC容器，其主要的数据结构都是基于HashMap或ConcurrentHashMap。这些数据结构通过键-值对的方式存储对象的定义和实例。

需要注意的是，Spring的IOC容器并不仅限于使用单一的数据结构，它还包含了一系列解析配置文件、管理对象生命周期的功能，因此，IOC容器可以视作是一种高度封装和抽象化的对象管理机制，而数据结构只是其中的一部分实现方式。

# Spring如何防止bean重复加载

Spring通过Bean的定义和注册机制来避免Bean的重复加载。当Spring容器初始化时，会根据配置文件或注解等方式，将Bean的定义注册到容器中。在注册过程中，Spring会对Bean的名称或其他限定符进行校验，确保Bean的唯一性。

具体来说，Spring采用了以下机制来防止Bean的重复加载：

- Bean的命名规范：Spring要求每个Bean都有一个唯一的名称，可以通过XML配置文件中的id属性、注解中的@Component("beanName")等方式指定。如果存在重复的名称，Spring容器在注册Bean时会抛出异常，提示冲突。
- Bean的作用域：Spring提供了多种Bean的作用域，如Singleton（单例）、Prototype（原型）等。默认情况下，Bean的作用域为Singleton，即在容器中只有一个实例。如果尝试重复加载同一个Singleton Bean，Spring容器会直接返回已存在的实例，而不会重新创建。
- 检查已注册的Bean定义：在注册Bean时，Spring会先检查是否已经存在相同名称或相同类型的Bean定义。如果发现重复的定义，容器将会抛出异常，避免重复加载。
- 使用注解方式的扫描：Spring支持使用注解（如@Component、@Service、@Controller等）进行自动扫描和Bean的注册。在注解扫描过程中，Spring会检查并跳过已经注册过的Bean，避免重复加载。
- 使用延迟初始化（Lazy Initialization）：Spring提供了延迟初始化的功能，即在需要使用某个Bean时才进行实例化。这样可以避免在容器启动时一次性加载所有Bean，减少重复加载的可能性。
通过以上机制和策略，Spring能够有效地防止Bean的重复加载，从而确保容器中的Bean是唯一且正确的。

#常见网络攻击

## 什么是XSS攻击

- 攻击方式：通过注入恶意脚本代码到受信任的网站中，使用户在浏览器中执行该恶意脚本。
- 目标：攻击者可以使用XSS攻击来盗取用户的敏感信息，例如登录凭证、个人资料，或者在用户所在的网页上执行恶意操作。

## 什么是CSRF攻击

CSRF（Cross-Site Request Forgery，跨站请求伪造）攻击是一种常见的网络安全攻击方式，它利用被攻击用户在已认证过的网站上的身份，通过跨站点请求来执行未经授权的操作。

攻击者通常利用以下步骤执行CSRF攻击：

1. 攻击者创建一个恶意网站，并在其中包含一个针对被攻击网站的请求（比如提交表单、发送请求等）。
2. 攻击者诱使用户访问恶意网站。这可以通过发送诱导邮件、社交媒体诱导、钓鱼网站等方式进行。
3. 用户在浏览器中登陆了被攻击网站，该网站的身份认证信息（比如Cookie）被保存在用户的浏览器中。
4. 在用户未登出被攻击网站的情况下，攻击者的恶意请求会自动触发用户的浏览器发出该请求。
5. 用户的浏览器会自动发送携带被攻击网站身份认证信息的请求给被攻击网站。
6. 被攻击网站会认为该请求是合法的，然后执行攻击者设定的操作，比如修改用户密码、转账、删除数据等。

CSRF攻击的危害在于，攻击者可以在不知情的情况下利用受害者的身份进行操作，被攻击的用户可能无法察觉到攻击的存在。攻击者可以通过各种手段获取用户在被攻击网站上的权限或敏感信息，破坏用户的数据完整性和安全性。

为了有效防止CSRF攻击，常见的防护手段包括：

- 在关键操作上使用验证码或其他身份验证机制，增强用户操作的可靠性。
- 在关键操作或敏感数据访问上使用双因素身份认证。
- 在请求中增加验证机制，比如使用Token或自定义请求头，确保请求的合法性。
- 设置Cookie属性为SameSite属性，限制仅在同站点访问时发送请求。
- 定期更新密码，避免遭受攻击后长时间保持同一密码。


通过采用这些防护措施，可以有效减轻并防止CSRF攻击的风险。

## 什么是SSRF攻击

- 攻击方式：攻击者通过构造恶意请求，使服务器发起对内部或外部资源的请求，绕过应用程序的访问控制。
- 目标：SSRF攻击的目标是让服务器请求受限制的资源，例如内部网络、本地文件系统或其他第三方服务，从而获取敏感数据或执行未经授权的操作。

# 对于http协议，服务端如何知道客户端的请求数据已经发送完成

在HTTP协议中，服务端无法直接知道客户端的请求数据是否已经发送完成。HTTP是基于请求-响应模型的，客户端发送请求，服务端返回响应。客户端发送完请求后，会等待服务端的响应，而服务端也在接收到请求后开始处理，并在处理完后发送响应。

服务端通常会通过以下方式来判断请求数据已经发送完成：

- Content-Length 头信息：客户端在请求中可以添加Content-Length头信息，用于指定请求体的长度。服务端通过读取Content-Length的值，来判断请求体的大小是否与实际接收到的数据一致。当服务端读取到请求体大小与Content-Length的值相等时，可以认为请求数据已经发送完成。

- Transfer-Encoding 头信息：客户端可以使用Transfer-Encoding头信息来指定数据传输的编码方式，如chunked。在使用chunked编码时，客户端会在请求中将请求体拆分成多个块(chunk)进行发送。服务端在接收到每个块后，可以立即开始处理数据，而不需要等待整个请求体发送完成。

- Connection 头信息：在HTTP/1.1中，默认使用持久连接(Connection: keep-alive)，即同一个TCP连接可以传输多个请求-响应对。当服务端接收到一个请求后，可以通过Connection头信息来判断客户端是否会发送后续的请求。如果客户端不再发送请求，服务端可以认为请求数据已经发送完成。

需要注意的是，上述方式只是服务端根据HTTP协议规范进行的处理判断，服务端无法确切知道客户端是否发送完所有数据。因此，在特定应用场景下，开发者可能需要采用其他手段，如自定义协议或在应用层进行数据完整性的验证，来确保请求数据的完整性和准确性。

# mysql联合索引的数据结构是什么

MySQL中的联合索引是由多列组合而成的索引，也称为复合索引或多列索引。它是一种特殊类型的索引，通过对多个列进行索引来加快查询的速度。

联合索引的数据结构可以通过B+树来表示。B+树是一种常见的自平衡搜索树数据结构，适用于范围查询和按序访问等操作。对于联合索引，MySQL使用B+树的变体来组织索引数据。

在B+树中，根节点包含一个或多个索引项，每个索引项由键值（索引列的值）和指针组成。叶子节点存储真实的数据行，每个数据行包含联合索引的所有列值。叶子节点之间通过指针连接形成双向链表，方便范围查询的快速定位。

对于联合索引，B+树的构建方式会考虑多个列值的顺序。在B+树的内部节点中，存储了每个索引键的前缀，这样可以在搜索时参与索引键的比较，以快速定位对应的叶子节点。

使用联合索引时，查询语句可以利用各个索引列的顺序去匹配索引节点，从而快速定位到叶子节点，降低了磁盘I/O的开销，提升了查询性能。

需要注意的是，联合索引的顺序对查询的效果有影响。联合索引中的列顺序应该根据查询的频率和条件进行选择，将经常一起使用的列放在联合索引的前面，这样可以提高查询的效率。

总结起来，MySQL中的联合索引数据结构是基于B+树的，通过多列组合构建索引，提供高效的查询性能。

# mysql join操作底层原理

在MySQL中，JOIN操作是用于将多个表的数据按照特定条件进行关联查询的操作。JOIN操作的底层原理可以简单概括为以下几个步骤：

1. 语法解析与查询优化：MySQL首先解析查询语句，生成查询语法树，并进行查询优化，确定最优的查询执行计划。

2. 表的连接顺序：MySQL通过查询优化器确定表的连接顺序。根据表的大小、索引情况、查询条件等因素，决定在什么顺序下进行连接操作。

3. 连接算法：MySQL使用多种连接算法执行JOIN操作，常见的有以下几种：

- 嵌套循环连接（Nested Loop Join）：对于每个左表的记录，都会在右表中进行查找匹配的记录。这是一种简单的连接算法，但对于大型数据集和无索引的情况下，性能可能较差。

- 哈希连接（Hash Join）：MySQL会先将较小的表构建一个哈希表，然后遍历另一个较大的表，通过查找哈希表中的值来进行连接操作。哈希连接适用于数据量大且可以完全载入内存的情况。

- 排序合并连接（Merge Join）：MySQL会对连接参与的两个表按照连接条件进行排序，然后通过扫描有序数据集进行匹配操作。排序合并连接适用于有序数据集或索引的情况。

- 索引连接（Index Join）：如果JOIN操作可以使用索引覆盖，MySQL会使用索引连接。索引连接避免了对表的全表扫描，效率较高。

4. 数据读取与返回：根据连接算法，在连接过程中，MySQL会同时读取两个表的数据，并进行匹配操作。匹配成功后，返回匹配的结果集给用户。

需要注意的是，MySQL的查询优化器会根据表的结构、数据分布、统计信息等动态选择使用哪种连接算法。不同的连接操作和数据场景下，其底层原理和实现方式可能有所不同。此外，优化表结构、合理使用索引、避免跨表查询等方法也可以提升JOIN操作的性能。

# java 新建对象的方式

1. 使用关键字new和构造函数：通过调用类的构造函数来创建一个对象。例如：
```java
ClassName obj = new ClassName();  // 使用默认的构造函数创建对象
```
2. 使用反射机制：通过`java.lang.reflect`包中的`Constructor`类来动态创建对象。例如：
```java
Class<?> cls = Class.forName("ClassName");  // 获取类的Class对象
Constructor<?> constructor = cls.getConstructor();  // 获取构造函数对象
ClassName obj = (ClassName) constructor.newInstance();  // 使用构造函数对象创建对象
```
3. 使用clone()方法：对已有对象进行拷贝，创建一个新对象。要使用此方法，类需要实现Cloneable接口并重写clone()方法。例如：
```java
ClassName obj1 = new ClassName();
ClassName obj2 = obj1.clone();
```
4. 使用工厂方法：通过调用静态方法来创建对象，通常在工厂类中实现。例如：
```java
public class Factory {
    public static ClassName createObject() {
        return new ClassName();
    }
}

// 在其他地方调用工厂方法
ClassName obj = Factory.createObject();
```
5. 使用反序列化：通过将对象写入字节流，再从字节流中读取对象，来创建一个新的对象。例如：
```java
// 将对象写入字节流
FileOutputStream fileOut = new FileOutputStream("file.ser");
ObjectOutputStream out = new ObjectOutputStream(fileOut);
out.writeObject(obj);
out.close();
fileOut.close();

// 从字节流中读取对象
FileInputStream fileIn = new FileInputStream("file.ser");
ObjectInputStream in = new ObjectInputStream(fileIn);
ClassName newObj = (ClassName) in.readObject();
in.close();
fileIn.close();
```

# 为什么说hashmap线程不安全
HashMap 使用了 fail-fast 机制，即在进行迭代遍历的过程中，如果发现其他线程对 HashMap 进行了结构上的修改（如增加、删除元素），就会抛出 ConcurrentModificationException 异常。这个机制旨在通过快速失败的方式，提醒开发者在多线程环境下操作 HashMap 可能会导致不确定的结果。

当一个线程在迭代 的同时，另一个线程对其进行结构上的修改，比如新增删除键值对，会导致 HashMap 的 modCount（计数器）发生变化。每个迭器会在创建的时候，保存一个期望的 modCount 值，当迭代器进行 next 操作时，会对比期望的 modCount 和实际的 modCount，若不致，则抛出 ConcurrentModificationException 异常。

这种速失败的机制可以尽早地检测到并发修改的情况，避免在读取数据时可能导致不一致问题。但是需要注意的是，该机制并不能保证并发修改一定会导致异常，因它仅仅是在迭代器中进行了检查。而对于直接在多个线程之间并发修改 HashMap 的情况，还需要额外并发控制来保证数据的一致性。

对 fail-fast 机制，可以采取以下几种处理方式：

1. 使用并发安全的 ConcurrentHashMap，它不抛出 ConcurrentModificationException 异常。
2. 使用迭代器的 remove() 方法进行元素的删除，而不直接在遍历的过程中使用 HashMap 的 remove() 方法。
3. 在多线程环境下对 HashMap 进行操作时，使用适的同步机制来保证线程安全如使用锁或 synchronized 块。
4. 尽量避免在代过程中对 HashMap 进行修改操作，以免出现不确定的结果。

综上所述，HashMap 的 fail 机制是为了检测并发修改操作而设计的，提醒开发者注意多线环境下对 HashMap 的正确使用。

# 如何理解池化资源

池化资源是一种有效管理和复用资源的机制。它将一组资源预先创建并保存在一个池中，这些资源可以被多个线程或客户端共享和复用。池化资源的主要目的是提高资源的利用率和性能。

在池化资源中，资源可以是各种类型的对象，比如数据库连接、线程、网络连接、文件句柄等。当有线程或客户端需要使用资源时，从资源池中获取一个可用的资源。一旦使用完毕，将资源释放回池中，以供其他线程继续使用。

池化资源的好处包括：

1. 资源复用：池化资源可以被多个线程或客户端共享，避免了每次使用时创建和销毁资源的开销，提高了资源的利用率。
2. 性能提升：使用池化资源可以避免频繁地创建和销毁资源，减少了系统开销，提高了系统的响应速度和吞吐量。
3. 控制资源数量：通过限制池中可用资源的数量，可以有效控制系统特定资源的使用量，防止资源的过度消耗。
4. 资源管理：通过池资源，可以更好地管理和监控资源的分配和释放，提高了系统的稳定性和可扩展性。

实际上，池化资源在许多场景中都得到了广泛应用，比如连接池、线程池、对象池等。这些池化资源的实现一般采用合适的数据结构来管理资源的可用状态和分配策略，确保资源的可靠性和高效性。

总之，通过池化资源，可以更好地管理和复用系统中的资源，提高了资源的利用率和系统的性能，是一种常用的优化手段。

# kafka的消息是pull还是push模式

Kafka 是一种消息队列系统，它采用 pull 模式。在 Kafka 中，消费者（consumer）负责从 Kafka 集群中主动拉取（pull）消息。

具体来说，Kafka 的消费者可以通过调用 API 发起拉取请求，向 Kafka 服务器请求获取新的消息。消费者可以控制拉取的频率和批量大小，以适应消费者的处理能力和需求。

相比于 push 模式，pull 模式具有以下优势：

1. 消费可以按照自己的节奏和处理能力来拉取消息，避免了消息的积压和资源浪费。
2. 消费者可以灵活地控制消息的处理顺序和并发度，以满足特定的业务需求。
3. 消费者可以根据需要重复消费某些消息，或者进行消息的回溯和重新处理。

需要注意的是，虽然 Kafka 是通过拉取的方式获取消息，但是它的典型使用方式是结合长轮询（long polling）和流式处理（stream processing）的方式，从而实现高效的消息传递和处理能力。

总之，Kafka 采用拉取（pull）模式，费者通过主动请求来获取消息，这种模式使得消费者能够更加灵活、高效地处理消息。

# 如何让时间最短的线程停止后，安全的通知其他线程停止

要实现一个线程安全地通知其他线程停止，可以使用Java中的Thread类的`interrupt()`和`isInterrupted()`方法。下面是一个基本的实现示例：

1. 创建一个volatile类型的共享变量isStopRequested，用来表示是否请求停止线程。
2. 在其他线程中，通过调用被请求停止的线程的interrupt()方法来向该线程发送中断信号。
3. 被请求停止的线程在适当的时候，通过检查自身的isInterrupted()方法来判断是否应该停止，并执行相应的逻辑。
4. 如果被请求停止的线程发现`isStopRequested`为`true`，或者检测到自身线程的`isInterrupted()`为`true`，则停止执行并退出线程。

示例代码:
```java
public class WorkerThread extends Thread {
    private volatile boolean isStopRequested = false;

    public void requestStop() {
        isStopRequested = true;
        this.interrupt(); // 向线程发送中断信号
    }

    @Override
    public void run() {
        while (!isStopRequested && !Thread.currentThread().isInterrupted()) {
            // 线程逻辑代码
        }
        // 线程停止后的逻辑处理
    }
}
```

在其他线程中，可以通过调用`requestStop()`方法来请求停止`WorkerThread`线程，该方法会设置`isStopRequested`为`true`，并调用`interrupt()`方法发送中断信号。被请求停止的线程在适当的时候会检查`isStopRequested`和`isInterrupted()`来判断是否停止执行。

使用这种方式，在保证线程安全的前提下，能够让时间最短的线程停止后，安全地通知其他线程止。

# 双写

https://www.jianshu.com/p/ab2efddfafbd

## 迁移过程需要满足的目标
- 迁移应该是在线的迁移，也就是在迁移的同时还会有数据的写入；
- 数据应该保证完整性，也就是说在迁移之后需要保证新的库和旧的库的数据是一致的；
- 迁移的过程需要做到可以回滚，这样一旦迁移的过程中出现问题，可以立刻回滚到源库不会对系统的可用性造成影响。

## 同步双写

[double_write](./image/2022/double_write.webp)

1. 将新的库配置为源库的从库用来同步数据；如果需要将数据同步到多库多表，那么可以使用一些第三方工具获取 Binlog 的增量日志（比如开源工具 Canal），在获取增量日志之后就可以按照分库分表的逻辑写入到新的库表中了。
2. 同时我们需要改造业务代码，在数据写入的时候不仅要写入旧库也要写入新库。当然，基于性能的考虑，我们可以异步地写入新库，只要保证旧库写入成功即可。但是我们需要注意的是，需要将写入新库失败的数据记录在单独的日志中，这样方便后续对这些数据补写，保证新库和旧库的数据一致性。
3. 然后我们就可以开始校验数据了。由于数据库中数据量很大，做全量的数据校验不太现实。你可以抽取部分数据，具体数据量依据总体数据量而定，只要保证这些数据是一致的就可以。
4. 双写时加开关，默认关闭双写，上线完成后关闭同步，同时打开开关，在低峰期的话数据丢失的概率不高。再配合数据校验的工作，是可以保证一致性的
5. 如果一切顺利，我们就可以将读流量切换到新库了。由于担心一次切换全量读流量可能会对系统产生未知的影响，所以这里最好采用灰度的方式来切换，比如开始切换 10% 的流量，如果没有问题再切换到 50% 的流量，最后再切换到 100%。
6. 由于有双写的存在，所以在切换的过程中出现任何的问题都可以将读写流量随时切换到旧库去，保障系统的性能。
7. 在观察了几天发现数据的迁移没有问题之后，就可以将数据库的双写改造成只写新库，数据的迁移也就完成了。

### 最容易出现的问题

数据校验的工作

建议迁移前写好数据校验的工具或脚本，在测试环境上充分测试之后，在开始正式的数据迁移
