# 2019学习笔记

标签（空格分隔）： presto hive mongDB 响应式编程 spring的状态机 缓存相关

---

  * [mysql间歇锁](#mysql间歇锁)
  * [Java的泛型](#java的泛型)
  * [拼多多面经](#拼多多面经)
  * [tcc事务学习  2pc 3pc 比较](#tcc事务学习--2pc-3pc-比较)
  * [springcloud部分组件底层原理](#springcloud部分组件底层原理)
  * [springcloud参数调优](#springcloud参数调优)
  * [面试经典面经](#面试经典面经)
  * [腾讯面经](#腾讯面经)
  * [DispatcherServlet执行流程](#dispatcherservlet执行流程)
  * [主流Java数据库连接池分析](#主流java数据库连接池分析)
  * [OOM的常见情况](#oom的常见情况)
  * [Redis的缓存过期策略和内存淘汰策略](#redis的缓存过期策略和内存淘汰策略)
  * [tcp滑动窗口](#tcp滑动窗口)
  * [rpc通信和http通信的区别联系](#rpc通信和http通信的区别联系)
  * [http2.0协议](#http20协议)
  * [二进制协议与文本协议](#二进制协议与文本协议)
  * [java的序列化与反序列化](#java的序列化与反序列化)
  * [ArrayList的序列化（HashMap序列化类似）](#arraylist的序列化hashmap序列化类似)
  * [java序列化漏洞](#java序列化漏洞)
  * [hession序列化对比Java序列化](#hession序列化对比java序列化)
  * [ConcurrentHashMap的大小如何确定](#concurrenthashmap的大小如何确定)
  * [mybatis-plus的基本操作及批量操作](#mybatis-plus的基本操作及批量操作)
  * [springboot的starter原理](#springboot的starter原理)
  * [@Repository @Mapper @Resource @Autowired](#repository-mapper-resource-autowired)
  * [MyBatisPlus中使用 @TableField完成字段自动填充](#mybatisplus中使用-tablefield完成字段自动填充)
  * [github添加多个sshkey](#github添加多个sshkey)
  * [CDN缓存](#cdn缓存)
  * [ThreadLocal原理及实际应用](#threadlocal原理及实际应用)
  * [三个线程交替打印输出](#三个线程交替打印输出)

presto参考文档：

 1. https://tech.meituan.com/2014/06/16/presto.html
 2. http://www.stay-stupid.com/?p=395
 

hive参考文档：

 1. https://blog.csdn.net/wangyang1354/article/details/50570903
 2. https://blog.csdn.net/LW_GHY/article/details/51469753
 3. https://www.jianshu.com/p/dbad3b3d40eb

MongoDB参考文档：

 1. https://www.w3cschool.cn/mongodb/mongodb-query.html
 

spring5的webflux参考资料：

 1. https://juejin.im/post/5b3a22a16fb9a024db5ff13e
 2. https://zhuanlan.zhihu.com/p/37846655
 3. https://github.com/pkpk1234/learn-reactor
 

spring的状态机参考资料：

 1. http://blog.didispace.com/spring-statemachine/
 


缓存相关参考资料：

2. http://zhuanlan.51cto.com/art/201806/577116.htm
3. https://tech.meituan.com/2017/03/17/cache-about.html


poi生成pdf文档：

 1. https://blog.csdn.net/makang456/article/details/70161037
 

## mysql间歇锁

 - https://blog.csdn.net/andyxm/article/details/44810417
 - https://zhuanlan.zhihu.com/p/48269420
 - https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html#innodb-gap-locks

所谓间隙锁，区别于行锁只锁住一行，间隙锁会锁住一个区间段的数据。

需要强调一下：间隙锁在**主键索引、普通索引**上的性质略微有些差别。


下面先讨论**主键索引**上的间隙锁：

比如：
```mysql
CREATE TABLE `test` (
  `id` int(1) NOT NULL AUTO_INCREMENT,
  `name` varchar(8) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

INSERT INTO `test` VALUES ('1', '小罗');
INSERT INTO `test` VALUES ('5', '小黄');
INSERT INTO `test` VALUES ('7', '小明');
INSERT INTO `test` VALUES ('11', '小红');
```
上面数据存在左开右闭隐藏的间隙锁：

 1. (-infinity, 1]
 2. (1, 5]
 3. (5, 7]
 4. (7, 11]
 5. (11, +infinity]

然后执行如下的sql语句，会产生间隙锁。
```mysql
/* 开启事务1 */
BEGIN;
/* 查询 id 在 5 - 7 范围的数据并加记录锁，产生的锁的范围是[5-7] */
SELECT * FROM `test` WHERE `id` BETWEEN 5 AND 7 FOR UPDATE;
/* 延迟300秒执行，防止锁释放 */
SELECT SLEEP(300);

# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句

/* 事务2插入一条 id = 3，name = '小张1' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (3, '小张1'); # 正常执行

/* 事务3插入一条 id = 4，name = '小白' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (4, '小白'); # 正常执行

/* 事务4插入一条 id = 6，name = '小东' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (6, '小东'); # 阻塞

/* 事务5插入一条 id = 8， name = '大罗' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (8, '大罗'); # 正常执行

/* 事务6插入一条 id = 9， name = '大东' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (9, '大东'); # 正常执行

/* 事务7插入一条 id = 11， name = '李西' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (11, '李西'); # 正常执行

/* 事务8插入一条 id = 12， name = '张三' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (12, '张三'); # 正常执行

/* 提交事务1，释放事务1的锁 */
COMMIT;
```
上面的for update语句锁住了[5-7]区间的数据

另外。如果for update语句或者delete语句操作的数据在数据库不存在，那么也会产生间隙锁。比如，删除id=3的数据（忽略上面事务的sql产生的数据），那么会产生[1-5]的间歇锁。

但是如果for update语句或者delete语句操作的数据存在，那么不会产生间歇锁，只会有行锁。

下面再看一下**普通索引**上的间隙锁：
准备如下表结构及初始数据：
```mysql
# 注意：number 不是唯一值

CREATE TABLE `test1` (
  `id` int(1) NOT NULL AUTO_INCREMENT,
  `number` int(1) NOT NULL COMMENT '数字',
  PRIMARY KEY (`id`),
  KEY `number` (`number`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;

# 初始化数据
INSERT INTO `test1` VALUES (1, 1);
INSERT INTO `test1` VALUES (5, 3);
INSERT INTO `test1` VALUES (7, 8);
INSERT INTO `test1` VALUES (11, 12);
```
 number 索引存在的隐藏间隙：
 
 1. (-infinity, 1]
 2. (1, 3]
 3. (3, 8]
 4. (8, 12]
 5. (12, +infinity]

然后执行如下的事务：
```mysql
/* 开启事务1 */
BEGIN;
/* 查询 number = 2 的数据并加记录锁 */
SELECT * FROM `test1` WHERE `number` = 2 FOR UPDATE;
/* 延迟30秒执行，防止锁释放 */
SELECT SLEEP(30);

# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句

/* 事务2插入一条 number = 0 的数据 */
INSERT INTO `test1` (`number`) VALUES (0); # 正常执行

/* 事务3插入一条 number = 1 的数据 */
INSERT INTO `test1` (`number`) VALUES (1); # 被阻塞

/* 事务4插入一条 number = 2 的数据 */
INSERT INTO `test1` (`number`) VALUES (2); # 被阻塞

/* 事务5插入一条 number = 4 的数据 */
INSERT INTO `test1` (`number`) VALUES (4); # 正常执行

/* 事务6插入一条 number = 8 的数据 */
INSERT INTO `test1` (`number`) VALUES (8); # 正常执行

/* 事务7插入一条 number = 9 的数据 */
INSERT INTO `test1` (`number`) VALUES (9); # 正常执行

/* 事务8插入一条 number = 10 的数据 */
INSERT INTO `test1` (`number`) VALUES (10); # 正常执行

/* 提交事务1 */
COMMIT;
```

可以看到，上面的sql产生number索引了[1-3]的间隙锁。

再看下面的sql：
```mysql
/* 开启事务1 */
BEGIN;
/* 查询 number = 3 的数据并加记录锁 */
SELECT * FROM `test1` WHERE `number` = 3 FOR UPDATE;
/* 延迟30秒执行，防止锁释放 */
SELECT SLEEP(30);

# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句

/* 事务2插入一条 number = 0 的数据 */
INSERT INTO `test1` (`number`) VALUES (0); # 正常执行

/* 事务3插入一条 number = 1 的数据 */
INSERT INTO `test1` (`number`) VALUES (1); # 被阻塞

/* 事务4插入一条 number = 2 的数据 */
INSERT INTO `test1` (`number`) VALUES (2); # 被阻塞

/* 事务5插入一条 number = 4 的数据 */
INSERT INTO `test1` (`number`) VALUES (4); # 被阻塞

/* 事务6插入一条 number = 8 的数据 */
INSERT INTO `test1` (`number`) VALUES (8); # 正常执行

/* 事务7插入一条 number = 9 的数据 */
INSERT INTO `test1` (`number`) VALUES (9); # 正常执行

/* 事务8插入一条 number = 10 的数据 */
INSERT INTO `test1` (`number`) VALUES (10); # 正常执行

/* 提交事务1 */
COMMIT;
```
上面的sql，即便存在number=3的记录，但是还是产生了[1-8]的间隙锁。这就是普通索引和主键索引不一样的地方。

至于原因，我们先看一下下面的sql再解释:
```java
/* 开启事务1 */
BEGIN;
/* 查询 number = 5 的数据并加记录锁 */
SELECT * FROM `test1` WHERE `number` = 3 FOR UPDATE;
/* 延迟30秒执行，防止锁释放 */
SELECT SLEEP(30);

/* 事务1插入一条 id = 2， number = 1 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (2, 1); # 阻塞

/* 事务2插入一条 id = 3， number = 2 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (3, 2); # 阻塞

/* 事务3插入一条 id = 6， number = 8 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (6, 8); # 阻塞

/* 事务4插入一条 id = 8， number = 8 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (8, 8); # 正常执行

/* 事务5插入一条 id = 9， number = 9 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (9, 9); # 正常执行

/* 事务6插入一条 id = 10， number = 12 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (10, 12); # 正常执行

/* 事务7修改 id = 11， number = 12 的数据 */
UPDATE `test1` SET `number` = 5 WHERE `id` = 11 AND `number` = 12; # 阻塞

/* 提交事务1 */
COMMIT;
```
注意上面的事务3和事务4.事务3阻塞了，事务4却成功了。这是为啥？看看下面这张图：
![此处输入图片的描述][1]

如上图所示，存在原始数据id=7，number=8，因此事务4的id=8，number=8不会被阻塞。

 1. 在普通索引列上，**不管**是何种查询，只要加锁，都会产生间隙锁，这跟唯一索引不一样；
 2. 在普通索引跟唯一索引中，数据间隙的分析，数据行是**优先**根据**普通索引**排序，再根据唯一索引排序。
 4. 临键锁，是记录锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间。临键锁的主要目的，也是为了避免幻读(Phantom Read)。如果把事务的隔离级别降级为RC，临键锁则也会失效，RR级别才会生效。


## Java的泛型
- https://www.toutiao.com/i6728925616785080844/

Java 泛型（generics）是 JDK 5 中引入的一个新特性,泛型提供了编译时类型安全检测机制，该机制允许开发者在编译时检测到非法的类型。泛型的好处就是在编译的时候能够检查类型安全，并且所有的强制转换都是自动和隐式的。

我们在定义泛型类，泛型方法，泛型接口的时候经常会碰见很多不同的通配符，比如 T，E，K，V 等等，这些通配符又都是什么意思呢？

本质上这些个都是通配符，没啥区别，只不过是编码时的一种约定俗成的东西。比如上述代码中的 T ，我们可以换成 A-Z 之间的任何一个 字母都可以，并不会影响程序的正常运行，但是如果换成其他的字母代替 T ，在可读性上可能会弱一些。通常情况下，T，E，K，V，？ 是这样约定的：

 - ？ 表示不确定的 java 类型
 - T (type) 表示具体的一个java类型
 - K V (key value) 分别代表java键值中的Key Value
 - E (element) 代表Element

首先说一下**?**通配符：

![此处输入图片的描述][2]

当调用 countLegs1 时，就会飘红，提示的错误信息如下：

![此处输入图片的描述][3]

所以，对于不确定或者不关心实际要操作的类型，可以使用无限制通配符（尖括号里一个问号，即 <?> ），表示可以持有任何类型。像 countLegs 方法中，限定了**上界**，但是不关心具体类型是什么，所以对于传入的 **Animal** 的所有**子类**都可以支持，并且不会报错。而 countLegs1 就不行。

## 拼多多面经

https://www.toutiao.com/i6673066864022651396/

## tcc事务学习  2pc 3pc 比较

 1. https://blog.csdn.net/u013380694/article/details/83347764
 2. https://www.cnblogs.com/jajian/p/10014145.html
 3. https://juejin.im/post/5bf201f7f265da610f63528a

tcc（try-confirm-Cancel）事务这个东西，个人觉得其实就是一个类似于2pc的分布式事务处理机制。为了达到某种操作（例如商品订单的业务），它并不会直接进行操作，而是有一个预操作的过程（try），如果try成功了，那么才会进行confirm（注意：tcc有一个隐含的设定，try成功以后，那么必须confirm也成功，就好比我们订火车票一样，一旦锁定席位成功，在剩余的15分钟内，只要我们付款，那么一定可以保证出票成功，一样的道理）。如果try失败了，那么才会有cancel这一步。

连接3里使用我们平时都会接触到的订单的业务来讲解，通俗易懂。

一般tcc框架，除非大公司，一般公司都会使用一些开源的框架比如

 1. https://github.com/liuyangming/ByteTCC

ByteTCC可以方便的与springcloud或者dubbo集成。
![此处输入图片的描述][4]
 
 

## springcloud部分组件底层原理

 
 

## springcloud参数调优

 1. https://juejin.im/post/5be83e166fb9a049a7115580

其实，就如参考文档中说的一样，一般的调优主要分成下面几个方面

 - 数据库做读写分离（甚至分库分表）
 - 调整服务之间调用的超时时间（当然这个也不能太大，否则会导致容器的连接池被占满，从而hang住）
 - 数据库合理配置索引，且不要写「大」sql，复杂的业务处理最好放在java代码中进行，一时降低数据库压力，二是方便后期维护（看java代码总比看sql好吧。。）
 - 配置ribbon的自动重试次数（一般失败后自动重试一次,如下图所示），并且需要进行接口幂等性设置，防止重刷
 - 使用缓存，对于非频繁变化的数据进行缓存，合理设置过期时间（对于redis的key建议参考阿里的redis规范进行设置）

 ![此处输入图片的描述][5]
 

## 面试经典面经

 1. https://juejin.im/post/5d6f0987f265da03cf7aab4f


## 腾讯面经

 1. https://www.toutiao.com/i6628527382590390798/

## DispatcherServlet执行流程

 1. https://www.jianshu.com/p/0f981efdfbbd
 2. https://www.cnblogs.com/tengyunhao/p/7518481.html

DispatcherServlet执行流程，其实简单来说，无非就是根据url定位到可以处理该请求的controller方法上，然后进行相关处理，然后返回。

详细一点的话就是，DispatcherServlet通过HandlerMapping定位具体的controller类，并且返回HandlerExecutionChain对象，然后根据HandlerExecutionChain定位到具体的执行方法，也就是HandlerAdapter，最后执行完成进行返回。

更详细的可以参考链接1：

![此处输入图片的描述][6]

具体处理过程如下:

1、用户请求发送至DispatcherServlet类进行处理。

2、DispatcherServlet类遍历所有配置的HandlerMapping类请求查找Handler。

3、HandlerMapping类根据request请求的URL等信息查找能够进行处理的Handler，以及相关拦截器interceptor并构造HandlerExecutionChain。

4、HandlerMapping类将构造的HandlerExecutionChain类的对象返回给前端控制器DispatcherServlet类。

5、前端控制器拿着上一步的Handler遍历所有配置的HandlerAdapter类请求执行Handler。

6、HandlerAdapter类执行相关Handler并获取ModelAndView类的对象。

7、HandlerAdapter类将上一步Handler执行结果的ModelAndView 类的对象返回给前端控制器。

8、DispatcherServlet类遍历所有配置的ViewResolver类请求进行视图解析。

9、ViewResolver类进行视图解析并获取View对象。

10、ViewResolver类向前端控制器返回上一步骤的View对象。

11、DispatcherServlet类进行视图View的渲染，填充Model。

12、DispatcherServlet类向用户返回响应。

## 主流Java数据库连接池分析

 1. https://www.toutiao.com/i6551532416505217539/

常用的主流开源数据库连接池有C3P0、DBCP、Tomcat Jdbc Pool、BoneCP、Druid等。

**C3p0**: 开源的JDBC连接池，实现了数据源和JNDI绑定，支持JDBC3规范和JDBC2的标准扩展。目前使用它的开源项目有Hibernate、Spring等。单线程，性能较差，适用于小型系统，代码600KB左右。

**DBCP (Database Connection Pool)**:由Apache开发的一个Java数据库连接池项目， Jakarta commons-pool对象池机制，Tomcat使用的连接池组件就是DBCP。单独使用dbcp需要3个包：common-dbcp.jar,common-pool.jar,common-collections.jar，预先将数据库连接放在内存中，应用程序需要建立数据库连接时直接到连接池中申请一个就行，用完再放回。单线程，并发量低，性能不好，适用于小型系统。

**Tomcat Jdbc Pool**：Tomcat在7.0以前都是使用common-dbcp做为连接池组件，但是dbcp是单线程，为保证线程安全会锁整个连接池，性能较差，dbcp有超过60个类，也相对复杂。Tomcat从7.0开始引入了新增连接池模块叫做Tomcat jdbc pool，基于Tomcat JULI，使用Tomcat日志框架，完全兼容dbcp，通过异步方式获取连接，支持高并发应用环境，超级简单核心文件只有8个，支持JMX，支持XA Connection。

**BoneCP**：官方说法BoneCP是一个高效、免费、开源的Java数据库连接池实现库。设计初衷就是为了提高数据库连接池性能，根据某些测试数据显示，BoneCP的速度是最快的，要比当时第二快速的连接池快25倍左右，完美集成到一些持久化产品如Hibernate和DataNucleus中。BoneCP特色：高度可扩展，快速；连接状态切换的回调机制；允许直接访问连接；自动化重置能力；JMX支持；懒加载能力；支持XML和属性文件配置方式；较好的Java代码组织，100%单元测试分支代码覆盖率；代码40KB左右。

**Druid**：Druid是Java语言中最好的数据库连接池，Druid能够提供强大的监控和扩展功能，是一个可用于大数据实时查询和分析的高容错、高性能的开源分布式系统，尤其是当发生代码部署、机器故障以及其他产品系统遇到宕机等情况时，Druid仍能够保持100%正常运行。主要特色：为分析监控设计；快速的交互式查询；高可用；可扩展；Druid是一个开源项目，源码托管在github上。

主流连接池各项功能对比如下：

![此处输入图片的描述][7]

HikariCP与其他数据库连接池的对比：

![此处输入图片的描述][8]

**HikariCP性能分析：**

 1. HikariCP通过优化(concurrentBag，fastStatementList )集合来提高并发的读写效率。
 2. HikariCP使用threadlocal缓存连接及大量使用CAS的机制，最大限度的避免lock。但可能带来cpu使用率的上升（因为cas会有while循环）。
 3. 从字节码的维度优化代码。 (default inline threshold for a JVM running the server
    Hotspot compiler is 35 bytecodes ）让方法尽量在35个字节码一下，来提升jvm的处理效率。

 

## OOM的常见情况

 1. https://monkeysayhi.github.io/2018/11/05/Java%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81OOM%E5%8F%8A%E5%8E%9F%E5%9B%A0/
 2. https://www.jianshu.com/p/2fdee831ed03

**堆内存溢出**

这应该是我们最为常见的一种OOM，当堆内没有足够的内存供申请对象使用时，就会出现，示例代码如下：
```java
  public static void main(String args[]) throws Exception {
        UserController userController = new UserController();
        List list = new ArrayList();
        while (true) {
            list.add(userController);
        }
    }
```

运行输出：
```java
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3210)
	at java.util.Arrays.copyOf(Arrays.java:3181)
	at java.util.ArrayList.grow(ArrayList.java:265)
	at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:239)
	at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:231)
	at java.util.ArrayList.add(ArrayList.java:462)
	at com.t4f.gaea.sample.controller.UserNormalController.main(UserNormalController.java:47)
```

**java.lang.OutOfMemoryError:GC overhead limit exceeded**

当应用程序花费超过98%的时间用来做GC并且回收了不到2%的堆内存时，会抛出java.lang.OutOfMemoryError:GC overhead limit exceeded错误。具体的表现就是你的应用几乎耗尽所有可用内存，并且GC多次均未能清理干净。示例代码如下：
```java
 public static void main(String args[]) throws Exception {
        Map map = new HashMap();
        Random r = new Random();
        while (true) {
            map.put(r.nextInt(), "value");
        }
    }
```

注意设置JVM参数：
```java
-Xmx10m -XX:+UseParallelGC
```
运行输出：
```java
Exception in thread "main" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Integer.valueOf(Integer.java:832)
	at com.t4f.gaea.sample.controller.UserNormalController.main(UserNormalController.java:48)
```
 需要注意的是 ，上面的错误信息跟GC算法有很大关系，如果是以G1收集器的话，那么输出如下：
 ```java
 Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
	at java.util.HashMap.newNode(HashMap.java:1750)
	at java.util.HashMap.putVal(HashMap.java:631)
	at java.util.HashMap.put(HashMap.java:612)
	at com.t4f.gaea.sample.controller.UserNormalController.main(UserNormalController.java:48)
 ```
 
 **java.lang.OutOfMemoryError:Metaspace**
 
 元数据区溢出，示例代码如下：
```java
     public static Class generate(String name) throws Exception {
        ClassPool pool = ClassPool.getDefault();
        return pool.makeClass(name).toClass();
    }

    public static void main(String args[]) throws Exception {
        for (int i = 0; ; i++) {
            generate("com.audi" + i);
        }
    }
```
 需要设置虚拟机参数，-XX：MaxMetaspaceSize = 32m，否则可能元数据区在内存中，可能内存还没耗完，堆内存就先溢出了。
 
 输出如下：
```java
Exception in thread "main" org.apache.ibatis.javassist.CannotCompileException: by java.lang.OutOfMemoryError: Compressed class space
	at org.apache.ibatis.javassist.util.proxy.DefineClassHelper.toClass2(DefineClassHelper.java:140)
	at org.apache.ibatis.javassist.util.proxy.DefineClassHelper.toClass(DefineClassHelper.java:95)
	at org.apache.ibatis.javassist.ClassPool.toClass(ClassPool.java:1143)
	at org.apache.ibatis.javassist.ClassPool.toClass(ClassPool.java:1106)
```
 
 **栈溢出**
 递归调用，如果没有break的话，最容易出现栈溢出，示例代码如下：
```java
    
    private static void rec(){
        rec();
        return;
   }

    public static void main(String args[]) throws Exception {
        for (int i = 0; ; i++) {
            rec();
        }
    }
```
 

## Redis的缓存过期策略和内存淘汰策略
- https://www.jianshu.com/p/8aa619933ebb

Redis的缓存过期策略和内存淘汰策略，是两个完全不同的东西，**过期策略**讲的是缓存key到期了，redis该如何处理这些数据；而**淘汰策略**讲的是redis内存不足时，redis该如何为新插入的数据申请内存空间。

 - Redis的过期策略

过期策略通常有以下三种：

**定时过期**：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。

**惰性过期**：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。

**定期过期**：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。
(expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)

Redis中同时使用了**惰性**过期和**定期**过期两种过期策略。

 - Redis的内存淘汰策略

Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。

 1. noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。
 2. allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。
 3. allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。
 4. volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。
 5. volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。
 6. volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。

## tcp滑动窗口

 
## rpc通信和http通信的区别联系

 1. https://www.zhihu.com/question/41609070
 2. https://www.cnblogs.com/winner-0715/p/5847638.html
 3. https://blog.csdn.net/MOU_IT/article/details/79873612

个人总结来看，其实就是rpc协议相对来说，更加灵活，它可以直接建立在tcp之上，也可以建立在http协议之上。但是http不行，它必须遵循特定的规范，且每次请求附加的冗余信息较多。比如下面是一个典型的http响应报文格式：
```html
HTTP/1.0 200 OK 
Content-Type: text/plain
Content-Length: 137582
Expires: Thu, 05 Dec 1997 16:00:00 GMT
Last-Modified: Wed, 5 August 1996 15:55:28 GMT
Server: Apache 0.84

<html>
  <body>Hello World</body>
</html>
```
与之对比，由于rpc可以自定义交互格式，假如定义成如下格式：
![此处输入图片的描述][9]

可以明显看出，在一次交互过程中，rpc传输的冗余信息更少。在带宽一定的条件下，效率相对会更高（这一般都是针对http1.1而言）。

http2.0版本已经针对http协议本身进行了大幅的优化，比如支持并发请求服务器，下面会进行介绍。

## http2.0协议

 1. https://www.thewebmaster.com/hosting/2015/dec/14/what-is-http2-and-how-does-it-compare-to-http1-1/#targetText=HTTP1.x%20uses%20text%2Dbased%20commands%20to%20complete%20HTTP%20requests.&targetText=HTTP2%2C%20on%20the%20other%20hand,0s)%20to%20complete%20HTTP%20requests.
 2. 

HTTP2相对于http1.1而言，主要有以下一些优势（引用链接1的部分内容）。

The key differences HTTP/2 has to HTTP/1.x are as follows:

 1. It is binary, instead of textual
 2. It is fully multiplexed, instead of ordered and blocking
 3. It can use one connection for parallelism
 4. It uses header compression to reduce overhead
 5. It allows Server Pushing to add responses proactively into the
    Browser cache.

整体来说，其实就是是用了二进制协议进行数据帧的传输，使用了分帧技术进行数据传输，得益于使用了数据分帧技术，使得多个请求可以同时发送，而不会阻塞。并且，http2具有数据优先级的概念，使得例如css信息，js依赖库可以优先返回。并且给予server push技术，服务器可以主动向客户端推送信息。

下图展示了数据的分帧过程：
![此处输入图片的描述][10]

下图展示了数据优先级的过程：
![此处输入图片的描述][11]

需要注意的是，一个http2链接的并发数量也是有一定限制，如果超出，那么还是需要新家http链接才可以。不同浏览器的规定如下：
|Browser|	Max Parallel Connections Per Host|
| --------   | -----  |
|IE 9	|6
|IE 10	|8
|Firefox 4+	|6|
|Opera 11+|	6|
|Chrome 4+|	6|
|Safari|	4|

下面详细说一下server push，主要通过有和没有server push的请求过程来说明，首先是没有server push的过程：

Page Load without Server Push
A server without Server Push will follow a simple process:

 1. An HTTP Request is made for the HTML file.
 2. The Server provides the HTML file in response.
 3. The HTML document references a CSS file, JavaScript file, and maybe
    some images. Client requests are then made for those resources,
    which are then returned by the server.
 4. Once the resources are returned the browser renders the page.

You can see this process visualized below:
![此处输入图片的描述][12]

The problem with this method is that it takes time to discover the assets in the HTML page, and then more time to retrieve them. This delays the rendering of the web page and increases web page load times.

然后是有server push的过程：

Page load with Server Push
A server with Server Push will follow an even more straightforward process:

 1. The HTTP Request is made for the HTML file.
 2. The Server provides the HTML file in response, along with the CSS
    file.
 3. **The page starts to render straight away.**
 4. Afterward, other less critical assets can be retrieved, such as the
    JavaScript file, and images.

You can see this process visualized below:

![此处输入图片的描述][13]
server push需要慎重使用，因为这会加重服务器的负载：

The best practice **is not to** push all your assets; just the ones that hold up the page from rendering. If you push too many resources at once it can cause your site to be slower to load, so be careful.


 - 下面简要介绍一下**http3**

http3最大的区别是，给予udp协议，而不再是tcp。

HTTP/3 is an evolution of the QUIC (Quick UDP Internet Connections) protocol from Google, first suggested by Mark Nottingham in October 2018. HTTP/3 is due to be released in 2019 (hopefully).

QUIC is similar to TCP+TLS+HTTP2 but is implemented on top of UDP. UDP stands for User Datagram Protocol. UDP is essentially TCP without all the error checking.

This has many benefits:

 - UDP packets are received by the recipient more quickly.
 - The sender will not have to wait to ensure the packet has been
   received.

Because no error checks are made, when the recipient misses packets, they cannot be requested again. As a result, UDP is used when speed is more important than the occasional lossed packet, such as live broadcasts or online gaming.

Key features of QUIC:

 - Dramatically reduced connection establishment time
 - Improved congestion control
 - Multiplexing without head-of-line blocking
 - Forward error correction
 - Connection migration

 
 
## 二进制协议与文本协议


 

## java的序列化与反序列化

 1. https://www.hollischuang.com/archives/1140
 2. https://blog.chaitin.cn/2015-11-11_java_unserialize_rce/

首先说一下定义：

 - 序列化：将java对象的属性值转换为流的形式，保存在磁盘或进行网络传输的过程叫序列化（对象序列化**不关注类中的静态变量**）。
 - 反序列化：将数据从流中读出，转换为对象的过程叫反序列化。

下面稍微修改链接1的例子，演示一下序列化，反序列化的过程：

首先是新建一个User对象，实现Serializable接口：
```java
package com.hollis;
import java.io.Serializable;
import java.util.Date;

/**
 * Created by hollis on 16/2/2.
 */
public class User implements Serializable{
    private String name;
    private int age;
    private static String country;
    private Date birthday;
    private transient String gender;
    private static final long serialVersionUID = -6849794470754667710L;

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getAge() {
        return age;
    }

    public void setAge(int age) {
        this.age = age;
    }

    public Date getBirthday() {
        return birthday;
    }

    public void setBirthday(Date birthday) {
        this.birthday = birthday;
    }

    public String getGender() {
        return gender;
    }

    public void setGender(String gender) {
        this.gender = gender;
    }

    @Override
    public String toString() {
        return "User{" +
                "name='" + name + '\'' +
                ", age=" + age +
                ", gender=" + gender +
                ", birthday=" + birthday +
                '}';
    }
}
```
 然后是对User进行序列化：
```java
 import org.apache.commons.io.IOUtils;

import java.io.FileOutputStream;
import java.io.IOException;
import java.io.ObjectOutputStream;
import java.util.Date;

/**
 * Created by hollis on 16/2/2.
 */
public class SerializableDemo {

    public static void main(String[] args) {
        //Initializes The Object
        User user = new User();
        user.setName("hollis");
        user.setGender("male");
        user.setAge(23);
        user.setGender("Male");
        user.setBirthday(new Date());
        User.setCountry("China");
        System.out.println(user);

        //Write Obj to File
        ObjectOutputStream oos = null;
        try {
            oos = new ObjectOutputStream(new FileOutputStream("tempFile"));
            oos.writeObject(user);
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            IOUtils.closeQuietly(oos);
        }

    }
}
// User{name='hollis', age=23, gender=Male, birthday=Wed Oct 02 11:59:40 CST 2019, country=China}
```
 
 执行完序列化代码以后，再执行如下的反序列化代码：
```java
 import org.apache.commons.io.FileUtils;
import org.apache.commons.io.IOUtils;

import java.io.*;
import java.util.Date;

/**
 * Created by hollis on 16/2/2.
 */
public class SerializableDemo2 {

    public static void main(String[] args) {

        //Read Obj from File
        File file = new File("tempFile");
        ObjectInputStream ois = null;
        try {
            ois = new ObjectInputStream(new FileInputStream(file));
            User newUser = (User) ois.readObject();
            System.out.println(newUser);
        } catch (IOException e) {
            e.printStackTrace();
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        } finally {
            IOUtils.closeQuietly(ois);
            try {
                FileUtils.forceDelete(file);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

    }
}
// User{name='hollis', age=23, gender=null, birthday=Wed Oct 02 11:59:40 CST 2019, country=null}
```
 
可以看出，gender和country都没有被序列化，也就是transient和static修饰的都无法被序列化。

**序列化及反序列化相关知识**

1、在Java中，只要一个类实现了java.io.Serializable接口，那么它就可以被序列化。

2、通过ObjectOutputStream和ObjectInputStream对对象进行序列化及反序列化

3、虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的一点是两个类的**序列化 ID** 是否一致（就是 private static final long serialVersionUID）

4、序列化并不保存静态变量。

5、要想将父类对象也序列化，就需要让父类也实现Serializable 接口。

6、Transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。

7、服务器端给客户端发送序列化对象数据，对象中有一些数据是敏感的，比如密码字符串等，希望对该密码字段在序列化时，进行加密，而客户端如果拥有解密的密钥，只有在客户端进行反序列化时，才可以对密码进行读取，这样可以一定程度保证序列化对象的数据安全。
 
 

## ArrayList的序列化（HashMap序列化类似）
ArrayList的部分定义：

```java
public class ArrayList<E> extends AbstractList<E>
        implements List<E>, RandomAccess, Cloneable, java.io.Serializable
{
    private static final long serialVersionUID = 8683452581122892189L;
    transient Object[] elementData; // non-private to simplify nested class access
    private int size;
}
```

从上面的代码中可以知道ArrayList实现了java.io.Serializable接口，那么我们就可以对它进行序列化及反序列化。因为elementData是transient的，所以我们认为这个成员变量不会被序列化而保留下来。我们写一个Demo，验证一下我们的想法：
```java
public static void main(String[] args) throws IOException, ClassNotFoundException {
        List<String> stringList = new ArrayList<String>();
        stringList.add("hello");
        stringList.add("world");
        stringList.add("hollis");
        stringList.add("chuang");
        System.out.println("init StringList" + stringList);
        ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream("stringlist"));
        objectOutputStream.writeObject(stringList);

        IOUtils.close(objectOutputStream);
        File file = new File("stringlist");
        ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(file));
        List<String> newStringList = (List<String>)objectInputStream.readObject();
        IOUtils.close(objectInputStream);
        if(file.exists()){
            file.delete();
        }
        System.out.println("new StringList" + newStringList);
    }
//init StringList[hello, world, hollis, chuang]
//new StringList[hello, world, hollis, chuang]
```

了解ArrayList的人都知道，ArrayList底层是通过数组实现的。那么数组elementData其实就是用来保存列表中的元素的。通过该属性的声明方式我们知道，他是无法通过序列化持久化下来的。那么为什么code 4的结果却通过序列化和反序列化把List中的元素保留下来了呢？

**writeObject和readObject方法**

在ArrayList中定义了来个方法： writeObject和readObject。

具体看一下两个方法是实现：
```java
private void readObject(java.io.ObjectInputStream s)
        throws java.io.IOException, ClassNotFoundException {
        elementData = EMPTY_ELEMENTDATA;

        // Read in size, and any hidden stuff
        s.defaultReadObject();

        // Read in capacity
        s.readInt(); // ignored

        if (size > 0) {
            // be like clone(), allocate array based upon size not capacity
            ensureCapacityInternal(size);

            Object[] a = elementData;
            // Read in all elements in the proper order.
            for (int i=0; i<size; i++) {
                a[i] = s.readObject();
            }
        }
    }



private void writeObject(java.io.ObjectOutputStream s)
        throws java.io.IOException{
        // Write out element count, and any hidden stuff
        int expectedModCount = modCount;
        s.defaultWriteObject();

        // Write out size as capacity for behavioural compatibility with clone()
        s.writeInt(size);

        // Write out all elements in the proper order.
        for (int i=0; i<size; i++) {
            s.writeObject(elementData[i]);
        }

        if (modCount != expectedModCount) {
            throw new ConcurrentModificationException();
        }
    }
```

为什么ArrayList要将数组元素设置为transiet，通过实现writeObject和readObject来进行序列化操作呢？
 
 ArrayList实际上是动态数组，每次在放满以后自动增长设定的长度值，如果数组自动增长长度设为100，而实际只放了一个元素，那就会序列化99个null元素。为了保证在序列化的时候不会将这么多null同时进行序列化，ArrayList把元素数组设置为transient。
 
 虽然ArrayList中写了writeObject 和 readObject 方法，但是这两个方法并没有显示的被调用啊。

那么如果一个类中包含writeObject 和 readObject 方法，那么这两个方法是怎么被调用的呢?

对象的序列化过程通过ObjectOutputStream和ObjectInputputStream来实现的，那么带着刚刚的问题，我们来分析一下ArrayList中的writeObject 和 readObject 方法到底是如何被调用的呢？

为了节省篇幅，这里给出ObjectOutputStream的writeObject的调用栈：

writeObject ---> writeObject0 --->writeOrdinaryObject--->writeSerialData--->invokeWriteObject

这里看一下invokeWriteObject：
```java
void invokeWriteObject(Object obj, ObjectOutputStream out)
        throws IOException, UnsupportedOperationException
    {
        if (writeObjectMethod != null) {
            try {
                writeObjectMethod.invoke(obj, new Object[]{ out });
            } catch (InvocationTargetException ex) {
                Throwable th = ex.getTargetException();
                if (th instanceof IOException) {
                    throw (IOException) th;
                } else {
                    throwMiscException(th);
                }
            } catch (IllegalAccessException ex) {
                // should not occur, as access checks have been suppressed
                throw new InternalError(ex);
            }
        } else {
            throw new UnsupportedOperationException();
        }
    }
```

其中writeObjectMethod.invoke(obj, new Object[]{ out });是关键，通过反射的方式调用writeObjectMethod方法。

如果一个类中包含writeObject 和 readObject 方法,在使用ObjectOutputStream的writeObject方法和ObjectInputStream的readObject方法时，会通过反射的方式调用。

**Serializable明明就是一个空的接口，它是怎么保证只有实现了该接口的方法才能进行序列化与反序列化的呢？**

Serializable接口的定义：
```java
public interface Serializable {
}
```
 
 其实这个问题也很好回答，我们再回到刚刚ObjectOutputStream的writeObject的调用栈：
 
 writeObject ---> writeObject0 --->writeOrdinaryObject--->writeSerialData--->invokeWriteObject
 
 writeObject0方法中有这么一段代码：
```java
 if (obj instanceof String) {
                writeString((String) obj, unshared);
            } else if (cl.isArray()) {
                writeArray(obj, desc, unshared);
            } else if (obj instanceof Enum) {
                writeEnum((Enum<?>) obj, desc, unshared);
            } else if (obj instanceof Serializable) {
                writeOrdinaryObject(obj, desc, unshared);
            } else {
                if (extendedDebugInfo) {
                    throw new NotSerializableException(
                        cl.getName() + "\n" + debugInfoStack.toString());
                } else {
                    throw new NotSerializableException(cl.getName());
                }
            }
```
在进行序列化操作时，会判断要被序列化的类是否是Enum、Array和Serializable类型，如果不是则直接抛出NotSerializableException。

**结论**

在序列化过程中，如果被序列化的类中定义了writeObject 和 readObject 方法，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化。

如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。

用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程，比如可以在序列化的过程中动态改变序列化的数值。

## java序列化漏洞

 1. https://blog.chaitin.cn/2015-11-11_java_unserialize_rce/
 2. https://www.ibm.com/developerworks/cn/java/j-5things1/


 java序列化的安全问题，个人觉得其实是针对反序列化来说的，因为序列化以后的二进制数据可以完整的被反序列化，这会导致某些敏感信息被泄露。例如，银行卡号，序列化以后，经过网络传输，如果传输过程中被被反序列化。。。
 
 
我们可以通过实现自己的writeObject  来进行敏感数据的编码， readObject进行解码。

例如，下面的代码对Person类的年龄进行编解码：

```java
public class Person
    implements java.io.Serializable
{
    public Person(String fn, String ln, int a)
    {
        this.firstName = fn; this.lastName = ln; this.age = a;
    }
 
    public String getFirstName() { return firstName; }
    public String getLastName() { return lastName; }
    public int getAge() { return age; }
    public Person getSpouse() { return spouse; }
     
    public void setFirstName(String value) { firstName = value; }
    public void setLastName(String value) { lastName = value; }
    public void setAge(int value) { age = value; }
    public void setSpouse(Person value) { spouse = value; }
 
    private void writeObject(java.io.ObjectOutputStream stream)
        throws java.io.IOException
    {
        // "Encrypt"/obscure the sensitive data
        age = age << 2;
        stream.defaultWriteObject();
    }
 
    private void readObject(java.io.ObjectInputStream stream)
        throws java.io.IOException, ClassNotFoundException
    {
        stream.defaultReadObject();
 
        // "Decrypt"/de-obscure the sensitive data
        age = age << 2;
    }
     
    public String toString()
    {
        return "[Person: firstName=" + firstName + 
            " lastName=" + lastName +
            " age=" + age +
            " spouse=" + (spouse!=null ? spouse.getFirstName() : "[null]") +
            "]";
    }      
 
    private String firstName;
    private String lastName;
    private int age;
    private Person spouse;
}
```

 初次之外，序列化的数据可以被**签名和密封**。
 
 通过使用 writeObject 和 readObject 可以实现密码加密和签名管理，但其实还有更好的方式。

如果需要对整个对象进行加密和签名，最简单的是将它放在一个 javax.crypto.SealedObject 和/或 java.security.SignedObject 包装器中。两者都是可序列化的，所以将对象包装在 SealedObject 中可以围绕原对象创建一种 “包装盒”。必须有对称密钥才能解密，而且密钥必须单独管理。同样，也可以将 SignedObject 用于数据验证，并且对称密钥也必须单独管理。

结合使用这两种对象，便可以轻松地对序列化数据进行密封和签名，而不必强调关于数字签名验证或加密的细节。

对于序列化的数据，反序列化时，如果需要验证，可以实现 ObjectInputValidation 接口，并覆盖 validateObject() 方法。如果调用该方法时发现某处有错误，则抛出一个 InvalidObjectException。
 

## hession序列化对比Java序列化
- https://www.cnblogs.com/wzyxidian/p/5726584.html

**Java序列化：**

　　Java序列化会把要序列化的对象类的元数据和业务数据全部序列化为字节流，而且是把整个继承关系上的东西全部序列化了。它序列化出来的字节流是对那个对象结构到内容的完全描述，包含所有的信息，因此效率较低而且字节流比较大。但是由于确实是序列化了所有内容，所以可以说什么都可以传输，因此也更可用和可靠。


**hession序列化：**

　　它的实现机制是着重于数据，附带简单的类型信息的方法。就像Integer a = 1，hessian会序列化成I 1这样的流，I表示int or Integer，1就是数据内容。而对于复杂对象，通过Java的反射机制，hessian把对象所有的属性当成一个Map来序列化，产生类似M className propertyName1 I 1 propertyName S stringValue（大概如此，确切的忘了）这样的流，包含了基本的类型描述和数据内容。而在序列化过程中，如果一个对象之前出现过，hessian会直接插入一个R index这样的块来表示一个引用位置，从而省去再次序列化和反序列化的时间。这样做的代价就是hessian需要对不同的类型进行不同的处理（因此hessian直接偷懒不支持short），而且遇到某些特殊对象还要做特殊的处理（比如StackTraceElement）。而且同时因为并没有深入到实现内部去进行序列化，所以在某些场合会发生一定的不一致，比如通过Collections.synchronizedMap得到的map。
　　
　　

## ConcurrentHashMap的大小如何确定

 1. https://www.jianshu.com/p/88881fdfcf4c
 2. https://crunchify.com/hashmap-vs-concurrenthashmap-vs-synchronizedmap-how-a-hashmap-can-be-synchronized-in-java/
 3. https://zhuanlan.zhihu.com/p/27149377

ConcurrentHashMap因为其并发的特性，所以要想准确的获取其大小相对困难，我们只能获取一个尽量准确的值。
 
它有一个比较重要的属性sizeCtl，它可能有如下一些值：

 - 0：默认值
 - -1 :代表table正在初始化,其他线程应该交出CPU时间片,退出
 - -N: 表示正有N-1个线程执行扩容操作
 - 大于0: 如果table已经初始化,代表table容量,默认为table大小的0.75,如果还未初始化,代表需要初始化的大小

关于该字段的源码定义如下：
```java
    /**
     * Table initialization and resizing control.  When negative, the
     * table is being initialized or resized: -1 for initialization,
     * else -(1 + the number of active resizing threads).  Otherwise,
     * when table is null, holds the initial table size to use upon
     * creation, or 0 for default. After initialization, holds the
     * next element count value upon which to resize the table.
     */
    private transient volatile int sizeCtl;
```

初始化操作发生在第一次put操作,那么多个线程执行put时,如何保证只执行一次初始化呢?我们看一下put方法：

```java
 /**
     * Maps the specified key to the specified value in this table.
     * Neither the key nor the value can be null.
     *
     * <p>The value can be retrieved by calling the {@code get} method
     * with a key that is equal to the original key.
     *
     * @param key key with which the specified value is to be associated
     * @param value value to be associated with the specified key
     * @return the previous value associated with {@code key}, or
     *         {@code null} if there was no mapping for {@code key}
     * @throws NullPointerException if the specified key or value is null
     */
    public V put(K key, V value) {
        return putVal(key, value, false);
    }

    /** Implementation for put and putIfAbsent */
    final V putVal(K key, V value, boolean onlyIfAbsent) {
        if (key == null || value == null) throw new NullPointerException();
        int hash = spread(key.hashCode());
        int binCount = 0;
        for (Node<K,V>[] tab = table;;) {
            Node<K,V> f; int n, i, fh;
            if (tab == null || (n = tab.length) == 0)
                tab = initTable();
            else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
                if (casTabAt(tab, i, null,
                             new Node<K,V>(hash, key, value, null)))
                    break;                   // no lock when adding to empty bin
            }
            else if ((fh = f.hash) == MOVED)
                tab = helpTransfer(tab, f);
            else {
                V oldVal = null;
                synchronized (f) {
                    if (tabAt(tab, i) == f) {
                        if (fh >= 0) {
                            binCount = 1;
                            for (Node<K,V> e = f;; ++binCount) {
                                K ek;
                                if (e.hash == hash &&
                                    ((ek = e.key) == key ||
                                     (ek != null && key.equals(ek)))) {
                                    oldVal = e.val;
                                    if (!onlyIfAbsent)
                                        e.val = value;
                                    break;
                                }
                                Node<K,V> pred = e;
                                if ((e = e.next) == null) {
                                    pred.next = new Node<K,V>(hash, key,
                                                              value, null);
                                    break;
                                }
                            }
                        }
                        else if (f instanceof TreeBin) {
                            Node<K,V> p;
                            binCount = 2;
                            if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                           value)) != null) {
                                oldVal = p.val;
                                if (!onlyIfAbsent)
                                    p.val = value;
                            }
                        }
                    }
                }
                if (binCount != 0) {
                    if (binCount >= TREEIFY_THRESHOLD)
                        treeifyBin(tab, i);
                    if (oldVal != null)
                        return oldVal;
                    break;
                }
            }
        }
        addCount(1L, binCount);
        return null;
    }
```

put方法调用了putVal方法，putVal方法内部又调用了initTable方法，源码如下：
```java
 /**
     * Initializes table, using the size recorded in sizeCtl.
     */
    private final Node<K,V>[] initTable() {
        Node<K,V>[] tab; int sc;
        while ((tab = table) == null || tab.length == 0) {
            if ((sc = sizeCtl) < 0)
                Thread.yield(); // lost initialization race; just spin
            else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
                try {
                    if ((tab = table) == null || tab.length == 0) {
                        int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
                        @SuppressWarnings("unchecked")
                        Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
                        table = tab = nt;
                        sc = n - (n >>> 2);
                    }
                } finally {
                    sizeCtl = sc;
                }
                break;
            }
        }
        return tab;
    }
```
从源码可以看出，如果map未进行初始化，那么会while一直循环去尝试初始化。

进入while循环以后，首先会判断sizeCtl的值，如果 < 0，也就是已经有其他线程在进行初始化，那么它就让出cpu时间片。

否则，进行map初始化的工作。

这里有个问题需要注意一下，try-finally语句块，这个代码块中并没有使用synchronized关键字进行锁定，那他是如何保证线程安全的呢？也就是如何保证初始化的操作不会由多个线程执行呢？

其实关键点还下面这两句代码：
```java
if ((sc = sizeCtl) < 0)
...
else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) 
```
假设有线程A、B都执行到了else if语句处，那么根据compareAndSwapInt（CAS）的特性，只会有一个方法执行成功，另外一个线程则只能退出，这就是原因。

说完了初始化，我们再看一下他的put操作，源码已经在上面，主要分析一下插入的过程：

 1. 如果待插入的键值对中key或value为null,抛出异常,结束.否则执行2
 2. 如果table为null,则进行初始化操作initTable(),否则执行3
 3. 如果table[i]为空,则用CAS在table[i]头结点直接插入,如果CAS执行成功,退出插入操作;执行步骤7;如果CAS失败,则说明有其他节点已经插入,执行4
 4. 此时判断,hash值是否为MOVED(-1),如果是则说明其他有其他线程在执行扩容操作,帮助他们一起扩容,来提高性能.如果没有在扩容,那么执行5
 5. 判断hash的值,,如果>=0,则在链表合适的位置插入,否则,查看table[i]是否是红黑树结构,如果是,则在红黑树适当位置插入.到此时,键值对已经顺利插入.接下来执行6
 6. 如果table[i]节点数binCount不为0,判断它此时的状态,是否需要转变为红黑树
 7. 执行addcount(1L, binCount)

我自己整理了一个流程图，大致如下所示，部分无关紧要的细节被我省去了，可以参照源码，看下面的流程图：
![此处输入图片的描述][14]

最末尾执行addcount()方法会进行加1操作。在 addCount 方法中，会对baseCount这个变量做 CAS 加法。如下图所示：
![此处输入图片的描述][15]

但是如果并发导致 CAS 失败了，怎么办呢？使用 counterCells。

![此处输入图片的描述][16]

如果上面 CAS 失败了，在 fullAddCount 方法中，会继续死循环操作，直到成功。

![此处输入图片的描述][17]

关于CounterCell的定义：
```java
/**
     * A padded cell for distributing counts.  Adapted from LongAdder
     * and Striped64.  See their internal docs for explanation.
     */
    @sun.misc.Contended 
    static final class CounterCell {
        volatile long value;
        CounterCell(long x) { value = x; }
    }
```
关于Contended注解，这个注解标识着这个类防止需要防止 "伪共享".

关于伪共享：
```java
避免伪共享(false sharing)。
先引用个伪共享的解释：
缓存系统中是以缓存行（cache line）为单位存储的。缓存行是2的整数幂个连续字节，
一般为32-256个字节。最常见的缓存行大小是64个字节。当多线程修改互相独立的变量时，
如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。
```

JDK 8 版本之前没有这个注解，Doug Lea 使用拼接来解决这个问题，把缓存行加满，让缓存之间的修改互不影响。

关于 Size 方法就简单介绍到这里。总结一下：

JDK 8 推荐使用mappingCount 方法，因为这个方法的返回值是 long 类型，不会因为 size 方法是 int 类型限制最大值（size 方法是接口定义的，不能修改）。
在没有并发的情况下，使用一个 baseCount volatile 变量就足够了，当并发的时候，CAS 修改 baseCount 失败后，就会使用 CounterCell 类了，会创建一个这个对象，通常对象的 volatile value 属性是 1。在计算 size 的时候，会将 baseCount 和 CounterCell 数组中的元素的 value 累加，得到总的大小，但这个数字仍旧可能是不准确的。
还有一个需要注意的地方就是，这个 CounterCell 类使用了 **@sun.misc.Contended**  注解标识，这个注解是防止伪共享的。是 1.8 新增的。使用时，需要加上 **-XX:-RestrictContended** 参数。

 
 put方法会执行add操作，那么同样的remove操作会执行减的操作。
 
 源码如下：
 ```java
 /**
     * Removes the key (and its corresponding value) from this map.
     * This method does nothing if the key is not in the map.
     *
     * @param  key the key that needs to be removed
     * @return the previous value associated with {@code key}, or
     *         {@code null} if there was no mapping for {@code key}
     * @throws NullPointerException if the specified key is null
     */
    public V remove(Object key) {
        return replaceNode(key, null, null);
    }

    /**
     * Implementation for the four public remove/replace methods:
     * Replaces node value with v, conditional upon match of cv if
     * non-null.  If resulting value is null, delete.
     */
    final V replaceNode(Object key, V value, Object cv) {
        int hash = spread(key.hashCode());
        for (Node<K,V>[] tab = table;;) {
            Node<K,V> f; int n, i, fh;
            if (tab == null || (n = tab.length) == 0 ||
                (f = tabAt(tab, i = (n - 1) & hash)) == null)
                break;
            else if ((fh = f.hash) == MOVED)
                tab = helpTransfer(tab, f);
            else {
                V oldVal = null;
                boolean validated = false;
                synchronized (f) {
                    if (tabAt(tab, i) == f) {
                        if (fh >= 0) {
                            validated = true;
                            for (Node<K,V> e = f, pred = null;;) {
                                K ek;
                                if (e.hash == hash &&
                                    ((ek = e.key) == key ||
                                     (ek != null && key.equals(ek)))) {
                                    V ev = e.val;
                                    if (cv == null || cv == ev ||
                                        (ev != null && cv.equals(ev))) {
                                        oldVal = ev;
                                        if (value != null)
                                            e.val = value;
                                        else if (pred != null)
                                            pred.next = e.next;
                                        else
                                            setTabAt(tab, i, e.next);
                                    }
                                    break;
                                }
                                pred = e;
                                if ((e = e.next) == null)
                                    break;
                            }
                        }
                        else if (f instanceof TreeBin) {
                            validated = true;
                            TreeBin<K,V> t = (TreeBin<K,V>)f;
                            TreeNode<K,V> r, p;
                            if ((r = t.root) != null &&
                                (p = r.findTreeNode(hash, key, null)) != null) {
                                V pv = p.val;
                                if (cv == null || cv == pv ||
                                    (pv != null && cv.equals(pv))) {
                                    oldVal = pv;
                                    if (value != null)
                                        p.val = value;
                                    else if (t.removeTreeNode(p))
                                        setTabAt(tab, i, untreeify(t.first));
                                }
                            }
                        }
                    }
                }
                if (validated) {
                    if (oldVal != null) {
                        if (value == null)
                            addCount(-1L, -1);
                        return oldVal;
                    }
                    break;
                }
            }
        }
        return null;
    }
 ```
 remove的过程和put的过程很类似，较大的差异就是，put操作后无论如何都会执行addCount方法，但是remove却不一定会执行addCount方法（依靠validated判断），以为remove有可能存在key不在map中的情况，此时是不需要进行size的变更的。
 
 

## mybatis-plus的基本操作及批量操作
mybatis-plus是一款很强大的数据库中间件，他的作用，相比较于mybatis，个人认为就好比sping Boot与spring MVC的差异。一个需要些xml，一个不再需要写xml，极大的提升了开发效率。

**基本操作：**

 1. 首先需要引入mybatis-plus的jar包，目前最新的版本是3.2.0
 2. 其次在dao层定义一个接口（如下代码所示），并且需要继承自plus的BaseMapper接口
 3. 然后service层注入第2步定义的接口，然后就可以使用lamda语法进行数据库的CRUD了

```java
@Repository
@Mapper
public interface OrderConfigMapper extends BaseMapper<OrderConfig> {
}
```

BaseMapper接口提供了丰富的接口供使用者调用，如下图所示：

![此处输入图片的描述][18]

service层调用示例如下：

```java
OrderConfig orderConfig = orderConfigMapper.selectOne(new LambdaQueryWrapper<OrderConfig>().eq(OrderConfig::getAppid, appId));
```
 **分页查询**
 
 上面示范了一个简单的查询接口，那么分页的查询接口，该怎么写呢？可以看到在BaseMapper接口中，有这么一个方法：
 ```java
     /**
     * <p>
     * 根据 entity 条件，查询全部记录（并翻页）
     * </p>
     *
     * @param page         分页查询条件（可以为 RowBounds.DEFAULT）
     * @param queryWrapper 实体对象封装操作类（可以为 null）
     */
    IPage<T> selectPage(IPage<T> page, @Param(Constants.WRAPPER) Wrapper<T> queryWrapper);
 ```
 
 具体一点，需要在dao层接口中，实现一个**default**方法，该方法发返回是IPage类型的，示例代码如下：
 ```java
 default IPage<OperationLog> selectPaged(QueryLogDTO queryLogDTO) {
        Page<OperationLog> page = new Page<>(pageIndex, pageSize);
        LambdaQueryWrapper<OperationLog> condition = new LambdaQueryWrapper<>();
        condition.eq(OperationLog::getGameId, id);
        condition.orderByDesc(OperationLog::getCreatedAt);
        return selectPage(page, condition);
    }
 ```
 IPage对象的内部结构如下所示：
 ![此处输入图片的描述][19]
 
 其中的getRecords方法就可以取出分页的数据。
 
 **批量操作**
 
 上面的BaseMapper类中，貌似没有看到批量操作（批量插入，更新）的接口，那么plus支持批量，必须支持啊。
 
 mybatis-plus有一个Iservice对象，其接口方法如下，可以看到他有很多批量操作的方法：
 
 ![此处输入图片的描述][20]
 
 它的使用也比较简单，与上面不同的是，它是需要在service层创建一个接口继承自IService：
 
 ```java
 public interface BatchLogicServerService extends IService<LogicServer> {
}
 ```
 
 然后创建一个BatchLogicServerService接口的实现类：
 
 ```java
 @Service
public class BatchLogicServerServiceImpl extends ServiceImpl<LogicServerMapper, LogicServer> implements BatchLogicServerService {
}
 ```
 
 然后再其他的service中注入这个对象，就可以进行批量操作了.
 
 ```java
 batchLogicServerService.saveBatch(logicServerList);
 ```
 
 

## springboot的starter原理

 1. https://www.jianshu.com/p/346cac67bfcc
 2. https://yq.aliyun.com/articles/713269

总的来说，starter主要替我们完成了如下一些事情：

 1. 首先是引入了相关的jar文件，并且解决了jar文件相互之间的依赖关系（避免了版本冲突）
 2. 依靠spring本身的自动配置，将相关的bean注入到spring容器中
 3. 解决了配置的自动解析，例如，配置数据源，我们只需要按照规范在properties或者yml文件写好用户名、密码即可完成自动配置。
 4. 更多的可参考链接1和2
 5. 如何自己实现一个starter可以参考https://www.xncoding.com/2017/07/22/spring/sb-starter.html

 

## @Repository @Mapper @Resource @Autowired
- https://www.jianshu.com/p/e64fd29b429a

**@Repository @Mapper**

 1. 来源不同

@Repository 为spring注解
@Mapper 为 mybatis注解

 1. 功能略有不同

@Repository在dao的接口上注解时，必须使用xml配置sql语句
@Mapper在dao的接口上注解时，可以将sql语句配置在dao接口文件中

 **@AutoWired @Resource**
 
 首先是**@AutoWired**注解

 1. 这个注解是属于spring
 2. 默认按类型装配
 3. 默认情况下必须要求依赖对象必须存在，如果要允许null值，可以设置它的required属性为false，如：@Autowired(required=false)
 4. 如果我们想使用名称装配可以结合@Qualifier注解进行使用，如下：
```java
@AutoWired
@Qualifier("baseDao")
private BaseDao baseDao;
```

首先是**@Resource**注解

 1. 这个注解属于J2EE
 2. 默认按照名称进行装配
 3. 名称可以通过name属性进行指定，

如果没有指定name属性，当注解写在字段上时，默认取字段名进行安装名称查找，

如果注解写在setter方法上默认取属性名进行装配。

当找不到与名称匹配的bean时才按照类型进行装配。

但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。

## MyBatisPlus中使用 @TableField完成字段自动填充

 1. https://blog.csdn.net/BADAO_LIUMANG_QIZHI/article/details/89450006
 2. https://mp.baomidou.com/guide/auto-fill-metainfo.html

MyBatisPlus提供了自动填充功能，该功能在插入或者更新数据的时候，对于created_at、created_by、updated_at、updated_by可以自动填充。

要达到这种效果我们需要分两步：

 1. 在需要自动填充的字段上添加@TableField注解
 2. 定义自己的handler实现MetaObjectHandler接口

```java
public class User {

    // 注意！这里需要标记为填充字段
    @TableField(.. fill = FieldFill.INSERT)
    private String fillField;

    ....
}
```

自定义实现类 MyMetaObjectHandler

```java
@Component
public class MyMetaObjectHandler implements MetaObjectHandler {

    private static final Logger LOGGER = LoggerFactory.getLogger(MyMetaObjectHandler.class);

    @Override
    public void insertFill(MetaObject metaObject) {
        LOGGER.info("start insert fill ....");
        this.setFieldValByName("operator", "Jerry", metaObject);//版本号3.0.6以及之前的版本
        //this.setInsertFieldValByName("operator", "Jerry", metaObject);//@since 快照：3.0.7.2-SNAPSHOT， @since 正式版暂未发布3.0.7
    }

    @Override
    public void updateFill(MetaObject metaObject) {
        LOGGER.info("start update fill ....");
        this.setFieldValByName("operator", "Tom", metaObject);
        //this.setUpdateFieldValByName("operator", "Tom", metaObject);//@since 快照：3.0.7.2-SNAPSHOT， @since 正式版暂未发布3.0.7
    }
}
```

注意：

 1. 填充处理器MyMetaObjectHandler 在 Spring Boot 中需要声明@Component 注入。
 2. 必须使用父类的setFieldValByName()或者setInsertFieldValByName/setUpdateFieldValByName方法，否则不会根据注解FieldFill.xxx来区分


## github添加多个sshkey

为自己的电脑配置多个ssh-key，方法如下（如果不配做，直接使用公司邮箱生成的sshkey也是可以的，只不过github是以邮箱来记录commit的，公司的邮箱提交到git的代码不会变绿）：

 1. https://www.cnblogs.com/sheting/p/7992063.html

按照参考链接里的，可以基于两个邮箱，生成两套公司钥，只是需要注意生成第二套公私钥的时候，注意设置文件名字，不要覆盖了第一套。

```shell
ssh-keygen -t rsa -C “youremail@yourcompany.com”
```
然后做一下设置：
因为SSH默认只读取id_rsa,为了让SSH识别新的私钥,需要使用命令将其添加到SSH agent,命令如下：
```shell
ssh-add ~/.ssh/id_rsa
ssh-add ~/.ssh/id_rsa_github
```
　　　　
然后就是在~/.ssh/下配置config文件：

```shell
# gitlab
Host gitlab.com
HostName gitlab.com
PreferredAuthentications publickey
IdentityFile ~/.ssh/id_rsa
# GitHub
Host github.com
HostName github.com
PreferredAuthentications publickey
IdentityFile ~/.ssh/id_rsa_github
```

然后执行：
```shell
ssh -T git@github.com 
Hi Audi-A7! You've successfully authenticated, but GitHub does not provide shell access.
```

如果出现上面是提示，就证明可以正常pull github的代码了。
 
 

## CDN缓存

一般部署CDN会有如下两个问题：

 1. 如何将用户的请求映射到 CDN 节点上？
 2. 如何根据用户的地理位置信息选择到比较近的节点？

第一个问题：一般对于需要缓存到CDN的静态资源，后端基本都只会存储一个资源的地址，浏览器拿到这个地址再去实际的地址请求资源。

那，后端如何存储这个地址呢？

首先这个地址不能是CDN的ip，因为ip有可能会改变，极端的，如果更换CDN厂商，ip肯定会变，如何保证后端存储的资源地址不会随着ip的改变而改变呢？这时候后端可能就需要存储资源ip对应的「域名」了，但是域名解析也需要花时间，这个可以通过缓存域名解析的结果来提高速度。一般浏览器也会内置域名解析缓存的功能。


第二个问题：ip地址地域映射，一般可以根据ip定位一个大致的范围，除非是有了VPN等，否则一般都是比较准确的地理位置，然后选择合适的CDN节点提供资源服务即可。


## ThreadLocal原理及实际应用

 1. https://www.cnblogs.com/dolphin0520/p/3920407.html
 2. http://ifeve.com/%E4%BD%BF%E7%94%A8threadlocal%E4%B8%8D%E5%BD%93%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%AF%BC%E8%87%B4%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2/
 3. http://www.jasongj.com/java/threadlocal/


## 三个线程交替打印输出
这个问题，很多面试场景都出现过。其实本质上来讲，这就是一个获取公平锁的过程，这样大家才可以交替的打印。

下面，我们使用ReentrantLock来实现这过程：
```java
package com.interview.javabasic.thread;


import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

/**
 * 三个线程交替输出
 *
 * @author: WangQuanzhou
 * @date: 2020/1/11 14:55
 */
public class FairLock implements Runnable {
    // true表示是公平锁
    private Lock lock = new ReentrantLock(true);

    @Override
    public void run() {
        while (true) {
            try {
                lock.lock();
                System.out.println("current thread is " + Thread.currentThread().getName());
            } finally {
                lock.unlock();
            }
        }
    }

    public static void main(String[] args) {
        FairLock fairLock = new FairLock();
        new Thread(fairLock).start();
        new Thread(fairLock).start();
        new Thread(fairLock).start();
    }
}
```

这道题，还有一种类似的问法就是，三个线程交替累加输出一个数字，比如三个线程交替+1输出，直到20（20方便调试）。按照上面的思路，实现也很简单：
```java
package com.interview.javabasic.thread;


import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

/**
 * 三个线程交替输出
 *
 * @author: WangQuanzhou
 * @date: 2020/1/11 14:55
 */
public class FairLock implements Runnable {
    // true表示是公平锁
    private Lock lock = new ReentrantLock(true);
    private int count = 0;

    @Override
    public void run() {
        while (count < 100) {
            try {
                lock.lock();
                if (count > 99) {
                    return;
                }
                count++;
                System.out.println("after count, current thread is " + Thread.currentThread().getName() + " ,count = " + count);
                System.out.println();
            } finally {
                lock.unlock();
            }
        }
    }

    public static void main(String[] args) {
        FairLock fairLock = new FairLock();
        new Thread(fairLock).start();
        new Thread(fairLock).start();
        new Thread(fairLock).start();
    }
}
```
运行输出：
```java
after count, current thread is Thread-0 ,count = 1

after count, current thread is Thread-1 ,count = 2

after count, current thread is Thread-2 ,count = 3

after count, current thread is Thread-0 ,count = 4

after count, current thread is Thread-1 ,count = 5

after count, current thread is Thread-2 ,count = 6

after count, current thread is Thread-0 ,count = 7

after count, current thread is Thread-1 ,count = 8

after count, current thread is Thread-2 ,count = 9

after count, current thread is Thread-0 ,count = 10

after count, current thread is Thread-1 ,count = 11

after count, current thread is Thread-2 ,count = 12

after count, current thread is Thread-0 ,count = 13

after count, current thread is Thread-1 ,count = 14

after count, current thread is Thread-2 ,count = 15

after count, current thread is Thread-0 ,count = 16

after count, current thread is Thread-1 ,count = 17

after count, current thread is Thread-2 ,count = 18

after count, current thread is Thread-0 ,count = 19

after count, current thread is Thread-1 ,count = 20
```
注意上面的if判断不可少，因为lock是在while循环之后，可能在这段时间其他线程已经更新了count的值，因此虽然线程已经通过了while循环的判断，但是count已经发生了改变，在获取了lock以后，线程的count会被刷新，此时，count再自加就会出错。

可以通过如下代码，输出便于理解：
```java
package com.interview.javabasic.thread;


import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

/**
 * 三个线程交替输出
 *
 * @author: WangQuanzhou
 * @date: 2020/1/11 14:55
 */
public class FairLock implements Runnable {
    // true表示是公平锁
    private Lock lock = new ReentrantLock(true);
    private int count = 0;

    @Override
    public void run() {
        while (count < 20) {
            try {
                System.out.println("before lock, current thread is " + Thread.currentThread().getName() + " ,count = " + count);
                lock.lock();
                System.out.println("after lock, current thread is " + Thread.currentThread().getName() + " ,count = " + count);
                if (count > 19) {
                    return;
                }
                count++;
                System.out.println("after count, current thread is " + Thread.currentThread().getName() + " ,count = " + count);
                System.out.println();
            } finally {
                lock.unlock();
            }
        }
    }

    public static void main(String[] args) {
        FairLock fairLock = new FairLock();
        new Thread(fairLock).start();
        new Thread(fairLock).start();
        new Thread(fairLock).start();
    }
}
```
程序输出：
```java
before lock, current thread is Thread-0 ,count = 0
before lock, current thread is Thread-2 ,count = 0
before lock, current thread is Thread-1 ,count = 0
after lock, current thread is Thread-0 ,count = 0
after count, current thread is Thread-0 ,count = 1

before lock, current thread is Thread-0 ,count = 1
after lock, current thread is Thread-2 ,count = 1
after count, current thread is Thread-2 ,count = 2

before lock, current thread is Thread-2 ,count = 2
after lock, current thread is Thread-1 ,count = 2
after count, current thread is Thread-1 ,count = 3

before lock, current thread is Thread-1 ,count = 3
after lock, current thread is Thread-0 ,count = 3
after count, current thread is Thread-0 ,count = 4

before lock, current thread is Thread-0 ,count = 4
after lock, current thread is Thread-2 ,count = 4
after count, current thread is Thread-2 ,count = 5

before lock, current thread is Thread-2 ,count = 5
after lock, current thread is Thread-1 ,count = 5
after count, current thread is Thread-1 ,count = 6

before lock, current thread is Thread-1 ,count = 6
after lock, current thread is Thread-0 ,count = 6
after count, current thread is Thread-0 ,count = 7

before lock, current thread is Thread-0 ,count = 7
after lock, current thread is Thread-2 ,count = 7
after count, current thread is Thread-2 ,count = 8

before lock, current thread is Thread-2 ,count = 8
after lock, current thread is Thread-1 ,count = 8
after count, current thread is Thread-1 ,count = 9

before lock, current thread is Thread-1 ,count = 9
after lock, current thread is Thread-0 ,count = 9
after count, current thread is Thread-0 ,count = 10

before lock, current thread is Thread-0 ,count = 10
after lock, current thread is Thread-2 ,count = 10
after count, current thread is Thread-2 ,count = 11

before lock, current thread is Thread-2 ,count = 11
after lock, current thread is Thread-1 ,count = 11
after count, current thread is Thread-1 ,count = 12

before lock, current thread is Thread-1 ,count = 12
after lock, current thread is Thread-0 ,count = 12
after count, current thread is Thread-0 ,count = 13

before lock, current thread is Thread-0 ,count = 13
after lock, current thread is Thread-2 ,count = 13
after count, current thread is Thread-2 ,count = 14

before lock, current thread is Thread-2 ,count = 14
after lock, current thread is Thread-1 ,count = 14
after count, current thread is Thread-1 ,count = 15

before lock, current thread is Thread-1 ,count = 15
after lock, current thread is Thread-0 ,count = 15
after count, current thread is Thread-0 ,count = 16

before lock, current thread is Thread-0 ,count = 16
after lock, current thread is Thread-2 ,count = 16
after count, current thread is Thread-2 ,count = 17

before lock, current thread is Thread-2 ,count = 17
after lock, current thread is Thread-1 ,count = 17
after count, current thread is Thread-1 ,count = 18

before lock, current thread is Thread-1 ,count = 18
after lock, current thread is Thread-0 ,count = 18
after count, current thread is Thread-0 ,count = 19

before lock, current thread is Thread-0 ,count = 19
after lock, current thread is Thread-2 ,count = 19
after count, current thread is Thread-2 ,count = 20

after lock, current thread is Thread-1 ,count = 20
after lock, current thread is Thread-0 ,count = 20
```
注意最后几行，以Thread-1，他在lock前后的输出，count的值发生改变，获取lock之前是18，获取lock以后是20，此时还未进行自加操作，if判断就过不来了，因此也才可以正确输出。


 

  [1]: https://github.com/WQZ321123/learn/blob/master/source/image/mysql/%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E7%9A%84%E9%97%B4%E9%9A%99%E9%94%81.jpg?raw=true
  [2]: https://github.com/Audi-A7/learn/blob/master/source/image/other/%E9%97%AE%E5%A5%BD%E9%80%9A%E9%85%8D%E7%AC%A6.jpg?raw=true
  [3]: https://github.com/Audi-A7/learn/blob/master/source/image/other/error.jpg?raw=true
  [4]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/ByteTCC.png?raw=true
  [5]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/ribbon_auto_retry.png?raw=true
  [6]: https://raw.githubusercontent.com/Audi-A7/learn/master/source/image/2019/dispatcherServlet.webp?token=ABXI2UIEHF4VBVTMDBAGLX25RIIJQ
  [7]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/connector.jpg?raw=true
  [8]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/connector2.jpg?raw=true
  [9]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/rpc.jpg?raw=true
  [10]: https://raw.githubusercontent.com/Audi-A7/learn/master/source/image/2019/http2%20frame.webp?token=ABXI2UOURBKDKRA5B7VQ7NK5R362S
  [11]: https://raw.githubusercontent.com/Audi-A7/learn/master/source/image/2019/http2%20prority.webp?token=ABXI2UKI354CO4HMKMESPC25R37LI
  [12]: https://raw.githubusercontent.com/Audi-A7/learn/master/source/image/2019/no-server-push.webp?token=ABXI2UMF23I7B4AV4JT6F2C5R374U
  [13]: https://raw.githubusercontent.com/Audi-A7/learn/master/source/image/2019/server-push.webp?token=ABXI2ULPDY7LN5AU4TXVSFK5R4AE2
  [14]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/ConcurrentHashMap.png?raw=true
  [15]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/addcount.png?raw=true
  [16]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/addcount2.png?raw=true
  [17]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/addcount3.png?raw=true
  [18]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/BaseMapper.png?raw=true
  [19]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/IPage.png?raw=true
  [20]: https://github.com/Audi-A7/learn/blob/master/source/image/2019/IService.png?raw=true
  [21]: image/spring/IRule.png
